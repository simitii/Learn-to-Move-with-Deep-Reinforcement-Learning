2018-05-15 22:11:26.324859 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #0 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                         2.26095e-06
Policy Loss                     0.00478001
Raw Policy Loss                 0.00478001
Preactivation Policy Loss       0
Q Predictions Mean             -0.00470202
Q Predictions Std               0.000682768
Q Predictions Max              -0.00334
Q Predictions Min              -0.00726667
Q Targets Mean                 -0.00407557
Q Targets Std                   0.00100333
Q Targets Max                  -0.000676796
Q Targets Min                  -0.0062603
Bellman Errors Mean             2.26095e-06
Bellman Errors Std              5.03719e-06
Bellman Errors Max              4.34264e-05
Bellman Errors Min              1.84381e-09
Policy Action Mean              0.00812949
Policy Action Std               0.036279
Policy Action Max               0.0861204
Policy Action Min              -0.0633957
Test Rewards Mean              -0.00687301
Test Rewards Std                0.00820009
Test Rewards Max                0.0129362
Test Rewards Min               -0.0217516
Test Returns Mean              -0.852253
Test Returns Std                0.0112295
Test Returns Max               -0.826941
Test Returns Min               -0.865681
Test Actions Mean              -0.0657573
Test Actions Std                0.954123
Test Actions Max                0.999939
Test Actions Min               -0.999955
Num Paths                       4
Exploration Rewards Mean       -0.00436657
Exploration Rewards Std         0.00709643
Exploration Rewards Max         0.0129371
Exploration Rewards Min        -0.0206479
Exploration Returns Mean       -0.550187
Exploration Returns Std         0.256429
Exploration Returns Max        -0.15524
Exploration Returns Min        -0.804392
Exploration Actions Mean        0.079661
Exploration Actions Std         0.743326
Exploration Actions Max         1
Exploration Actions Min        -1
AverageReturn                  -0.852253
Number of train steps total   863
Number of env steps total    1000
Number of rollouts total        4
Train Time (s)                 70.2905
(Previous) Eval Time (s)        0
Sample Time (s)                64.9942
Epoch Time (s)                135.285
Total Train Time (s)          364.851
Epoch                           0
---------------------------  --------------
2018-05-15 22:15:13.260240 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #0 | Epoch Duration: 364.87902879714966
2018-05-15 22:15:13.260355 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #0 | Started Training: True
2018-05-15 22:15:49.948446 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #1 | Epoch Duration: 36.687986850738525
2018-05-15 22:15:49.948675 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #1 | Started Training: True
2018-05-15 22:16:25.903746 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #2 | Epoch Duration: 35.9549036026001
2018-05-15 22:16:25.903952 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #2 | Started Training: True
2018-05-15 22:17:26.812654 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #3 | Epoch Duration: 60.90854549407959
2018-05-15 22:17:26.812875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #3 | Started Training: True
2018-05-15 22:18:40.059256 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #4 | Epoch Duration: 73.2462248802185
2018-05-15 22:18:40.059468 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #4 | Started Training: True
2018-05-15 22:19:49.935706 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #5 | Epoch Duration: 69.8761076927185
2018-05-15 22:19:49.935911 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #5 | Started Training: True
2018-05-15 22:20:53.697716 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #6 | Epoch Duration: 63.76167583465576
2018-05-15 22:20:53.697947 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #6 | Started Training: True
2018-05-15 22:22:13.051972 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #7 | Epoch Duration: 79.35387015342712
2018-05-15 22:22:13.052165 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #7 | Started Training: True
2018-05-15 22:24:40.996271 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #8 | Epoch Duration: 147.9439821243286
2018-05-15 22:24:40.996518 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #8 | Started Training: True
2018-05-15 22:27:09.138747 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #9 | Epoch Duration: 148.14205813407898
2018-05-15 22:27:09.138946 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #9 | Started Training: True
2018-05-15 22:29:37.070602 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #10 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          1.4028e-05
Policy Loss                      8.68605e-05
Raw Policy Loss                  8.68605e-05
Preactivation Policy Loss        0
Q Predictions Mean              -0.00204191
Q Predictions Std                0.0353405
Q Predictions Max                0.0656773
Q Predictions Min               -0.0783604
Q Targets Mean                  -0.00142412
Q Targets Std                    0.0354095
Q Targets Max                    0.065682
Q Targets Min                   -0.0798481
Bellman Errors Mean              1.4028e-05
Bellman Errors Std               7.86214e-05
Bellman Errors Max               0.000884487
Bellman Errors Min               2.21024e-11
Policy Action Mean              -0.0659405
Policy Action Std                0.954082
Policy Action Max                0.999946
Policy Action Min               -0.999962
Test Rewards Mean                0.00618047
Test Rewards Std                 0.00345557
Test Rewards Max                 0.0140973
Test Rewards Min                -0.00079513
Test Returns Mean                0.47413
Test Returns Std                 0.00547022
Test Returns Max                 0.484324
Test Returns Min                 0.463648
Test Actions Mean               -0.131747
Test Actions Std                 0.982338
Test Actions Max                 1
Test Actions Min                -1
Num Paths                       10
Exploration Rewards Mean         0.00602824
Exploration Rewards Std          0.00330399
Exploration Rewards Max          0.013089
Exploration Rewards Min         -0.000831049
Exploration Returns Mean         0.518428
Exploration Returns Std          0.0197737
Exploration Returns Max          0.549915
Exploration Returns Min          0.486557
Exploration Actions Mean        -0.110574
Exploration Actions Std          0.825833
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.47413
Number of train steps total  10863
Number of env steps total    11000
Number of rollouts total       144
Train Time (s)                  72.685
(Previous) Eval Time (s)         1.936e-06
Sample Time (s)                 72.5038
Epoch Time (s)                 145.189
Total Train Time (s)          1422.35
Epoch                           10
---------------------------  ---------------
2018-05-15 22:32:51.000755 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #10 | Epoch Duration: 341.8616907596588
2018-05-15 22:32:51.000872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #10 | Started Training: True
2018-05-15 22:35:16.067276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #11 | Epoch Duration: 145.0663013458252
2018-05-15 22:35:16.067500 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #11 | Started Training: True
2018-05-15 22:37:43.164261 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #12 | Epoch Duration: 147.09661197662354
2018-05-15 22:37:43.164442 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #12 | Started Training: True
2018-05-15 22:38:22.803796 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #13 | Epoch Duration: 39.63921880722046
2018-05-15 22:38:22.804003 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #13 | Started Training: True
2018-05-15 22:39:01.853154 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #14 | Epoch Duration: 39.048994302749634
2018-05-15 22:39:01.853364 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #14 | Started Training: True
2018-05-15 22:39:41.603061 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #15 | Epoch Duration: 39.74954795837402
2018-05-15 22:39:41.603237 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #15 | Started Training: True
2018-05-15 22:40:21.552712 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #16 | Epoch Duration: 39.94936537742615
2018-05-15 22:40:21.552912 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #16 | Started Training: True
2018-05-15 22:42:00.873850 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #17 | Epoch Duration: 99.3208110332489
2018-05-15 22:42:00.874019 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #17 | Started Training: True
2018-05-15 22:44:31.265361 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #18 | Epoch Duration: 150.39121413230896
2018-05-15 22:44:31.265565 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #18 | Started Training: True
2018-05-15 22:46:58.549287 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #19 | Epoch Duration: 147.2835865020752
2018-05-15 22:46:58.549498 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #19 | Started Training: True
2018-05-15 22:49:31.986564 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #20 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          7.4146e-05
Policy Loss                     -0.188199
Raw Policy Loss                 -0.188199
Preactivation Policy Loss        0
Q Predictions Mean               0.184651
Q Predictions Std                0.303983
Q Predictions Max                0.615249
Q Predictions Min               -0.517002
Q Targets Mean                   0.186206
Q Targets Std                    0.301692
Q Targets Max                    0.618463
Q Targets Min                   -0.512191
Bellman Errors Mean              7.4146e-05
Bellman Errors Std               9.28128e-05
Bellman Errors Max               0.000415866
Bellman Errors Min               1.1685e-09
Policy Action Mean              -0.101608
Policy Action Std                0.982519
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00651208
Test Rewards Std                 0.0039271
Test Rewards Max                 0.0143133
Test Rewards Min                -0.00100184
Test Returns Mean                0.7007
Test Returns Std                 0.00426093
Test Returns Max                 0.70867
Test Returns Min                 0.695126
Test Actions Mean                0.212977
Test Actions Std                 0.971993
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        7
Exploration Rewards Mean         0.00571176
Exploration Rewards Std          0.00367841
Exploration Rewards Max          0.0153914
Exploration Rewards Min         -0.00205971
Exploration Returns Mean         0.860028
Exploration Returns Std          0.0882071
Exploration Returns Max          1.01158
Exploration Returns Min          0.757661
Exploration Actions Mean         0.0886748
Exploration Actions Std          0.836725
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.7007
Number of train steps total  20863
Number of env steps total    21000
Number of rollouts total       227
Train Time (s)                  76.1408
(Previous) Eval Time (s)         2.645e-06
Sample Time (s)                 74.3961
Epoch Time (s)                 150.537
Total Train Time (s)          2661.1
Epoch                           20
---------------------------  ---------------
2018-05-15 22:53:29.989736 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #20 | Epoch Duration: 391.4401123523712
2018-05-15 22:53:29.989851 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #20 | Started Training: True
2018-05-15 22:55:05.192047 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #21 | Epoch Duration: 95.20209670066833
2018-05-15 22:55:05.192244 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #21 | Started Training: True
2018-05-15 22:55:46.867108 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #22 | Epoch Duration: 41.674739599227905
2018-05-15 22:55:46.867330 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #22 | Started Training: True
2018-05-15 22:56:28.050225 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #23 | Epoch Duration: 41.182727575302124
2018-05-15 22:56:28.050432 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #23 | Started Training: True
2018-05-15 22:57:09.621363 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #24 | Epoch Duration: 41.57080340385437
2018-05-15 22:57:09.621572 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #24 | Started Training: True
2018-05-15 22:57:50.103842 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #25 | Epoch Duration: 40.482123136520386
2018-05-15 22:57:50.104023 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #25 | Started Training: True
2018-05-15 22:58:32.139200 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #26 | Epoch Duration: 42.035056352615356
2018-05-15 22:58:32.139388 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #26 | Started Training: True
2018-05-15 22:59:12.758201 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #27 | Epoch Duration: 40.61869692802429
2018-05-15 22:59:12.758475 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #27 | Started Training: True
2018-05-15 22:59:53.463980 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #28 | Epoch Duration: 40.70528221130371
2018-05-15 22:59:53.464178 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #28 | Started Training: True
2018-05-15 23:00:33.228028 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #29 | Epoch Duration: 39.7636775970459
2018-05-15 23:00:33.228226 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #29 | Started Training: True
2018-05-15 23:02:15.420732 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #30 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          4.23342e-05
Policy Loss                     -0.261163
Raw Policy Loss                 -0.261163
Preactivation Policy Loss        0
Q Predictions Mean               0.25785
Q Predictions Std                0.161883
Q Predictions Max                0.411354
Q Predictions Min               -0.259963
Q Targets Mean                   0.260676
Q Targets Std                    0.160474
Q Targets Max                    0.420346
Q Targets Min                   -0.24531
Bellman Errors Mean              4.23342e-05
Bellman Errors Std               0.000121359
Bellman Errors Max               0.00115937
Bellman Errors Min               1.00272e-10
Policy Action Mean               0.201306
Policy Action Std                0.971861
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00702822
Test Rewards Std                 0.00385623
Test Rewards Max                 0.0139512
Test Rewards Min                -0.000751155
Test Returns Mean                0.890827
Test Returns Std                 0.00456421
Test Returns Max                 0.897179
Test Returns Min                 0.883197
Test Actions Mean                0.018138
Test Actions Std                 0.99137
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        7
Exploration Rewards Mean         0.00704367
Exploration Rewards Std          0.0044803
Exploration Rewards Max          0.0172903
Exploration Rewards Min         -0.0020226
Exploration Returns Mean         0.837191
Exploration Returns Std          0.0736464
Exploration Returns Max          0.958126
Exploration Returns Min          0.752573
Exploration Actions Mean         0.0156048
Exploration Actions Std          0.837343
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.890827
Number of train steps total  30863
Number of env steps total    31000
Number of rollouts total       314
Train Time (s)                  50.6563
(Previous) Eval Time (s)         2.6e-06
Sample Time (s)                 48.3691
Epoch Time (s)                  99.0255
Total Train Time (s)          3408.63
Epoch                           30
---------------------------  ---------------
2018-05-15 23:05:57.740716 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #30 | Epoch Duration: 324.51233196258545
2018-05-15 23:05:57.740889 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #30 | Started Training: True
2018-05-15 23:08:30.687572 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #31 | Epoch Duration: 152.94655632972717
2018-05-15 23:08:30.687767 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #31 | Started Training: True
2018-05-15 23:11:03.880842 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #32 | Epoch Duration: 153.192946434021
2018-05-15 23:11:03.881028 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #32 | Started Training: True
2018-05-15 23:13:33.682957 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #33 | Epoch Duration: 149.80179834365845
2018-05-15 23:13:33.683144 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #33 | Started Training: True
2018-05-15 23:15:53.894271 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #34 | Epoch Duration: 140.21099829673767
2018-05-15 23:15:53.894463 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #34 | Started Training: True
2018-05-15 23:16:34.549764 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #35 | Epoch Duration: 40.65518260002136
2018-05-15 23:16:34.549951 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #35 | Started Training: True
2018-05-15 23:17:15.487945 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #36 | Epoch Duration: 40.93787097930908
2018-05-15 23:17:15.488133 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #36 | Started Training: True
2018-05-15 23:17:56.380349 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #37 | Epoch Duration: 40.89207410812378
2018-05-15 23:17:56.380607 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #37 | Started Training: True
2018-05-15 23:18:37.076651 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #38 | Epoch Duration: 40.69583034515381
2018-05-15 23:18:37.076852 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #38 | Started Training: True
2018-05-15 23:19:18.130333 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #39 | Epoch Duration: 41.05333185195923
2018-05-15 23:19:18.130563 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #39 | Started Training: True
2018-05-15 23:20:01.811782 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #40 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          3.05899e-05
Policy Loss                     -0.312003
Raw Policy Loss                 -0.312003
Preactivation Policy Loss        0
Q Predictions Mean               0.309053
Q Predictions Std                0.181823
Q Predictions Max                0.49013
Q Predictions Min               -0.426042
Q Targets Mean                   0.30511
Q Targets Std                    0.180784
Q Targets Max                    0.488369
Q Targets Min                   -0.423316
Bellman Errors Mean              3.05899e-05
Bellman Errors Std               3.28344e-05
Bellman Errors Max               0.000206731
Bellman Errors Min               9.59037e-09
Policy Action Mean               0.0325084
Policy Action Std                0.991926
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00752756
Test Rewards Std                 0.00354231
Test Rewards Max                 0.0143179
Test Rewards Min                -6.47943e-05
Test Returns Mean                0.981405
Test Returns Std                 0.0129909
Test Returns Max                 1.00613
Test Returns Min                 0.964701
Test Actions Mean               -0.00335255
Test Actions Std                 0.984886
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        8
Exploration Rewards Mean         0.00707491
Exploration Rewards Std          0.00373552
Exploration Rewards Max          0.0153831
Exploration Rewards Min         -0.00119478
Exploration Returns Mean         0.830418
Exploration Returns Std          0.0670577
Exploration Returns Max          0.986049
Exploration Returns Min          0.749509
Exploration Actions Mean         0.00243664
Exploration Actions Std          0.834836
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.981405
Number of train steps total  40863
Number of env steps total    41000
Number of rollouts total       401
Train Time (s)                  22.09
(Previous) Eval Time (s)         2.65e-06
Sample Time (s)                 18.4435
Epoch Time (s)                  40.5335
Total Train Time (s)          4456.98
Epoch                           40
---------------------------  ---------------
2018-05-15 23:23:26.327071 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #40 | Epoch Duration: 248.1963517665863
2018-05-15 23:23:26.327191 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #40 | Started Training: True
2018-05-15 23:25:59.929939 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #41 | Epoch Duration: 153.60263919830322
2018-05-15 23:25:59.930126 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #41 | Started Training: True
2018-05-15 23:28:34.851142 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #42 | Epoch Duration: 154.92087960243225
2018-05-15 23:28:34.851322 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #42 | Started Training: True
2018-05-15 23:31:05.747434 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #43 | Epoch Duration: 150.8959982395172
2018-05-15 23:31:05.747612 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #43 | Started Training: True
2018-05-15 23:33:39.077403 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #44 | Epoch Duration: 153.32967472076416
2018-05-15 23:33:39.077592 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #44 | Started Training: True
2018-05-15 23:36:09.459312 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #45 | Epoch Duration: 150.38159012794495
2018-05-15 23:36:09.459518 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #45 | Started Training: True
2018-05-15 23:38:42.279225 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #46 | Epoch Duration: 152.81957840919495
2018-05-15 23:38:42.279426 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #46 | Started Training: True
2018-05-15 23:41:13.492688 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #47 | Epoch Duration: 151.21313381195068
2018-05-15 23:41:13.492898 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #47 | Started Training: True
2018-05-15 23:43:44.247257 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #48 | Epoch Duration: 150.75420308113098
2018-05-15 23:43:44.247443 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #48 | Started Training: True
2018-05-15 23:46:14.691443 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #49 | Epoch Duration: 150.44388794898987
2018-05-15 23:46:14.691659 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #49 | Started Training: True
2018-05-15 23:47:20.440644 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #50 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          2.24706e-05
Policy Loss                     -0.357946
Raw Policy Loss                 -0.357946
Preactivation Policy Loss        0
Q Predictions Mean               0.355521
Q Predictions Std                0.147946
Q Predictions Max                0.503835
Q Predictions Min               -0.363488
Q Targets Mean                   0.356356
Q Targets Std                    0.147176
Q Targets Max                    0.505294
Q Targets Min                   -0.36315
Bellman Errors Mean              2.24706e-05
Bellman Errors Std               5.10805e-05
Bellman Errors Max               0.000280385
Bellman Errors Min               2.86811e-09
Policy Action Mean               0.0163215
Policy Action Std                0.991499
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00765719
Test Rewards Std                 0.00433163
Test Rewards Max                 0.0156327
Test Rewards Min                -0.00053924
Test Returns Mean                0.944387
Test Returns Std                 0.0105606
Test Returns Max                 0.96611
Test Returns Min                 0.932319
Test Actions Mean                0.0190806
Test Actions Std                 0.989779
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        7
Exploration Rewards Mean         0.00760412
Exploration Rewards Std          0.00435789
Exploration Rewards Max          0.0177006
Exploration Rewards Min         -0.00128915
Exploration Returns Mean         0.803864
Exploration Returns Std          0.0557827
Exploration Returns Max          0.899688
Exploration Returns Min          0.75441
Exploration Actions Mean         0.0386517
Exploration Actions Std          0.838204
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.944387
Number of train steps total  50863
Number of env steps total    51000
Number of rollouts total       485
Train Time (s)                  34.1285
(Previous) Eval Time (s)         2.876e-06
Sample Time (s)                 28.3851
Epoch Time (s)                  62.5136
Total Train Time (s)          6092.92
Epoch                           50
---------------------------  ---------------
2018-05-15 23:50:42.527262 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #50 | Epoch Duration: 267.8354604244232
2018-05-15 23:50:42.527372 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #50 | Started Training: True
2018-05-15 23:51:23.695753 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #51 | Epoch Duration: 41.16828513145447
2018-05-15 23:51:23.695926 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #51 | Started Training: True
2018-05-15 23:52:03.792913 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #52 | Epoch Duration: 40.09687376022339
2018-05-15 23:52:03.793138 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #52 | Started Training: True
2018-05-15 23:52:43.016535 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #53 | Epoch Duration: 39.223233461380005
2018-05-15 23:52:43.016735 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #53 | Started Training: True
2018-05-15 23:54:51.935491 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #54 | Epoch Duration: 128.91860795021057
2018-05-15 23:54:51.935668 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #54 | Started Training: True
2018-05-15 23:57:23.531499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #55 | Epoch Duration: 151.59570693969727
2018-05-15 23:57:23.531685 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #55 | Started Training: True
2018-05-15 23:59:54.003697 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #56 | Epoch Duration: 150.47188258171082
2018-05-15 23:59:54.003961 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #56 | Started Training: True
2018-05-16 00:02:24.887425 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #57 | Epoch Duration: 150.8832643032074
2018-05-16 00:02:24.887610 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #57 | Started Training: True
2018-05-16 00:04:54.912542 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #58 | Epoch Duration: 150.02481174468994
2018-05-16 00:04:54.912771 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #58 | Started Training: True
2018-05-16 00:07:25.856622 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #59 | Epoch Duration: 150.943696975708
2018-05-16 00:07:25.856837 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #59 | Started Training: True
2018-05-16 00:10:00.518006 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #60 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          6.89322e-05
Policy Loss                     -0.358302
Raw Policy Loss                 -0.358302
Preactivation Policy Loss        0
Q Predictions Mean               0.355958
Q Predictions Std                0.184794
Q Predictions Max                0.501236
Q Predictions Min               -0.50953
Q Targets Mean                   0.355843
Q Targets Std                    0.187226
Q Targets Max                    0.500363
Q Targets Min                   -0.520141
Bellman Errors Mean              6.89322e-05
Bellman Errors Std               0.000402308
Bellman Errors Max               0.00373279
Bellman Errors Min               3.16555e-10
Policy Action Mean               0.0539871
Policy Action Std                0.984202
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00802601
Test Rewards Std                 0.0041081
Test Rewards Max                 0.0140923
Test Rewards Min                -0.000829074
Test Returns Mean                0.803403
Test Returns Std                 0.00568519
Test Returns Max                 0.812604
Test Returns Min                 0.794329
Test Actions Mean                0.15571
Test Actions Std                 0.973219
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        7
Exploration Rewards Mean         0.00769215
Exploration Rewards Std          0.00368652
Exploration Rewards Max          0.0144895
Exploration Rewards Min         -0.00109318
Exploration Returns Mean         0.958222
Exploration Returns Std          0.0566089
Exploration Returns Max          1.08755
Exploration Returns Min          0.899881
Exploration Actions Mean         0.116891
Exploration Actions Std          0.829127
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.803403
Number of train steps total  60863
Number of env steps total    61000
Number of rollouts total       562
Train Time (s)                  77.8566
(Previous) Eval Time (s)         2.921e-06
Sample Time (s)                 73.0463
Epoch Time (s)                 150.903
Total Train Time (s)          7458.52
Epoch                           60
---------------------------  ---------------
2018-05-16 00:13:28.367342 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #60 | Epoch Duration: 362.51034569740295
2018-05-16 00:13:28.367470 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #60 | Started Training: True
2018-05-16 00:14:09.524568 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #61 | Epoch Duration: 41.15698504447937
2018-05-16 00:14:09.524764 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #61 | Started Training: True
2018-05-16 00:14:50.765044 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #62 | Epoch Duration: 41.24014091491699
2018-05-16 00:14:50.765277 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #62 | Started Training: True
2018-05-16 00:15:30.346662 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #63 | Epoch Duration: 39.58121585845947
2018-05-16 00:15:30.346878 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #63 | Started Training: True
2018-05-16 00:16:11.603811 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #64 | Epoch Duration: 41.25676345825195
2018-05-16 00:16:11.603972 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #64 | Started Training: True
2018-05-16 00:16:51.048530 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #65 | Epoch Duration: 39.444453954696655
2018-05-16 00:16:51.048735 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #65 | Started Training: True
2018-05-16 00:17:31.956957 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #66 | Epoch Duration: 40.90808463096619
2018-05-16 00:17:31.957162 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #66 | Started Training: True
2018-05-16 00:19:52.970316 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #67 | Epoch Duration: 141.0129897594452
2018-05-16 00:19:52.970595 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #67 | Started Training: True
2018-05-16 00:22:24.655734 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #68 | Epoch Duration: 151.68496417999268
2018-05-16 00:22:24.655939 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #68 | Started Training: True
2018-05-16 00:24:56.640977 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #69 | Epoch Duration: 151.9849030971527
2018-05-16 00:24:56.641199 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #69 | Started Training: True
2018-05-16 00:27:30.482086 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #70 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          6.148e-05
Policy Loss                     -0.400095
Raw Policy Loss                 -0.400095
Preactivation Policy Loss        0
Q Predictions Mean               0.397164
Q Predictions Std                0.130293
Q Predictions Max                0.538938
Q Predictions Min                0.014636
Q Targets Mean                   0.398269
Q Targets Std                    0.126851
Q Targets Max                    0.536977
Q Targets Min                    0.00424645
Bellman Errors Mean              6.148e-05
Bellman Errors Std               0.000151149
Bellman Errors Max               0.00150103
Bellman Errors Min               2.34653e-10
Policy Action Mean               0.180367
Policy Action Std                0.977616
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00777552
Test Rewards Std                 0.00352169
Test Rewards Max                 0.013011
Test Rewards Min                -0.000451323
Test Returns Mean                0.998183
Test Returns Std                 0.00339336
Test Returns Max                 1.0022
Test Returns Min                 0.990588
Test Actions Mean                0.164193
Test Actions Std                 0.964899
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        7
Exploration Rewards Mean         0.00762059
Exploration Rewards Std          0.00369051
Exploration Rewards Max          0.0143245
Exploration Rewards Min         -0.00146204
Exploration Returns Mean         0.950397
Exploration Returns Std          0.0543103
Exploration Returns Max          1.05569
Exploration Returns Min          0.860765
Exploration Actions Mean         0.0968972
Exploration Actions Std          0.832905
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.998183
Number of train steps total  70863
Number of env steps total    71000
Number of rollouts total       639
Train Time (s)                  76.0111
(Previous) Eval Time (s)         2.67e-06
Sample Time (s)                 73.776
Epoch Time (s)                 149.787
Total Train Time (s)          8523.38
Epoch                           70
---------------------------  ---------------
2018-05-16 00:31:13.460310 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #70 | Epoch Duration: 376.8189799785614
2018-05-16 00:31:13.460455 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #70 | Started Training: True
2018-05-16 00:32:54.572292 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #71 | Epoch Duration: 101.11171221733093
2018-05-16 00:32:54.572490 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #71 | Started Training: True
2018-05-16 00:33:36.025981 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #72 | Epoch Duration: 41.453359603881836
2018-05-16 00:33:36.026228 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #72 | Started Training: True
2018-05-16 00:34:19.481213 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #73 | Epoch Duration: 43.454777002334595
2018-05-16 00:34:19.481434 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #73 | Started Training: True
2018-05-16 00:35:03.119456 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #74 | Epoch Duration: 43.63786339759827
2018-05-16 00:35:03.119627 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #74 | Started Training: True
2018-05-16 00:35:44.591765 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #75 | Epoch Duration: 41.47202754020691
2018-05-16 00:35:44.591956 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #75 | Started Training: True
2018-05-16 00:36:25.067267 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #76 | Epoch Duration: 40.475191831588745
2018-05-16 00:36:25.067451 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #76 | Started Training: True
2018-05-16 00:37:07.732131 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #77 | Epoch Duration: 42.664554595947266
2018-05-16 00:37:07.732342 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #77 | Started Training: True
2018-05-16 00:39:21.779512 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #78 | Epoch Duration: 134.04702425003052
2018-05-16 00:39:21.779725 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #78 | Started Training: True
2018-05-16 00:41:53.489025 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #79 | Epoch Duration: 151.7091658115387
2018-05-16 00:41:53.489223 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #79 | Started Training: True
2018-05-16 00:44:32.030878 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #80 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          3.73454e-05
Policy Loss                     -0.428219
Raw Policy Loss                 -0.428219
Preactivation Policy Loss        0
Q Predictions Mean               0.425279
Q Predictions Std                0.157892
Q Predictions Max                0.584371
Q Predictions Min               -0.128167
Q Targets Mean                   0.423173
Q Targets Std                    0.160953
Q Targets Max                    0.584654
Q Targets Min                   -0.165533
Bellman Errors Mean              3.73454e-05
Bellman Errors Std               0.000130125
Bellman Errors Max               0.00139621
Bellman Errors Min               9.55538e-09
Policy Action Mean               0.160977
Policy Action Std                0.976175
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00719406
Test Rewards Std                 0.00291549
Test Rewards Max                 0.0139758
Test Rewards Min                -0.00082412
Test Returns Mean                1.05444
Test Returns Std                 0.00494499
Test Returns Max                 1.06159
Test Returns Min                 1.04441
Test Actions Mean                0.170524
Test Actions Std                 0.977915
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        5
Exploration Rewards Mean         0.00680931
Exploration Rewards Std          0.00355656
Exploration Rewards Max          0.0139142
Exploration Rewards Min         -0.00098806
Exploration Returns Mean         1.00233
Exploration Returns Std          0.0934684
Exploration Returns Max          1.16713
Exploration Returns Min          0.886363
Exploration Actions Mean         0.158503
Exploration Actions Std          0.819711
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    1.05444
Number of train steps total  80863
Number of env steps total    81000
Number of rollouts total       698
Train Time (s)                  78.525
(Previous) Eval Time (s)         2.279e-06
Sample Time (s)                 75.6763
Epoch Time (s)                 154.201
Total Train Time (s)          9539.98
Epoch                           80
---------------------------  ---------------
2018-05-16 00:48:10.285767 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #80 | Epoch Duration: 376.79642701148987
2018-05-16 00:48:10.285887 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #80 | Started Training: True
2018-05-16 00:50:43.968660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #81 | Epoch Duration: 153.68267154693604
2018-05-16 00:50:43.968904 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #81 | Started Training: True
2018-05-16 00:52:41.269185 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #82 | Epoch Duration: 117.30012488365173
2018-05-16 00:52:41.269404 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #82 | Started Training: True
2018-05-16 00:53:23.648280 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #83 | Epoch Duration: 42.37872385978699
2018-05-16 00:53:23.648461 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #83 | Started Training: True
2018-05-16 00:54:04.635620 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #84 | Epoch Duration: 40.98703646659851
2018-05-16 00:54:04.635792 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #84 | Started Training: True
2018-05-16 00:54:46.096182 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #85 | Epoch Duration: 41.46026253700256
2018-05-16 00:54:46.096367 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #85 | Started Training: True
2018-05-16 00:55:25.459936 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #86 | Epoch Duration: 39.36344909667969
2018-05-16 00:55:25.460159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #86 | Started Training: True
2018-05-16 00:56:05.860795 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #87 | Epoch Duration: 40.40047788619995
2018-05-16 00:56:05.861061 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #87 | Started Training: True
2018-05-16 00:57:16.623256 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #88 | Epoch Duration: 70.76198434829712
2018-05-16 00:57:16.623433 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #88 | Started Training: True
2018-05-16 00:59:45.805329 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #89 | Epoch Duration: 149.18176913261414
2018-05-16 00:59:45.805521 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #89 | Started Training: True
2018-05-16 01:02:19.091116 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #90 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          9.65927e-05
Policy Loss                     -0.421994
Raw Policy Loss                 -0.421994
Preactivation Policy Loss        0
Q Predictions Mean               0.416986
Q Predictions Std                0.148896
Q Predictions Max                0.583028
Q Predictions Min               -0.0311649
Q Targets Mean                   0.415495
Q Targets Std                    0.146551
Q Targets Max                    0.580746
Q Targets Min                    0.0117187
Bellman Errors Mean              9.65927e-05
Bellman Errors Std               0.00049935
Bellman Errors Max               0.0047749
Bellman Errors Min               8.21959e-10
Policy Action Mean               0.167737
Policy Action Std                0.979232
Policy Action Max                1
Policy Action Min               -1
Test Rewards Mean                0.00821331
Test Rewards Std                 0.00379009
Test Rewards Max                 0.0138605
Test Rewards Min                -0.00061127
Test Returns Mean                0.927191
Test Returns Std                 0.00542242
Test Returns Max                 0.935458
Test Returns Min                 0.920087
Test Actions Mean                0.185816
Test Actions Std                 0.975101
Test Actions Max                 1
Test Actions Min                -1
Num Paths                        8
Exploration Rewards Mean         0.00734501
Exploration Rewards Std          0.00376825
Exploration Rewards Max          0.013642
Exploration Rewards Min         -0.00107497
Exploration Returns Mean         1.05952
Exploration Returns Std          0.0886637
Exploration Returns Max          1.26814
Exploration Returns Min          0.973155
Exploration Actions Mean         0.139938
Exploration Actions Std          0.82465
Exploration Actions Max          1
Exploration Actions Min         -1
AverageReturn                    0.927191
Number of train steps total  90863
Number of env steps total    91000
Number of rollouts total       772
Train Time (s)                  74.9478
(Previous) Eval Time (s)         2.49e-06
Sample Time (s)                 74.031
Epoch Time (s)                 148.979
Total Train Time (s)         10594.6
Epoch                           90
---------------------------  ---------------
2018-05-16 01:05:45.104410 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #90 | Epoch Duration: 359.29877519607544
2018-05-16 01:05:45.104526 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #90 | Started Training: True
2018-05-16 01:08:16.872753 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #91 | Epoch Duration: 151.7681279182434
2018-05-16 01:08:16.872919 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #91 | Started Training: True
2018-05-16 01:10:47.782721 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #92 | Epoch Duration: 150.90968108177185
2018-05-16 01:10:47.782923 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #92 | Started Training: True
2018-05-16 01:12:43.936847 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #93 | Epoch Duration: 116.1537926197052
2018-05-16 01:12:43.937061 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #93 | Started Training: True
2018-05-16 01:13:23.571922 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #94 | Epoch Duration: 39.63471341133118
2018-05-16 01:13:23.572092 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #94 | Started Training: True
2018-05-16 01:14:03.592584 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #95 | Epoch Duration: 40.02038526535034
2018-05-16 01:14:03.592759 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #95 | Started Training: True
2018-05-16 01:14:45.152548 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #96 | Epoch Duration: 41.55968451499939
2018-05-16 01:14:45.152714 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #96 | Started Training: True
2018-05-16 01:15:25.839844 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #97 | Epoch Duration: 40.6870231628418
2018-05-16 01:15:25.840012 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #97 | Started Training: True
2018-05-16 01:16:05.974295 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #98 | Epoch Duration: 40.13416934013367
2018-05-16 01:16:05.974520 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #98 | Started Training: True
2018-05-16 01:16:47.948196 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #99 | Epoch Duration: 41.9735267162323
2018-05-16 01:16:47.948415 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #99 | Started Training: True
2018-05-16 01:17:35.244512 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #100 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.21821e-05
Policy Loss                      -0.400234
Raw Policy Loss                  -0.400234
Preactivation Policy Loss         0
Q Predictions Mean                0.397855
Q Predictions Std                 0.177113
Q Predictions Max                 0.588837
Q Predictions Min                -0.29378
Q Targets Mean                    0.396891
Q Targets Std                     0.17703
Q Targets Max                     0.585145
Q Targets Min                    -0.279375
Bellman Errors Mean               3.21821e-05
Bellman Errors Std                9.35188e-05
Bellman Errors Max                0.000802475
Bellman Errors Min                9.25874e-10
Policy Action Mean                0.192079
Policy Action Std                 0.96842
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00780187
Test Rewards Std                  0.0038142
Test Rewards Max                  0.0138463
Test Rewards Min                 -0.000300086
Test Returns Mean                 0.861327
Test Returns Std                  0.0171099
Test Returns Max                  0.910843
Test Returns Min                  0.845485
Test Actions Mean                 0.121569
Test Actions Std                  0.981914
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00790353
Exploration Rewards Std           0.00406556
Exploration Rewards Max           0.0145668
Exploration Rewards Min          -0.00104762
Exploration Returns Mean          0.99782
Exploration Returns Std           0.0445668
Exploration Returns Max           1.07001
Exploration Returns Min           0.924696
Exploration Actions Mean          0.0977385
Exploration Actions Std           0.831347
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.861327
Number of train steps total  100863
Number of env steps total    101000
Number of rollouts total        853
Train Time (s)                   24.388
(Previous) Eval Time (s)          2.354e-06
Sample Time (s)                  18.5198
Epoch Time (s)                   42.9078
Total Train Time (s)          11517.7
Epoch                           100
---------------------------  ----------------
2018-05-16 01:21:08.431787 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #100 | Epoch Duration: 260.4832320213318
2018-05-16 01:21:08.431897 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #100 | Started Training: True
2018-05-16 01:21:50.085383 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #101 | Epoch Duration: 41.6533887386322
2018-05-16 01:21:50.085572 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #101 | Started Training: True
2018-05-16 01:22:32.199229 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #102 | Epoch Duration: 42.113532304763794
2018-05-16 01:22:32.199432 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #102 | Started Training: True
2018-05-16 01:24:41.751347 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #103 | Epoch Duration: 129.55177569389343
2018-05-16 01:24:41.751539 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #103 | Started Training: True
2018-05-16 01:27:15.101044 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #104 | Epoch Duration: 153.34937834739685
2018-05-16 01:27:15.101265 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #104 | Started Training: True
2018-05-16 01:29:48.323046 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #105 | Epoch Duration: 153.2216351032257
2018-05-16 01:29:48.323212 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #105 | Started Training: True
2018-05-16 01:32:20.080762 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #106 | Epoch Duration: 151.75744366645813
2018-05-16 01:32:20.080918 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #106 | Started Training: True
2018-05-16 01:34:51.459602 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #107 | Epoch Duration: 151.37857460975647
2018-05-16 01:34:51.459832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #107 | Started Training: True
2018-05-16 01:37:23.816047 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #108 | Epoch Duration: 152.35606288909912
2018-05-16 01:37:23.816238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #108 | Started Training: True
2018-05-16 01:39:55.734125 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #109 | Epoch Duration: 151.9177691936493
2018-05-16 01:39:55.734286 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #109 | Started Training: True
2018-05-16 01:42:30.599877 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #110 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.92728e-05
Policy Loss                      -0.431519
Raw Policy Loss                  -0.431519
Preactivation Policy Loss         0
Q Predictions Mean                0.429354
Q Predictions Std                 0.182634
Q Predictions Max                 0.615859
Q Predictions Min                -0.342678
Q Targets Mean                    0.425888
Q Targets Std                     0.182909
Q Targets Max                     0.611446
Q Targets Min                    -0.341676
Bellman Errors Mean               5.92728e-05
Bellman Errors Std                0.000273899
Bellman Errors Max                0.00279107
Bellman Errors Min                1.60674e-09
Policy Action Mean                0.0978017
Policy Action Std                 0.984379
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00826665
Test Rewards Std                  0.00361766
Test Rewards Max                  0.0138658
Test Rewards Min                 -0.00017041
Test Returns Mean                 1.02047
Test Returns Std                  0.0157278
Test Returns Max                  1.04597
Test Returns Min                  1.00541
Test Actions Mean                 0.101374
Test Actions Std                  0.982787
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00774245
Exploration Rewards Std           0.00363545
Exploration Rewards Max           0.0135717
Exploration Rewards Min          -0.00111749
Exploration Returns Mean          1.022
Exploration Returns Std           0.05821
Exploration Returns Max           1.13068
Exploration Returns Min           0.962393
Exploration Actions Mean          0.100943
Exploration Actions Std           0.831567
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.02047
Number of train steps total  110863
Number of env steps total    111000
Number of rollouts total        935
Train Time (s)                   76.6194
(Previous) Eval Time (s)          1.871e-06
Sample Time (s)                  73.4618
Epoch Time (s)                  150.081
Total Train Time (s)          13015.5
Epoch                           110
---------------------------  ----------------
2018-05-16 01:46:06.534281 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #110 | Epoch Duration: 370.7992763519287
2018-05-16 01:46:06.534413 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #110 | Started Training: True
2018-05-16 01:46:47.938848 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #111 | Epoch Duration: 41.404319763183594
2018-05-16 01:46:47.939059 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #111 | Started Training: True
2018-05-16 01:47:27.224654 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #112 | Epoch Duration: 39.28544235229492
2018-05-16 01:47:27.224870 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #112 | Started Training: True
2018-05-16 01:48:07.790932 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #113 | Epoch Duration: 40.565897703170776
2018-05-16 01:48:07.791173 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #113 | Started Training: True
2018-05-16 01:48:48.752469 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #114 | Epoch Duration: 40.96112132072449
2018-05-16 01:48:48.752679 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #114 | Started Training: True
2018-05-16 01:49:29.255002 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #115 | Epoch Duration: 40.502174854278564
2018-05-16 01:49:29.255166 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #115 | Started Training: True
2018-05-16 01:50:10.575294 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #116 | Epoch Duration: 41.32001304626465
2018-05-16 01:50:10.575547 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #116 | Started Training: True
2018-05-16 01:50:50.745391 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #117 | Epoch Duration: 40.169644832611084
2018-05-16 01:50:50.745603 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #117 | Started Training: True
2018-05-16 01:51:31.544739 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #118 | Epoch Duration: 40.79896831512451
2018-05-16 01:51:31.544894 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #118 | Started Training: True
2018-05-16 01:52:11.712768 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #119 | Epoch Duration: 40.16776394844055
2018-05-16 01:52:11.712980 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #119 | Started Training: True
2018-05-16 01:53:41.310794 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #120 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000154758
Policy Loss                      -0.423925
Raw Policy Loss                  -0.423925
Preactivation Policy Loss         0
Q Predictions Mean                0.421353
Q Predictions Std                 0.160699
Q Predictions Max                 0.598164
Q Predictions Min                -0.264609
Q Targets Mean                    0.415755
Q Targets Std                     0.162259
Q Targets Max                     0.591356
Q Targets Min                    -0.274161
Bellman Errors Mean               0.000154758
Bellman Errors Std                0.00129004
Bellman Errors Max                0.0146764
Bellman Errors Min                9.64352e-10
Policy Action Mean                0.0964718
Policy Action Std                 0.984326
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00793233
Test Rewards Std                  0.00355412
Test Rewards Max                  0.0138383
Test Rewards Min                 -0.000552382
Test Returns Mean                 0.93866
Test Returns Std                  0.0312999
Test Returns Max                  0.986183
Test Returns Min                  0.902235
Test Actions Mean                 0.119521
Test Actions Std                  0.980227
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00769013
Exploration Rewards Std           0.00350115
Exploration Rewards Max           0.0140029
Exploration Rewards Min          -0.0004797
Exploration Returns Mean          0.983055
Exploration Returns Std           0.109743
Exploration Returns Max           1.14691
Exploration Returns Min           0.783741
Exploration Actions Mean          0.0721447
Exploration Actions Std           0.840761
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.93866
Number of train steps total  120863
Number of env steps total    121000
Number of rollouts total       1014
Train Time (s)                   44.2861
(Previous) Eval Time (s)          2.091e-06
Sample Time (s)                  40.0075
Epoch Time (s)                   84.2936
Total Train Time (s)          13691.7
Epoch                           120
---------------------------  ----------------
2018-05-16 01:57:22.888442 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #120 | Epoch Duration: 311.1753177642822
2018-05-16 01:57:22.888584 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #120 | Started Training: True
2018-05-16 01:59:57.027328 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #121 | Epoch Duration: 154.13862347602844
2018-05-16 01:59:57.027606 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #121 | Started Training: True
2018-05-16 02:02:26.927288 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #122 | Epoch Duration: 149.89948320388794
2018-05-16 02:02:26.927465 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #122 | Started Training: True
2018-05-16 02:04:57.548537 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #123 | Epoch Duration: 150.62095260620117
2018-05-16 02:04:57.548765 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #123 | Started Training: True
2018-05-16 02:07:30.307614 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #124 | Epoch Duration: 152.75869512557983
2018-05-16 02:07:30.307807 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #124 | Started Training: True
2018-05-16 02:08:10.401679 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #125 | Epoch Duration: 40.09373927116394
2018-05-16 02:08:10.402327 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #125 | Started Training: True
2018-05-16 02:08:50.832471 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #126 | Epoch Duration: 40.42995357513428
2018-05-16 02:08:50.832627 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #126 | Started Training: True
2018-05-16 02:09:30.481864 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #127 | Epoch Duration: 39.64912796020508
2018-05-16 02:09:30.482042 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #127 | Started Training: True
2018-05-16 02:10:10.601038 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #128 | Epoch Duration: 40.11887812614441
2018-05-16 02:10:10.601253 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #128 | Started Training: True
2018-05-16 02:10:49.872986 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #129 | Epoch Duration: 39.27157783508301
2018-05-16 02:10:49.873227 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #129 | Started Training: True
2018-05-16 02:13:00.658064 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #130 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           2.11126e-05
Policy Loss                      -0.407083
Raw Policy Loss                  -0.407083
Preactivation Policy Loss         0
Q Predictions Mean                0.405041
Q Predictions Std                 0.183569
Q Predictions Max                 0.573216
Q Predictions Min                -0.281495
Q Targets Mean                    0.406634
Q Targets Std                     0.183618
Q Targets Max                     0.580279
Q Targets Min                    -0.267537
Bellman Errors Mean               2.11126e-05
Bellman Errors Std                4.51837e-05
Bellman Errors Max                0.000299217
Bellman Errors Min                4.93423e-09
Policy Action Mean                0.146901
Policy Action Std                 0.975025
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00828514
Test Rewards Std                  0.00371567
Test Rewards Max                  0.0137457
Test Rewards Min                 -0.000498003
Test Returns Mean                 0.971202
Test Returns Std                  0.00376688
Test Returns Max                  0.977411
Test Returns Min                  0.96601
Test Actions Mean                 0.0872038
Test Actions Std                  0.985698
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00797232
Exploration Rewards Std           0.00429868
Exploration Rewards Max           0.0170421
Exploration Rewards Min          -0.00157804
Exploration Returns Mean          0.937744
Exploration Returns Std           0.0809048
Exploration Returns Max           1.12614
Exploration Returns Min           0.82794
Exploration Actions Mean          0.0729989
Exploration Actions Std           0.83273
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.971202
Number of train steps total  130863
Number of env steps total    131000
Number of rollouts total       1097
Train Time (s)                   65.6482
(Previous) Eval Time (s)          3.037e-06
Sample Time (s)                  59.9609
Epoch Time (s)                  125.609
Total Train Time (s)          14841.9
Epoch                           130
---------------------------  ----------------
2018-05-16 02:16:33.389036 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #130 | Epoch Duration: 343.51565289497375
2018-05-16 02:16:33.389162 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #130 | Started Training: True
2018-05-16 02:19:05.485455 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #131 | Epoch Duration: 152.09618377685547
2018-05-16 02:19:05.485639 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #131 | Started Training: True
2018-05-16 02:21:34.735791 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #132 | Epoch Duration: 149.25002217292786
2018-05-16 02:21:34.735979 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #132 | Started Training: True
2018-05-16 02:24:04.689023 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #133 | Epoch Duration: 149.95290875434875
2018-05-16 02:24:04.689210 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #133 | Started Training: True
2018-05-16 02:26:35.897405 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #134 | Epoch Duration: 151.20807433128357
2018-05-16 02:26:35.897641 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #134 | Started Training: True
2018-05-16 02:27:20.205386 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #135 | Epoch Duration: 44.307591676712036
2018-05-16 02:27:20.205599 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #135 | Started Training: True
2018-05-16 02:28:00.470287 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #136 | Epoch Duration: 40.26455354690552
2018-05-16 02:28:00.470526 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #136 | Started Training: True
2018-05-16 02:28:40.975737 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #137 | Epoch Duration: 40.50504732131958
2018-05-16 02:28:40.975933 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #137 | Started Training: True
2018-05-16 02:29:21.091211 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #138 | Epoch Duration: 40.11512327194214
2018-05-16 02:29:21.091367 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #138 | Started Training: True
2018-05-16 02:30:58.168481 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #139 | Epoch Duration: 97.07700896263123
2018-05-16 02:30:58.168660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #139 | Started Training: True
2018-05-16 02:33:34.572178 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #140 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           2.70931e-05
Policy Loss                      -0.443062
Raw Policy Loss                  -0.443062
Preactivation Policy Loss         0
Q Predictions Mean                0.441082
Q Predictions Std                 0.183408
Q Predictions Max                 0.590163
Q Predictions Min                -0.402058
Q Targets Mean                    0.443005
Q Targets Std                     0.181478
Q Targets Max                     0.595438
Q Targets Min                    -0.37347
Bellman Errors Mean               2.70931e-05
Bellman Errors Std                9.59569e-05
Bellman Errors Max                0.000817284
Bellman Errors Min                2.44157e-09
Policy Action Mean                0.106556
Policy Action Std                 0.980874
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00832227
Test Rewards Std                  0.00389111
Test Rewards Max                  0.0139106
Test Rewards Min                 -0.00071607
Test Returns Mean                 0.965383
Test Returns Std                  0.00313803
Test Returns Max                  0.968881
Test Returns Min                  0.957567
Test Actions Mean                 0.150861
Test Actions Std                  0.975933
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00779817
Exploration Rewards Std           0.00410624
Exploration Rewards Max           0.0152726
Exploration Rewards Min          -0.00107423
Exploration Returns Mean          0.907512
Exploration Returns Std           0.0826144
Exploration Returns Max           1.06172
Exploration Returns Min           0.766991
Exploration Actions Mean          0.112613
Exploration Actions Std           0.821014
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.965383
Number of train steps total  140863
Number of env steps total    141000
Number of rollouts total       1182
Train Time (s)                   77.1619
(Previous) Eval Time (s)          2.449e-06
Sample Time (s)                  73.9324
Epoch Time (s)                  151.094
Total Train Time (s)          16075
Epoch                           140
---------------------------  ----------------
2018-05-16 02:37:06.717941 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #140 | Epoch Duration: 368.5491609573364
2018-05-16 02:37:06.718066 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #140 | Started Training: True
2018-05-16 02:39:39.058438 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #141 | Epoch Duration: 152.34025263786316
2018-05-16 02:39:39.058616 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #141 | Started Training: True
2018-05-16 02:42:13.022838 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #142 | Epoch Duration: 153.964106798172
2018-05-16 02:42:13.023029 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #142 | Started Training: True
2018-05-16 02:44:43.566413 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #143 | Epoch Duration: 150.54321432113647
2018-05-16 02:44:43.566603 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #143 | Started Training: True
2018-05-16 02:45:50.149455 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #144 | Epoch Duration: 66.58271908760071
2018-05-16 02:45:50.149671 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #144 | Started Training: True
2018-05-16 02:46:30.688240 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #145 | Epoch Duration: 40.538421392440796
2018-05-16 02:46:30.688439 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #145 | Started Training: True
2018-05-16 02:47:11.854570 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #146 | Epoch Duration: 41.16599988937378
2018-05-16 02:47:11.854799 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #146 | Started Training: True
2018-05-16 02:47:52.328533 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #147 | Epoch Duration: 40.47358536720276
2018-05-16 02:47:52.328745 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #147 | Started Training: True
2018-05-16 02:48:33.944141 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #148 | Epoch Duration: 41.61524558067322
2018-05-16 02:48:33.944316 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #148 | Started Training: True
2018-05-16 02:49:56.599764 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #149 | Epoch Duration: 82.65532732009888
2018-05-16 02:49:56.599971 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #149 | Started Training: True
2018-05-16 02:52:38.607412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #150 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           1.35622e-05
Policy Loss                      -0.434899
Raw Policy Loss                  -0.434899
Preactivation Policy Loss         0
Q Predictions Mean                0.433466
Q Predictions Std                 0.159131
Q Predictions Max                 0.596561
Q Predictions Min                 0.00521065
Q Targets Mean                    0.432523
Q Targets Std                     0.159611
Q Targets Max                     0.596667
Q Targets Min                     0.000284822
Bellman Errors Mean               1.35622e-05
Bellman Errors Std                3.82045e-05
Bellman Errors Max                0.000288447
Bellman Errors Min                1.82641e-09
Policy Action Mean                0.160672
Policy Action Std                 0.977126
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00887829
Test Rewards Std                  0.00475971
Test Rewards Max                  0.0181052
Test Rewards Min                  0.000176506
Test Returns Mean                 1.14308
Test Returns Std                  0.0295476
Test Returns Max                  1.18419
Test Returns Min                  1.10113
Test Actions Mean                 0.191941
Test Actions Std                  0.953277
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00756607
Exploration Rewards Std           0.00431329
Exploration Rewards Max           0.0167126
Exploration Rewards Min          -0.00132152
Exploration Returns Mean          0.920538
Exploration Returns Std           0.132607
Exploration Returns Max           1.19694
Exploration Returns Min           0.80043
Exploration Actions Mean          0.134078
Exploration Actions Std           0.816428
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.14308
Number of train steps total  150863
Number of env steps total    151000
Number of rollouts total       1261
Train Time (s)                   79.7824
(Previous) Eval Time (s)          2.205e-06
Sample Time (s)                  76.4183
Epoch Time (s)                  156.201
Total Train Time (s)          17252.1
Epoch                           150
---------------------------  ----------------
2018-05-16 02:56:44.057164 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #150 | Epoch Duration: 407.45706701278687
2018-05-16 02:56:44.057308 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #150 | Started Training: True
2018-05-16 02:59:23.011299 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #151 | Epoch Duration: 158.9538698196411
2018-05-16 02:59:23.011567 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #151 | Started Training: True
2018-05-16 03:01:53.811637 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #152 | Epoch Duration: 150.79986000061035
2018-05-16 03:01:53.811866 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #152 | Started Training: True
2018-05-16 03:03:03.845780 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #153 | Epoch Duration: 70.03376460075378
2018-05-16 03:03:03.845944 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #153 | Started Training: True
2018-05-16 03:03:45.927499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #154 | Epoch Duration: 42.08144688606262
2018-05-16 03:03:45.927704 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #154 | Started Training: True
2018-05-16 03:04:29.115698 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #155 | Epoch Duration: 43.18784499168396
2018-05-16 03:04:29.115875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #155 | Started Training: True
2018-05-16 03:05:13.467506 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #156 | Epoch Duration: 44.35150098800659
2018-05-16 03:05:13.467666 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #156 | Started Training: True
2018-05-16 03:06:15.996493 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #157 | Epoch Duration: 62.52871751785278
2018-05-16 03:06:15.996673 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #157 | Started Training: True
2018-05-16 03:08:49.036930 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #158 | Epoch Duration: 153.04014110565186
2018-05-16 03:08:49.037127 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #158 | Started Training: True
2018-05-16 03:11:20.084147 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #159 | Epoch Duration: 151.04688787460327
2018-05-16 03:11:20.084309 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #159 | Started Training: True
2018-05-16 03:14:01.461480 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #160 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           2.47937e-05
Policy Loss                      -0.418746
Raw Policy Loss                  -0.418746
Preactivation Policy Loss         0
Q Predictions Mean                0.416243
Q Predictions Std                 0.190574
Q Predictions Max                 0.610853
Q Predictions Min                -0.423522
Q Targets Mean                    0.415789
Q Targets Std                     0.191012
Q Targets Max                     0.61093
Q Targets Min                    -0.419542
Bellman Errors Mean               2.47937e-05
Bellman Errors Std                7.72475e-05
Bellman Errors Max                0.000668918
Bellman Errors Min                6.73809e-10
Policy Action Mean                0.203312
Policy Action Std                 0.949315
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00881321
Test Rewards Std                  0.00405237
Test Rewards Max                  0.0165995
Test Rewards Min                  0.000278553
Test Returns Mean                 1.09578
Test Returns Std                  0.0236742
Test Returns Max                  1.12903
Test Returns Min                  1.05866
Test Actions Mean                 0.159412
Test Actions Std                  0.970453
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         4
Exploration Rewards Mean          0.004628
Exploration Rewards Std           0.00414256
Exploration Rewards Max           0.0152063
Exploration Rewards Min          -0.00265778
Exploration Returns Mean          0.911717
Exploration Returns Std           0.0804445
Exploration Returns Max           1.01439
Exploration Returns Min           0.802284
Exploration Actions Mean          0.166729
Exploration Actions Std           0.810309
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.09578
Number of train steps total  160863
Number of env steps total    161000
Number of rollouts total       1322
Train Time (s)                   81.2387
(Previous) Eval Time (s)          2.124e-06
Sample Time (s)                  74.3165
Epoch Time (s)                  155.555
Total Train Time (s)          18576.4
Epoch                           160
---------------------------  ----------------
2018-05-16 03:18:48.565607 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #160 | Epoch Duration: 448.48120069503784
2018-05-16 03:18:48.565721 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #160 | Started Training: True
2018-05-16 03:19:30.132917 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #161 | Epoch Duration: 41.56710171699524
2018-05-16 03:19:30.133086 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #161 | Started Training: True
2018-05-16 03:20:10.789322 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #162 | Epoch Duration: 40.656116247177124
2018-05-16 03:20:10.789476 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #162 | Started Training: True
2018-05-16 03:20:53.420790 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #163 | Epoch Duration: 42.63120245933533
2018-05-16 03:20:53.421018 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #163 | Started Training: True
2018-05-16 03:21:34.868989 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #164 | Epoch Duration: 41.447808027267456
2018-05-16 03:21:34.869202 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #164 | Started Training: True
2018-05-16 03:22:17.220336 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #165 | Epoch Duration: 42.350982666015625
2018-05-16 03:22:17.220511 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #165 | Started Training: True
2018-05-16 03:23:02.643062 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #166 | Epoch Duration: 45.422433376312256
2018-05-16 03:23:02.643220 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #166 | Started Training: True
2018-05-16 03:23:44.264121 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #167 | Epoch Duration: 41.620798110961914
2018-05-16 03:23:44.264284 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #167 | Started Training: True
2018-05-16 03:25:53.465828 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #168 | Epoch Duration: 129.20143365859985
2018-05-16 03:25:53.465992 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #168 | Started Training: True
2018-05-16 03:28:26.414309 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #169 | Epoch Duration: 152.9482023715973
2018-05-16 03:28:26.414566 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #169 | Started Training: True
2018-05-16 03:31:04.389971 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #170 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000115784
Policy Loss                      -0.518185
Raw Policy Loss                  -0.518185
Preactivation Policy Loss         0
Q Predictions Mean                0.514834
Q Predictions Std                 0.155581
Q Predictions Max                 0.706901
Q Predictions Min                -0.00386679
Q Targets Mean                    0.51336
Q Targets Std                     0.158707
Q Targets Max                     0.701526
Q Targets Min                    -0.0091261
Bellman Errors Mean               0.000115784
Bellman Errors Std                0.000856816
Bellman Errors Max                0.00969597
Bellman Errors Min                1.08645e-09
Policy Action Mean                0.142297
Policy Action Std                 0.973707
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00668935
Test Rewards Std                  0.0033287
Test Rewards Max                  0.0142852
Test Rewards Min                  0.000231296
Test Returns Mean                 1.41012
Test Returns Std                  0.00365196
Test Returns Max                  1.41568
Test Returns Min                  1.40589
Test Actions Mean                 0.278784
Test Actions Std                  0.935627
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00742683
Exploration Rewards Std           0.00377364
Exploration Rewards Max           0.0157604
Exploration Rewards Min          -0.000249528
Exploration Returns Mean          1.34129
Exploration Returns Std           0.0646954
Exploration Returns Max           1.40393
Exploration Returns Min           1.22266
Exploration Actions Mean          0.210146
Exploration Actions Std           0.808174
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.41012
Number of train steps total  170863
Number of env steps total    171000
Number of rollouts total       1384
Train Time (s)                   77.9505
(Previous) Eval Time (s)          2.496e-06
Sample Time (s)                  74.0089
Epoch Time (s)                  151.959
Total Train Time (s)          19545.1
Epoch                           170
---------------------------  ----------------
2018-05-16 03:34:57.464093 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #170 | Epoch Duration: 391.04937839508057
2018-05-16 03:34:57.464217 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #170 | Started Training: True
2018-05-16 03:37:33.094920 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #171 | Epoch Duration: 155.6305992603302
2018-05-16 03:37:33.095087 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #171 | Started Training: True
2018-05-16 03:38:28.082046 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #172 | Epoch Duration: 54.98685312271118
2018-05-16 03:38:28.082224 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #172 | Started Training: True
2018-05-16 03:39:10.893029 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #173 | Epoch Duration: 42.81068468093872
2018-05-16 03:39:10.893286 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #173 | Started Training: True
2018-05-16 03:39:52.754691 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #174 | Epoch Duration: 41.86121129989624
2018-05-16 03:39:52.754853 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #174 | Started Training: True
2018-05-16 03:40:34.972660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #175 | Epoch Duration: 42.21769881248474
2018-05-16 03:40:34.972848 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #175 | Started Training: True
2018-05-16 03:41:18.058445 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #176 | Epoch Duration: 43.085474252700806
2018-05-16 03:41:18.058631 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #176 | Started Training: True
2018-05-16 03:42:00.733713 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #177 | Epoch Duration: 42.67495942115784
2018-05-16 03:42:00.733920 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #177 | Started Training: True
2018-05-16 03:42:41.169384 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #178 | Epoch Duration: 40.43530511856079
2018-05-16 03:42:41.169596 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #178 | Started Training: True
2018-05-16 03:43:21.454974 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #179 | Epoch Duration: 40.285224199295044
2018-05-16 03:43:21.455237 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #179 | Started Training: True
2018-05-16 03:44:09.670727 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #180 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.43818e-05
Policy Loss                      -0.547189
Raw Policy Loss                  -0.547189
Preactivation Policy Loss         0
Q Predictions Mean                0.542626
Q Predictions Std                 0.189913
Q Predictions Max                 0.728343
Q Predictions Min                -0.0741726
Q Targets Mean                    0.546997
Q Targets Std                     0.189242
Q Targets Max                     0.741578
Q Targets Min                    -0.0709272
Bellman Errors Mean               5.43818e-05
Bellman Errors Std                9.17766e-05
Bellman Errors Max                0.000681383
Bellman Errors Min                2.63676e-09
Policy Action Mean                0.326517
Policy Action Std                 0.922823
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00625355
Test Rewards Std                  0.00390385
Test Rewards Max                  0.0136207
Test Rewards Min                 -0.000742078
Test Returns Mean                 0.982701
Test Returns Std                  0.0338773
Test Returns Max                  1.02531
Test Returns Min                  0.935426
Test Actions Mean                 0.131311
Test Actions Std                  0.980179
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.000862757
Exploration Rewards Std           0.00566502
Exploration Rewards Max           0.0159126
Exploration Rewards Min          -0.0193629
Exploration Returns Mean          0.274932
Exploration Returns Std           1.07583
Exploration Returns Max           1.43159
Exploration Returns Min          -0.803499
Exploration Actions Mean          0.077402
Exploration Actions Std           0.826405
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.982701
Number of train steps total  180863
Number of env steps total    181000
Number of rollouts total       1426
Train Time (s)                   23.8868
(Previous) Eval Time (s)          2.964e-06
Sample Time (s)                  18.0609
Epoch Time (s)                   41.9477
Total Train Time (s)          20361.6
Epoch                           180
---------------------------  ----------------
2018-05-16 03:48:34.143673 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #180 | Epoch Duration: 312.6882426738739
2018-05-16 03:48:34.143797 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #180 | Started Training: True
2018-05-16 03:51:10.087253 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #181 | Epoch Duration: 155.943350315094
2018-05-16 03:51:10.087456 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #181 | Started Training: True
2018-05-16 03:53:42.060030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #182 | Epoch Duration: 151.97244358062744
2018-05-16 03:53:42.060220 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #182 | Started Training: True
2018-05-16 03:56:16.243648 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #183 | Epoch Duration: 154.1832993030548
2018-05-16 03:56:16.243881 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #183 | Started Training: True
2018-05-16 03:58:51.764553 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #184 | Epoch Duration: 155.5205192565918
2018-05-16 03:58:51.764737 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #184 | Started Training: True
2018-05-16 04:01:11.508277 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #185 | Epoch Duration: 139.74341416358948
2018-05-16 04:01:11.508494 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #185 | Started Training: True
2018-05-16 04:01:53.221106 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #186 | Epoch Duration: 41.71245050430298
2018-05-16 04:01:53.221320 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #186 | Started Training: True
2018-05-16 04:02:35.037100 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #187 | Epoch Duration: 41.815613746643066
2018-05-16 04:02:35.037318 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #187 | Started Training: True
2018-05-16 04:03:17.264282 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #188 | Epoch Duration: 42.226813554763794
2018-05-16 04:03:17.264470 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #188 | Started Training: True
2018-05-16 04:04:00.211772 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #189 | Epoch Duration: 42.947182416915894
2018-05-16 04:04:00.211950 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #189 | Started Training: True
2018-05-16 04:04:49.153221 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #190 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.04533e-05
Policy Loss                      -0.525669
Raw Policy Loss                  -0.525669
Preactivation Policy Loss         0
Q Predictions Mean                0.52276
Q Predictions Std                 0.181123
Q Predictions Max                 0.683642
Q Predictions Min                 0.0401345
Q Targets Mean                    0.52639
Q Targets Std                     0.182189
Q Targets Max                     0.692932
Q Targets Min                     0.032765
Bellman Errors Mean               5.04533e-05
Bellman Errors Std                7.29376e-05
Bellman Errors Max                0.000386227
Bellman Errors Min                1.12564e-10
Policy Action Mean                0.159091
Policy Action Std                 0.971368
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00836174
Test Rewards Std                  0.00375193
Test Rewards Max                  0.0148743
Test Rewards Min                 -0.00090581
Test Returns Mean                 1.24351
Test Returns Std                  0.012118
Test Returns Max                  1.25312
Test Returns Min                  1.21609
Test Actions Mean                 0.243779
Test Actions Std                  0.953217
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         4
Exploration Rewards Mean          0.00617491
Exploration Rewards Std           0.00378113
Exploration Rewards Max           0.0152259
Exploration Rewards Min          -0.00166077
Exploration Returns Mean          1.2566
Exploration Returns Std           0.150305
Exploration Returns Max           1.38735
Exploration Returns Min           1.01055
Exploration Actions Mean          0.241043
Exploration Actions Std           0.800492
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.24351
Number of train steps total  190863
Number of env steps total    191000
Number of rollouts total       1481
Train Time (s)                   24.5303
(Previous) Eval Time (s)          2.302e-06
Sample Time (s)                  18.1786
Epoch Time (s)                   42.7089
Total Train Time (s)          21566.4
Epoch                           190
---------------------------  ----------------
2018-05-16 04:08:39.260313 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #190 | Epoch Duration: 279.04824352264404
2018-05-16 04:08:39.260436 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #190 | Started Training: True
2018-05-16 04:11:18.578698 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #191 | Epoch Duration: 159.31815695762634
2018-05-16 04:11:18.578877 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #191 | Started Training: True
2018-05-16 04:13:49.456992 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #192 | Epoch Duration: 150.87799644470215
2018-05-16 04:13:49.457163 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #192 | Started Training: True
2018-05-16 04:16:23.036898 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #193 | Epoch Duration: 153.57960772514343
2018-05-16 04:16:23.037092 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #193 | Started Training: True
2018-05-16 04:18:58.528960 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #194 | Epoch Duration: 155.49174189567566
2018-05-16 04:18:58.529129 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #194 | Started Training: True
2018-05-16 04:21:33.353614 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #195 | Epoch Duration: 154.82437896728516
2018-05-16 04:21:33.353809 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #195 | Started Training: True
2018-05-16 04:24:08.653080 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #196 | Epoch Duration: 155.29915022850037
2018-05-16 04:24:08.653310 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #196 | Started Training: True
2018-05-16 04:26:40.763953 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #197 | Epoch Duration: 152.11049032211304
2018-05-16 04:26:40.764146 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #197 | Started Training: True
2018-05-16 04:29:13.928870 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #198 | Epoch Duration: 153.16459369659424
2018-05-16 04:29:13.929063 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #198 | Started Training: True
2018-05-16 04:31:50.981042 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #199 | Epoch Duration: 157.05185079574585
2018-05-16 04:31:50.981231 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #199 | Started Training: True
2018-05-16 04:33:11.275243 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #200 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.58138e-05
Policy Loss                      -0.523728
Raw Policy Loss                  -0.523728
Preactivation Policy Loss         0
Q Predictions Mean                0.521221
Q Predictions Std                 0.173057
Q Predictions Max                 0.647536
Q Predictions Min                -0.318712
Q Targets Mean                    0.522492
Q Targets Std                     0.17393
Q Targets Max                     0.651697
Q Targets Min                    -0.325473
Bellman Errors Mean               3.58138e-05
Bellman Errors Std                0.000166863
Bellman Errors Max                0.00186435
Bellman Errors Min                2.43872e-10
Policy Action Mean                0.189669
Policy Action Std                 0.96805
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00877233
Test Rewards Std                  0.00421169
Test Rewards Max                  0.0152746
Test Rewards Min                 -0.000360463
Test Returns Mean                 1.22484
Test Returns Std                  0.00856769
Test Returns Max                  1.2378
Test Returns Min                  1.21449
Test Actions Mean                 0.261744
Test Actions Std                  0.951279
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00842832
Exploration Rewards Std           0.00420495
Exploration Rewards Max           0.0168909
Exploration Rewards Min          -0.00105429
Exploration Returns Mean          1.1912
Exploration Returns Std           0.104833
Exploration Returns Max           1.37508
Exploration Returns Min           1.05781
Exploration Actions Mean          0.187245
Exploration Actions Std           0.808732
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.22484
Number of train steps total  200863
Number of env steps total    201000
Number of rollouts total       1552
Train Time (s)                   39.2351
(Previous) Eval Time (s)          2.396e-06
Sample Time (s)                  34.7357
Epoch Time (s)                   73.9709
Total Train Time (s)          23274.5
Epoch                           200
---------------------------  ----------------
2018-05-16 04:37:07.579121 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #200 | Epoch Duration: 316.5977704524994
2018-05-16 04:37:07.579230 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #200 | Started Training: True
2018-05-16 04:37:50.209327 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #201 | Epoch Duration: 42.62999868392944
2018-05-16 04:37:50.209546 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #201 | Started Training: True
2018-05-16 04:38:31.041742 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #202 | Epoch Duration: 40.832038164138794
2018-05-16 04:38:31.041953 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #202 | Started Training: True
2018-05-16 04:39:13.399938 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #203 | Epoch Duration: 42.35782551765442
2018-05-16 04:39:13.400214 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #203 | Started Training: True
2018-05-16 04:39:55.316501 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #204 | Epoch Duration: 41.916072368621826
2018-05-16 04:39:55.316698 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #204 | Started Training: True
2018-05-16 04:40:50.511953 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #205 | Epoch Duration: 55.195125102996826
2018-05-16 04:40:50.512157 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #205 | Started Training: True
2018-05-16 04:43:23.332683 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #206 | Epoch Duration: 152.82036781311035
2018-05-16 04:43:23.332914 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #206 | Started Training: True
2018-05-16 04:45:57.305086 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #207 | Epoch Duration: 153.97201657295227
2018-05-16 04:45:57.305283 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #207 | Started Training: True
2018-05-16 04:48:34.937067 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #208 | Epoch Duration: 157.6316545009613
2018-05-16 04:48:34.937260 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #208 | Started Training: True
2018-05-16 04:51:11.342518 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #209 | Epoch Duration: 156.40513443946838
2018-05-16 04:51:11.342684 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #209 | Started Training: True
2018-05-16 04:53:52.508329 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #210 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.89273e-05
Policy Loss                      -0.487916
Raw Policy Loss                  -0.487916
Preactivation Policy Loss         0
Q Predictions Mean                0.485009
Q Predictions Std                 0.20575
Q Predictions Max                 0.634573
Q Predictions Min                -0.326778
Q Targets Mean                    0.482154
Q Targets Std                     0.207091
Q Targets Max                     0.640602
Q Targets Min                    -0.335176
Bellman Errors Mean               3.89273e-05
Bellman Errors Std                0.000109221
Bellman Errors Max                0.00114712
Bellman Errors Min                1.7985e-09
Policy Action Mean                0.232196
Policy Action Std                 0.95381
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00767698
Test Rewards Std                  0.00342462
Test Rewards Max                  0.0141329
Test Rewards Min                 -0.000494433
Test Returns Mean                 1.29357
Test Returns Std                  0.0170929
Test Returns Max                  1.31622
Test Returns Min                  1.26629
Test Actions Mean                 0.270927
Test Actions Std                  0.936007
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00739322
Exploration Rewards Std           0.00362971
Exploration Rewards Max           0.0151536
Exploration Rewards Min          -0.000949064
Exploration Returns Mean          1.17869
Exploration Returns Std           0.133293
Exploration Returns Max           1.41499
Exploration Returns Min           1.01674
Exploration Actions Mean          0.174685
Exploration Actions Std           0.819323
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.29357
Number of train steps total  210863
Number of env steps total    211000
Number of rollouts total       1622
Train Time (s)                   78.7725
(Previous) Eval Time (s)          2.07e-06
Sample Time (s)                  75.2285
Epoch Time (s)                  154.001
Total Train Time (s)          24501.5
Epoch                           210
---------------------------  ----------------
2018-05-16 04:57:34.815735 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #210 | Epoch Duration: 383.47293853759766
2018-05-16 04:57:34.815846 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #210 | Started Training: True
2018-05-16 04:58:16.763626 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #211 | Epoch Duration: 41.94768476486206
2018-05-16 04:58:16.763792 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #211 | Started Training: True
2018-05-16 04:58:58.824376 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #212 | Epoch Duration: 42.06047034263611
2018-05-16 04:58:58.824590 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #212 | Started Training: True
2018-05-16 04:59:42.576317 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #213 | Epoch Duration: 43.75157833099365
2018-05-16 04:59:42.576510 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #213 | Started Training: True
2018-05-16 05:00:24.190184 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #214 | Epoch Duration: 41.613548278808594
2018-05-16 05:00:24.190748 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #214 | Started Training: True
2018-05-16 05:01:06.449674 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #215 | Epoch Duration: 42.25876760482788
2018-05-16 05:01:06.449945 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #215 | Started Training: True
2018-05-16 05:01:48.893909 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #216 | Epoch Duration: 42.44376349449158
2018-05-16 05:01:48.894124 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #216 | Started Training: True
2018-05-16 05:02:31.369569 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #217 | Epoch Duration: 42.47528147697449
2018-05-16 05:02:31.369800 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #217 | Started Training: True
2018-05-16 05:03:13.021890 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #218 | Epoch Duration: 41.651920557022095
2018-05-16 05:03:13.022055 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #218 | Started Training: True
2018-05-16 05:03:54.565540 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #219 | Epoch Duration: 41.543365478515625
2018-05-16 05:03:54.565768 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #219 | Started Training: True
2018-05-16 05:04:44.705776 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #220 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000103909
Policy Loss                      -0.513743
Raw Policy Loss                  -0.513743
Preactivation Policy Loss         0
Q Predictions Mean                0.510782
Q Predictions Std                 0.21275
Q Predictions Max                 0.670337
Q Predictions Min                -0.309965
Q Targets Mean                    0.508474
Q Targets Std                     0.215001
Q Targets Max                     0.673461
Q Targets Min                    -0.354712
Bellman Errors Mean               0.000103909
Bellman Errors Std                0.000467866
Bellman Errors Max                0.00367468
Bellman Errors Min                2.3832e-10
Policy Action Mean                0.273266
Policy Action Std                 0.947778
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.0083508
Test Rewards Std                  0.00395952
Test Rewards Max                  0.0143864
Test Rewards Min                 -0.000319567
Test Returns Mean                 1.20609
Test Returns Std                  0.0174951
Test Returns Max                  1.2208
Test Returns Min                  1.17543
Test Actions Mean                 0.352554
Test Actions Std                  0.914219
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00789766
Exploration Rewards Std           0.00403162
Exploration Rewards Max           0.0154825
Exploration Rewards Min          -0.00127827
Exploration Returns Mean          1.18202
Exploration Returns Std           0.157776
Exploration Returns Max           1.27073
Exploration Returns Min           0.829986
Exploration Actions Mean          0.219579
Exploration Actions Std           0.804674
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.20609
Number of train steps total  220863
Number of env steps total    221000
Number of rollouts total       1690
Train Time (s)                   24.2048
(Previous) Eval Time (s)          3.124e-06
Sample Time (s)                  19.0148
Epoch Time (s)                   43.2196
Total Train Time (s)          25152.2
Epoch                           220
---------------------------  ----------------
2018-05-16 05:08:25.667350 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #220 | Epoch Duration: 271.1014394760132
2018-05-16 05:08:25.667465 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #220 | Started Training: True
2018-05-16 05:11:00.180916 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #221 | Epoch Duration: 154.5133502483368
2018-05-16 05:11:00.181124 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #221 | Started Training: True
2018-05-16 05:13:35.651824 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #222 | Epoch Duration: 155.4705617427826
2018-05-16 05:13:35.652013 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #222 | Started Training: True
2018-05-16 05:16:07.552952 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #223 | Epoch Duration: 151.90081119537354
2018-05-16 05:16:07.553173 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #223 | Started Training: True
2018-05-16 05:18:40.946649 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #224 | Epoch Duration: 153.39332461357117
2018-05-16 05:18:40.946814 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #224 | Started Training: True
2018-05-16 05:20:46.689349 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #225 | Epoch Duration: 125.74241805076599
2018-05-16 05:20:46.689609 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #225 | Started Training: True
2018-05-16 05:21:28.083897 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #226 | Epoch Duration: 41.39409112930298
2018-05-16 05:21:28.084077 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #226 | Started Training: True
2018-05-16 05:22:08.459771 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #227 | Epoch Duration: 40.37558650970459
2018-05-16 05:22:08.459990 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #227 | Started Training: True
2018-05-16 05:22:48.235386 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #228 | Epoch Duration: 39.77523946762085
2018-05-16 05:22:48.235637 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #228 | Started Training: True
2018-05-16 05:23:29.391563 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #229 | Epoch Duration: 41.15573334693909
2018-05-16 05:23:29.391731 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #229 | Started Training: True
2018-05-16 05:24:17.307102 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #230 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.60272e-05
Policy Loss                      -0.505526
Raw Policy Loss                  -0.505526
Preactivation Policy Loss         0
Q Predictions Mean                0.50136
Q Predictions Std                 0.19536
Q Predictions Max                 0.693832
Q Predictions Min                -0.0675791
Q Targets Mean                    0.502493
Q Targets Std                     0.193884
Q Targets Max                     0.694721
Q Targets Min                    -0.0708417
Bellman Errors Mean               6.60272e-05
Bellman Errors Std                0.00028579
Bellman Errors Max                0.00305061
Bellman Errors Min                9.42268e-10
Policy Action Mean                0.31644
Policy Action Std                 0.931501
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00428651
Test Rewards Std                  0.00623009
Test Rewards Max                  0.0174477
Test Rewards Min                 -0.018826
Test Returns Mean                 0.818724
Test Returns Std                  0.577434
Test Returns Max                  1.27551
Test Returns Min                 -0.429096
Test Actions Mean                 0.186202
Test Actions Std                  0.965456
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00761057
Exploration Rewards Std           0.00472579
Exploration Rewards Max           0.0185228
Exploration Rewards Min          -0.000708103
Exploration Returns Mean          1.05026
Exploration Returns Std           0.127919
Exploration Returns Max           1.18362
Exploration Returns Min           0.880372
Exploration Actions Mean          0.188001
Exploration Actions Std           0.814451
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.818724
Number of train steps total  230863
Number of env steps total    231000
Number of rollouts total       1750
Train Time (s)                   22.834
(Previous) Eval Time (s)          1.718e-06
Sample Time (s)                  17.8479
Epoch Time (s)                   40.6819
Total Train Time (s)          26378.5
Epoch                           230
---------------------------  ----------------
2018-05-16 05:28:52.287095 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #230 | Epoch Duration: 322.8952605724335
2018-05-16 05:28:52.287207 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #230 | Started Training: True
2018-05-16 05:29:35.288073 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #231 | Epoch Duration: 43.00076222419739
2018-05-16 05:29:35.288283 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #231 | Started Training: True
2018-05-16 05:30:16.071865 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #232 | Epoch Duration: 40.78343200683594
2018-05-16 05:30:16.072083 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #232 | Started Training: True
2018-05-16 05:32:40.074872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #233 | Epoch Duration: 144.00263857841492
2018-05-16 05:32:40.075062 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #233 | Started Training: True
2018-05-16 05:35:14.190469 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #234 | Epoch Duration: 154.11527848243713
2018-05-16 05:35:14.190659 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #234 | Started Training: True
2018-05-16 05:37:54.852291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #235 | Epoch Duration: 160.6614956855774
2018-05-16 05:37:54.852463 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #235 | Started Training: True
2018-05-16 05:40:27.358408 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #236 | Epoch Duration: 152.5058138370514
2018-05-16 05:40:27.358600 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #236 | Started Training: True
2018-05-16 05:42:57.618696 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #237 | Epoch Duration: 150.25997042655945
2018-05-16 05:42:57.618918 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #237 | Started Training: True
2018-05-16 05:45:31.217965 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #238 | Epoch Duration: 153.598895072937
2018-05-16 05:45:31.218186 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #238 | Started Training: True
2018-05-16 05:48:01.869618 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #239 | Epoch Duration: 150.65127968788147
2018-05-16 05:48:01.869800 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #239 | Started Training: True
2018-05-16 05:50:42.610063 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #240 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           2.7881e-05
Policy Loss                      -0.538113
Raw Policy Loss                  -0.538113
Preactivation Policy Loss         0
Q Predictions Mean                0.535004
Q Predictions Std                 0.18729
Q Predictions Max                 0.678718
Q Predictions Min                -0.324155
Q Targets Mean                    0.534496
Q Targets Std                     0.187605
Q Targets Max                     0.682658
Q Targets Min                    -0.318489
Bellman Errors Mean               2.7881e-05
Bellman Errors Std                8.76992e-05
Bellman Errors Max                0.000922699
Bellman Errors Min                1.83068e-10
Policy Action Mean                0.236758
Policy Action Std                 0.958322
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00776105
Test Rewards Std                  0.00389845
Test Rewards Max                  0.0153094
Test Rewards Min                 -0.0003224
Test Returns Mean                 1.24177
Test Returns Std                  0.00906495
Test Returns Max                  1.2498
Test Returns Min                  1.22288
Test Actions Mean                 0.265279
Test Actions Std                  0.945053
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00740655
Exploration Rewards Std           0.00393157
Exploration Rewards Max           0.0177065
Exploration Rewards Min          -0.000813312
Exploration Returns Mean          1.33318
Exploration Returns Std           0.0756244
Exploration Returns Max           1.41505
Exploration Returns Min           1.17997
Exploration Actions Mean          0.183643
Exploration Actions Std           0.807045
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.24177
Number of train steps total  240863
Number of env steps total    241000
Number of rollouts total       1812
Train Time (s)                   77.9154
(Previous) Eval Time (s)          2.73e-06
Sample Time (s)                  75.0337
Epoch Time (s)                  152.949
Total Train Time (s)          27919
Epoch                           240
---------------------------  ----------------
2018-05-16 05:54:32.995982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #240 | Epoch Duration: 391.12606859207153
2018-05-16 05:54:32.996094 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #240 | Started Training: True
2018-05-16 05:55:14.711665 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #241 | Epoch Duration: 41.71547079086304
2018-05-16 05:55:14.711838 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #241 | Started Training: True
2018-05-16 05:55:56.745391 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #242 | Epoch Duration: 42.033430099487305
2018-05-16 05:55:56.745571 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #242 | Started Training: True
2018-05-16 05:56:37.012276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #243 | Epoch Duration: 40.26656484603882
2018-05-16 05:56:37.012480 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #243 | Started Training: True
2018-05-16 05:57:21.144276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #244 | Epoch Duration: 44.1316442489624
2018-05-16 05:57:21.144429 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #244 | Started Training: True
2018-05-16 05:58:03.139733 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #245 | Epoch Duration: 41.99520182609558
2018-05-16 05:58:03.139908 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #245 | Started Training: True
2018-05-16 05:58:44.404132 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #246 | Epoch Duration: 41.26410889625549
2018-05-16 05:58:44.404301 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #246 | Started Training: True
2018-05-16 05:59:26.493977 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #247 | Epoch Duration: 42.08954882621765
2018-05-16 05:59:26.494186 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #247 | Started Training: True
2018-05-16 06:00:09.588308 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #248 | Epoch Duration: 43.09396743774414
2018-05-16 06:00:09.588481 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #248 | Started Training: True
2018-05-16 06:00:52.248033 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #249 | Epoch Duration: 42.65942859649658
2018-05-16 06:00:52.248275 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #249 | Started Training: True
2018-05-16 06:01:51.781310 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #250 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.94707e-05
Policy Loss                      -0.550707
Raw Policy Loss                  -0.550707
Preactivation Policy Loss         0
Q Predictions Mean                0.547709
Q Predictions Std                 0.192135
Q Predictions Max                 0.694797
Q Predictions Min                -0.24844
Q Targets Mean                    0.54429
Q Targets Std                     0.194203
Q Targets Max                     0.698534
Q Targets Min                    -0.251208
Bellman Errors Mean               9.94707e-05
Bellman Errors Std                0.000560427
Bellman Errors Max                0.00549836
Bellman Errors Min                6.37925e-11
Policy Action Mean                0.25869
Policy Action Std                 0.955441
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00757901
Test Rewards Std                  0.00359879
Test Rewards Max                  0.0149018
Test Rewards Min                 -0.00125489
Test Returns Mean                 1.18233
Test Returns Std                  0.0547636
Test Returns Max                  1.24357
Test Returns Min                  1.07151
Test Actions Mean                 0.21721
Test Actions Std                  0.960003
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00806186
Exploration Rewards Std           0.00401965
Exploration Rewards Max           0.016271
Exploration Rewards Min           0.000206062
Exploration Returns Mean          1.28299
Exploration Returns Std           0.0536422
Exploration Returns Max           1.36095
Exploration Returns Min           1.19904
Exploration Actions Mean          0.156341
Exploration Actions Std           0.822312
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.18233
Number of train steps total  250863
Number of env steps total    251000
Number of rollouts total       1873
Train Time (s)                   27.1896
(Previous) Eval Time (s)          3.04401e-06
Sample Time (s)                  24.738
Epoch Time (s)                   51.9276
Total Train Time (s)          28591.1
Epoch                           250
---------------------------  ----------------
2018-05-16 06:05:45.281833 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #250 | Epoch Duration: 293.03340697288513
2018-05-16 06:05:45.281950 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #250 | Started Training: True
2018-05-16 06:08:17.804632 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #251 | Epoch Duration: 152.5225806236267
2018-05-16 06:08:17.804827 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #251 | Started Training: True
2018-05-16 06:10:52.419747 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #252 | Epoch Duration: 154.61478471755981
2018-05-16 06:10:52.419965 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #252 | Started Training: True
2018-05-16 06:13:25.657084 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #253 | Epoch Duration: 153.23696494102478
2018-05-16 06:13:25.657278 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #253 | Started Training: True
2018-05-16 06:15:43.420521 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #254 | Epoch Duration: 137.76311445236206
2018-05-16 06:15:43.420752 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #254 | Started Training: True
2018-05-16 06:16:26.005294 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #255 | Epoch Duration: 42.58439111709595
2018-05-16 06:16:26.005484 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #255 | Started Training: True
2018-05-16 06:17:08.520138 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #256 | Epoch Duration: 42.514533281326294
2018-05-16 06:17:08.520306 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #256 | Started Training: True
2018-05-16 06:17:48.299587 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #257 | Epoch Duration: 39.77917432785034
2018-05-16 06:17:48.299761 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #257 | Started Training: True
2018-05-16 06:18:29.570843 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #258 | Epoch Duration: 41.270962953567505
2018-05-16 06:18:29.571104 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #258 | Started Training: True
2018-05-16 06:19:09.296007 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #259 | Epoch Duration: 39.72471523284912
2018-05-16 06:19:09.296173 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #259 | Started Training: True
2018-05-16 06:19:57.257327 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #260 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           1.91232e-05
Policy Loss                      -0.564046
Raw Policy Loss                  -0.564046
Preactivation Policy Loss         0
Q Predictions Mean                0.561602
Q Predictions Std                 0.163561
Q Predictions Max                 0.68137
Q Predictions Min                 0.0336206
Q Targets Mean                    0.561967
Q Targets Std                     0.1633
Q Targets Max                     0.684082
Q Targets Min                     0.030867
Bellman Errors Mean               1.91232e-05
Bellman Errors Std                3.20112e-05
Bellman Errors Max                0.000237062
Bellman Errors Min                6.26699e-10
Policy Action Mean                0.163743
Policy Action Std                 0.97724
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00110581
Test Rewards Std                  0.00227859
Test Rewards Max                  0.0139563
Test Rewards Min                 -0.000837787
Test Returns Mean                 0.682835
Test Returns Std                  0.154852
Test Returns Max                  0.837687
Test Returns Min                  0.527982
Test Actions Mean                 0.104601
Test Actions Std                  0.960431
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00713152
Exploration Rewards Std           0.00440245
Exploration Rewards Max           0.0163763
Exploration Rewards Min          -0.00130344
Exploration Returns Mean          1.15225
Exploration Returns Std           0.145669
Exploration Returns Max           1.373
Exploration Returns Min           0.894985
Exploration Actions Mean          0.198865
Exploration Actions Std           0.81382
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.682835
Number of train steps total  260863
Number of env steps total    261000
Number of rollouts total       1932
Train Time (s)                   22.5377
(Previous) Eval Time (s)          1.854e-06
Sample Time (s)                  17.3245
Epoch Time (s)                   39.8622
Total Train Time (s)          29649.3
Epoch                           260
---------------------------  ----------------
2018-05-16 06:23:23.743119 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #260 | Epoch Duration: 254.44684171676636
2018-05-16 06:23:23.743228 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #260 | Started Training: True
2018-05-16 06:24:31.345338 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #261 | Epoch Duration: 67.60201048851013
2018-05-16 06:24:31.345556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #261 | Started Training: True
2018-05-16 06:27:02.883464 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #262 | Epoch Duration: 151.53774189949036
2018-05-16 06:27:02.883658 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #262 | Started Training: True
2018-05-16 06:29:37.731570 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #263 | Epoch Duration: 154.84775352478027
2018-05-16 06:29:37.731770 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #263 | Started Training: True
2018-05-16 06:32:09.495789 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #264 | Epoch Duration: 151.76388263702393
2018-05-16 06:32:09.496019 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #264 | Started Training: True
2018-05-16 06:34:43.200192 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #265 | Epoch Duration: 153.70401215553284
2018-05-16 06:34:43.200424 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #265 | Started Training: True
2018-05-16 06:37:18.423774 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #266 | Epoch Duration: 155.22316932678223
2018-05-16 06:37:18.423964 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #266 | Started Training: True
2018-05-16 06:39:49.492799 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #267 | Epoch Duration: 151.0687072277069
2018-05-16 06:39:49.493000 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #267 | Started Training: True
2018-05-16 06:42:21.644059 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #268 | Epoch Duration: 152.15093421936035
2018-05-16 06:42:21.644249 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #268 | Started Training: True
2018-05-16 06:44:55.325596 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #269 | Epoch Duration: 153.68121767044067
2018-05-16 06:44:55.325799 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #269 | Started Training: True
2018-05-16 06:47:43.889947 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #270 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.51665e-05
Policy Loss                      -0.56178
Raw Policy Loss                  -0.56178
Preactivation Policy Loss         0
Q Predictions Mean                0.558441
Q Predictions Std                 0.165808
Q Predictions Max                 0.690883
Q Predictions Min                 0.0401636
Q Targets Mean                    0.555727
Q Targets Std                     0.169135
Q Targets Max                     0.694349
Q Targets Min                     0.0112993
Bellman Errors Mean               6.51665e-05
Bellman Errors Std                0.000261247
Bellman Errors Max                0.00285907
Bellman Errors Min                2.942e-11
Policy Action Mean                0.198334
Policy Action Std                 0.969464
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.0079618
Test Rewards Std                  0.00339801
Test Rewards Max                  0.0150838
Test Rewards Min                 -0.000646018
Test Returns Mean                 1.10569
Test Returns Std                  0.0323939
Test Returns Max                  1.14065
Test Returns Min                  1.03093
Test Actions Mean                 0.20462
Test Actions Std                  0.96526
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00726612
Exploration Rewards Std           0.00413515
Exploration Rewards Max           0.0164247
Exploration Rewards Min          -0.00166646
Exploration Returns Mean          1.24458
Exploration Returns Std           0.0875397
Exploration Returns Max           1.31062
Exploration Returns Min           1.03664
Exploration Actions Mean          0.158248
Exploration Actions Std           0.819384
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.10569
Number of train steps total  270863
Number of env steps total    271000
Number of rollouts total       1995
Train Time (s)                   81.9089
(Previous) Eval Time (s)          2.38101e-06
Sample Time (s)                  95.7547
Epoch Time (s)                  177.664
Total Train Time (s)          31351.6
Epoch                           270
---------------------------  ----------------
2018-05-16 06:51:46.295848 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #270 | Epoch Duration: 410.9699168205261
2018-05-16 06:51:46.295958 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #270 | Started Training: True
2018-05-16 06:52:31.226832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #271 | Epoch Duration: 44.930777072906494
2018-05-16 06:52:31.227020 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #271 | Started Training: True
2018-05-16 06:53:15.941542 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #272 | Epoch Duration: 44.7144033908844
2018-05-16 06:53:15.941710 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #272 | Started Training: True
2018-05-16 06:54:01.461483 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #273 | Epoch Duration: 45.519659757614136
2018-05-16 06:54:01.461663 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #273 | Started Training: True
2018-05-16 06:54:44.361907 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #274 | Epoch Duration: 42.900126457214355
2018-05-16 06:54:44.362083 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #274 | Started Training: True
2018-05-16 06:55:30.867466 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #275 | Epoch Duration: 46.50527381896973
2018-05-16 06:55:30.867635 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #275 | Started Training: True
2018-05-16 06:56:15.057053 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #276 | Epoch Duration: 44.18931436538696
2018-05-16 06:56:15.057235 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #276 | Started Training: True
2018-05-16 06:57:01.007957 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #277 | Epoch Duration: 45.95060062408447
2018-05-16 06:57:01.008139 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #277 | Started Training: True
2018-05-16 06:57:50.251865 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #278 | Epoch Duration: 49.2435986995697
2018-05-16 06:57:50.252036 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #278 | Started Training: True
2018-05-16 07:00:55.452982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #279 | Epoch Duration: 185.20083022117615
2018-05-16 07:00:55.453182 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #279 | Started Training: True
2018-05-16 07:04:14.671282 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #280 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.43577e-05
Policy Loss                      -0.530131
Raw Policy Loss                  -0.530131
Preactivation Policy Loss         0
Q Predictions Mean                0.526884
Q Predictions Std                 0.192588
Q Predictions Max                 0.689709
Q Predictions Min                -0.150199
Q Targets Mean                    0.525959
Q Targets Std                     0.195213
Q Targets Max                     0.693085
Q Targets Min                    -0.168906
Bellman Errors Mean               9.43577e-05
Bellman Errors Std                0.000548989
Bellman Errors Max                0.00590669
Bellman Errors Min                2.92633e-10
Policy Action Mean                0.184942
Policy Action Std                 0.970508
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00741817
Test Rewards Std                  0.00382825
Test Rewards Max                  0.0148071
Test Rewards Min                  0.000871529
Test Returns Mean                 1.28211
Test Returns Std                  0.00739231
Test Returns Max                  1.29311
Test Returns Min                  1.27156
Test Actions Mean                 0.244809
Test Actions Std                  0.955108
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00730431
Exploration Rewards Std           0.00372616
Exploration Rewards Max           0.0167048
Exploration Rewards Min          -0.00129043
Exploration Returns Mean          1.26121
Exploration Returns Std           0.166432
Exploration Returns Max           1.42265
Exploration Returns Min           0.93832
Exploration Actions Mean          0.150737
Exploration Actions Std           0.820799
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.28211
Number of train steps total  280863
Number of env steps total    281000
Number of rollouts total       2053
Train Time (s)                   96.226
(Previous) Eval Time (s)          2.792e-06
Sample Time (s)                 307.486
Epoch Time (s)                  403.712
Total Train Time (s)          32324.3
Epoch                           280
---------------------------  ----------------
2018-05-16 07:07:59.182756 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #280 | Epoch Duration: 423.7294547557831
2018-05-16 07:07:59.182872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #280 | Started Training: True
2018-05-16 07:10:47.257423 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #281 | Epoch Duration: 168.07445216178894
2018-05-16 07:10:47.257662 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #281 | Started Training: True
2018-05-16 07:13:21.150965 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #282 | Epoch Duration: 153.89314770698547
2018-05-16 07:13:21.151158 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #282 | Started Training: True
2018-05-16 07:15:14.584463 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #283 | Epoch Duration: 113.43317461013794
2018-05-16 07:15:14.584674 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #283 | Started Training: True
2018-05-16 07:15:56.232480 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #284 | Epoch Duration: 41.64765763282776
2018-05-16 07:15:56.232650 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #284 | Started Training: True
2018-05-16 07:16:41.713609 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #285 | Epoch Duration: 45.480844497680664
2018-05-16 07:16:41.713824 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #285 | Started Training: True
2018-05-16 07:17:29.090335 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #286 | Epoch Duration: 47.3763644695282
2018-05-16 07:17:29.090531 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #286 | Started Training: True
2018-05-16 07:18:14.966063 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #287 | Epoch Duration: 45.87542009353638
2018-05-16 07:18:14.966238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #287 | Started Training: True
2018-05-16 07:19:00.359050 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #288 | Epoch Duration: 45.39270067214966
2018-05-16 07:19:00.359238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #288 | Started Training: True
2018-05-16 07:20:30.219935 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #289 | Epoch Duration: 89.86056923866272
2018-05-16 07:20:30.220149 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #289 | Started Training: True
2018-05-16 07:23:30.940214 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #290 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.44541e-05
Policy Loss                      -0.539012
Raw Policy Loss                  -0.539012
Preactivation Policy Loss         0
Q Predictions Mean                0.536261
Q Predictions Std                 0.181372
Q Predictions Max                 0.679202
Q Predictions Min                -0.373997
Q Targets Mean                    0.537793
Q Targets Std                     0.17936
Q Targets Max                     0.68388
Q Targets Min                    -0.373038
Bellman Errors Mean               4.44541e-05
Bellman Errors Std                8.77686e-05
Bellman Errors Max                0.000657257
Bellman Errors Min                1.9984e-11
Policy Action Mean                0.215709
Policy Action Std                 0.964258
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00612197
Test Rewards Std                  0.00392026
Test Rewards Max                  0.0150554
Test Rewards Min                 -0.00143111
Test Returns Mean                 1.19276
Test Returns Std                  0.0254362
Test Returns Max                  1.24102
Test Returns Min                  1.15703
Test Actions Mean                 0.256218
Test Actions Std                  0.957355
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00726878
Exploration Rewards Std           0.00390243
Exploration Rewards Max           0.0162557
Exploration Rewards Min          -0.00109913
Exploration Returns Mean          1.24919
Exploration Returns Std           0.0930291
Exploration Returns Max           1.4079
Exploration Returns Min           1.16286
Exploration Actions Mean          0.141605
Exploration Actions Std           0.824538
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.19276
Number of train steps total  290863
Number of env steps total    291000
Number of rollouts total       2116
Train Time (s)                   88.0825
(Previous) Eval Time (s)          2.21599e-06
Sample Time (s)                 179.407
Epoch Time (s)                  267.49
Total Train Time (s)          33526.8
Epoch                           290
---------------------------  ----------------
2018-05-16 07:28:01.931423 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #290 | Epoch Duration: 451.71115350723267
2018-05-16 07:28:01.931541 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #290 | Started Training: True
2018-05-16 07:30:51.889023 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #291 | Epoch Duration: 169.95737433433533
2018-05-16 07:30:51.889224 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #291 | Started Training: True
2018-05-16 07:32:58.123620 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #292 | Epoch Duration: 126.23427033424377
2018-05-16 07:32:58.123793 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #292 | Started Training: True
2018-05-16 07:33:41.120903 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #293 | Epoch Duration: 42.99699521064758
2018-05-16 07:33:41.121120 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #293 | Started Training: True
2018-05-16 07:34:26.719576 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #294 | Epoch Duration: 45.59830379486084
2018-05-16 07:34:26.719772 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #294 | Started Training: True
2018-05-16 07:35:10.822164 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #295 | Epoch Duration: 44.10226917266846
2018-05-16 07:35:10.822363 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #295 | Started Training: True
2018-05-16 07:35:54.609664 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #296 | Epoch Duration: 43.7871470451355
2018-05-16 07:35:54.609901 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #296 | Started Training: True
2018-05-16 07:36:38.977616 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #297 | Epoch Duration: 44.36752533912659
2018-05-16 07:36:38.977841 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #297 | Started Training: True
2018-05-16 07:37:23.670168 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #298 | Epoch Duration: 44.6921443939209
2018-05-16 07:37:23.670345 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #298 | Started Training: True
2018-05-16 07:38:08.868818 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #299 | Epoch Duration: 45.198251485824585
2018-05-16 07:38:08.869006 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #299 | Started Training: True
2018-05-16 07:39:02.601827 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #300 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.02961e-05
Policy Loss                      -0.542492
Raw Policy Loss                  -0.542492
Preactivation Policy Loss         0
Q Predictions Mean                0.539373
Q Predictions Std                 0.205025
Q Predictions Max                 0.689586
Q Predictions Min                -0.378998
Q Targets Mean                    0.536866
Q Targets Std                     0.206726
Q Targets Max                     0.6938
Q Targets Min                    -0.419268
Bellman Errors Mean               9.02961e-05
Bellman Errors Std                0.000334502
Bellman Errors Max                0.00280568
Bellman Errors Min                4.97198e-09
Policy Action Mean                0.217704
Policy Action Std                 0.967678
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00785586
Test Rewards Std                  0.00455627
Test Rewards Max                  0.0143706
Test Rewards Min                  0.000658474
Test Returns Mean                 0.854718
Test Returns Std                  0.00687444
Test Returns Max                  0.863605
Test Returns Min                  0.840958
Test Actions Mean                 0.180851
Test Actions Std                  0.974295
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00777477
Exploration Rewards Std           0.00490327
Exploration Rewards Max           0.0169755
Exploration Rewards Min          -0.00184955
Exploration Returns Mean          0.954076
Exploration Returns Std           0.0223082
Exploration Returns Max           0.982813
Exploration Returns Min           0.911205
Exploration Actions Mean          0.0953497
Exploration Actions Std           0.832701
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.854718
Number of train steps total  300863
Number of env steps total    301000
Number of rollouts total       2185
Train Time (s)                   24.7152
(Previous) Eval Time (s)          2.439e-06
Sample Time (s)                  28.0726
Epoch Time (s)                   52.7878
Total Train Time (s)          34468.3
Epoch                           300
---------------------------  ----------------
2018-05-16 07:43:43.701865 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #300 | Epoch Duration: 334.8327434062958
2018-05-16 07:43:43.701976 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #300 | Started Training: True
2018-05-16 07:45:39.421230 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #301 | Epoch Duration: 115.71915364265442
2018-05-16 07:45:39.421420 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #301 | Started Training: True
2018-05-16 07:48:14.162084 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #302 | Epoch Duration: 154.74052572250366
2018-05-16 07:48:14.162268 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #302 | Started Training: True
2018-05-16 07:50:50.250237 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #303 | Epoch Duration: 156.08784866333008
2018-05-16 07:50:50.250444 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #303 | Started Training: True
2018-05-16 07:53:26.003416 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #304 | Epoch Duration: 155.7528429031372
2018-05-16 07:53:26.003580 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #304 | Started Training: True
2018-05-16 07:56:02.216987 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #305 | Epoch Duration: 156.21329617500305
2018-05-16 07:56:02.217221 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #305 | Started Training: True
2018-05-16 07:58:35.653003 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #306 | Epoch Duration: 153.43563103675842
2018-05-16 07:58:35.653224 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #306 | Started Training: True
2018-05-16 08:01:08.703884 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #307 | Epoch Duration: 153.0505084991455
2018-05-16 08:01:08.704112 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #307 | Started Training: True
2018-05-16 08:03:44.071307 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #308 | Epoch Duration: 155.36704015731812
2018-05-16 08:03:44.071513 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #308 | Started Training: True
2018-05-16 08:06:18.013363 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #309 | Epoch Duration: 153.94171285629272
2018-05-16 08:06:18.013546 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #309 | Started Training: True
2018-05-16 08:09:01.075310 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #310 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.09857e-05
Policy Loss                      -0.548633
Raw Policy Loss                  -0.548633
Preactivation Policy Loss         0
Q Predictions Mean                0.545853
Q Predictions Std                 0.182852
Q Predictions Max                 0.708097
Q Predictions Min                -0.045392
Q Targets Mean                    0.545321
Q Targets Std                     0.181968
Q Targets Max                     0.707506
Q Targets Min                     0.0136098
Bellman Errors Mean               6.09857e-05
Bellman Errors Std                0.000311963
Bellman Errors Max                0.00348121
Bellman Errors Min                3.73257e-11
Policy Action Mean                0.169446
Policy Action Std                 0.975605
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00628229
Test Rewards Std                  0.00370231
Test Rewards Max                  0.0155799
Test Rewards Min                 -0.000964793
Test Returns Mean                 1.24704
Test Returns Std                  0.0115255
Test Returns Max                  1.26122
Test Returns Min                  1.2322
Test Actions Mean                 0.131423
Test Actions Std                  0.975344
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00743706
Exploration Rewards Std           0.00374142
Exploration Rewards Max           0.0156908
Exploration Rewards Min          -0.00207114
Exploration Returns Mean          1.211
Exploration Returns Std           0.0345583
Exploration Returns Max           1.24391
Exploration Returns Min           1.14753
Exploration Actions Mean          0.133037
Exploration Actions Std           0.822849
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.24704
Number of train steps total  310863
Number of env steps total    311000
Number of rollouts total       2248
Train Time (s)                   78.8925
(Previous) Eval Time (s)          2.707e-06
Sample Time (s)                  75.1072
Epoch Time (s)                  154
Total Train Time (s)          36233
Epoch                           310
---------------------------  ----------------
2018-05-16 08:13:08.598837 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #310 | Epoch Duration: 410.58517360687256
2018-05-16 08:13:08.598947 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #310 | Started Training: True
2018-05-16 08:13:51.532993 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #311 | Epoch Duration: 42.93394231796265
2018-05-16 08:13:51.533208 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #311 | Started Training: True
2018-05-16 08:14:34.260442 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #312 | Epoch Duration: 42.7266571521759
2018-05-16 08:14:34.260591 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #312 | Started Training: True
2018-05-16 08:15:16.154785 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #313 | Epoch Duration: 41.89408278465271
2018-05-16 08:15:16.154949 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #313 | Started Training: True
2018-05-16 08:15:58.202164 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #314 | Epoch Duration: 42.047099113464355
2018-05-16 08:15:58.202396 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #314 | Started Training: True
2018-05-16 08:16:40.024035 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #315 | Epoch Duration: 41.82147526741028
2018-05-16 08:16:40.024255 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #315 | Started Training: True
2018-05-16 08:17:22.570608 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #316 | Epoch Duration: 42.54619383811951
2018-05-16 08:17:22.570822 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #316 | Started Training: True
2018-05-16 08:18:05.817237 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #317 | Epoch Duration: 43.246262311935425
2018-05-16 08:18:05.817449 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #317 | Started Training: True
2018-05-16 08:18:48.880257 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #318 | Epoch Duration: 43.06265425682068
2018-05-16 08:18:48.880437 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #318 | Started Training: True
2018-05-16 08:19:52.006796 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #319 | Epoch Duration: 63.12623047828674
2018-05-16 08:19:52.007030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #319 | Started Training: True
2018-05-16 08:22:35.040517 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #320 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000201728
Policy Loss                      -0.607104
Raw Policy Loss                  -0.607104
Preactivation Policy Loss         0
Q Predictions Mean                0.60376
Q Predictions Std                 0.201248
Q Predictions Max                 0.822667
Q Predictions Min                -0.128546
Q Targets Mean                    0.605848
Q Targets Std                     0.199057
Q Targets Max                     0.83082
Q Targets Min                     0.000824085
Bellman Errors Mean               0.000201728
Bellman Errors Std                0.0017359
Bellman Errors Max                0.0197178
Bellman Errors Min                2.38011e-09
Policy Action Mean                0.183904
Policy Action Std                 0.969653
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00777549
Test Rewards Std                  0.004179
Test Rewards Max                  0.0160506
Test Rewards Min                  0.00101346
Test Returns Mean                 0.897637
Test Returns Std                  0.00728673
Test Returns Max                  0.906837
Test Returns Min                  0.884879
Test Actions Mean                 0.314987
Test Actions Std                  0.93417
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00728311
Exploration Rewards Std           0.00381548
Exploration Rewards Max           0.0169021
Exploration Rewards Min          -0.00132184
Exploration Returns Mean          1.10911
Exploration Returns Std           0.17762
Exploration Returns Max           1.38289
Exploration Returns Min           0.900031
Exploration Actions Mean          0.150442
Exploration Actions Std           0.816257
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.897637
Number of train steps total  320863
Number of env steps total    321000
Number of rollouts total       2308
Train Time (s)                   78.1686
(Previous) Eval Time (s)          3.168e-06
Sample Time (s)                  75.3959
Epoch Time (s)                  153.565
Total Train Time (s)          37033.8
Epoch                           320
---------------------------  ----------------
2018-05-16 08:26:29.675829 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #320 | Epoch Duration: 397.6686327457428
2018-05-16 08:26:29.675948 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #320 | Started Training: True
2018-05-16 08:29:00.841412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #321 | Epoch Duration: 151.16536474227905
2018-05-16 08:29:00.841634 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #321 | Started Training: True
2018-05-16 08:31:34.517872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #322 | Epoch Duration: 153.67607927322388
2018-05-16 08:31:34.518032 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #322 | Started Training: True
2018-05-16 08:33:37.061556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #323 | Epoch Duration: 122.54341244697571
2018-05-16 08:33:37.061760 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #323 | Started Training: True
2018-05-16 08:34:20.744331 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #324 | Epoch Duration: 43.68241834640503
2018-05-16 08:34:20.744548 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #324 | Started Training: True
2018-05-16 08:35:04.393215 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #325 | Epoch Duration: 43.648529052734375
2018-05-16 08:35:04.393427 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #325 | Started Training: True
2018-05-16 08:35:46.451915 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #326 | Epoch Duration: 42.05832362174988
2018-05-16 08:35:46.452142 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #326 | Started Training: True
2018-05-16 08:36:28.823710 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #327 | Epoch Duration: 42.37139105796814
2018-05-16 08:36:28.823884 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #327 | Started Training: True
2018-05-16 08:37:11.848967 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #328 | Epoch Duration: 43.024972677230835
2018-05-16 08:37:11.849156 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #328 | Started Training: True
2018-05-16 08:37:53.492110 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #329 | Epoch Duration: 41.64283609390259
2018-05-16 08:37:53.492280 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #329 | Started Training: True
2018-05-16 08:38:46.437261 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #330 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.36331e-05
Policy Loss                      -0.608081
Raw Policy Loss                  -0.608081
Preactivation Policy Loss         0
Q Predictions Mean                0.605085
Q Predictions Std                 0.210019
Q Predictions Max                 0.810566
Q Predictions Min                -0.30065
Q Targets Mean                    0.602376
Q Targets Std                     0.211269
Q Targets Max                     0.808436
Q Targets Min                    -0.298883
Bellman Errors Mean               5.36331e-05
Bellman Errors Std                0.000114763
Bellman Errors Max                0.00101947
Bellman Errors Min                3.51021e-09
Policy Action Mean                0.255059
Policy Action Std                 0.951403
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00836819
Test Rewards Std                  0.00470968
Test Rewards Max                  0.014418
Test Rewards Min                 -0.000288024
Test Returns Mean                 0.872803
Test Returns Std                  0.0130094
Test Returns Max                  0.891943
Test Returns Min                  0.852263
Test Actions Mean                 0.269003
Test Actions Std                  0.941468
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.0073414
Exploration Rewards Std           0.00430601
Exploration Rewards Max           0.0156266
Exploration Rewards Min          -0.00113138
Exploration Returns Mean          0.894427
Exploration Returns Std           0.0626113
Exploration Returns Max           0.951715
Exploration Returns Min           0.760845
Exploration Actions Mean          0.168314
Exploration Actions Std           0.820446
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.872803
Number of train steps total  330863
Number of env steps total    331000
Number of rollouts total       2384
Train Time (s)                   24.2729
(Previous) Eval Time (s)          2.221e-06
Sample Time (s)                  19.5439
Epoch Time (s)                   43.8168
Total Train Time (s)          37997.6
Epoch                           330
---------------------------  ----------------
2018-05-16 08:42:33.683470 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #330 | Epoch Duration: 280.19106364250183
2018-05-16 08:42:33.683580 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #330 | Started Training: True
2018-05-16 08:43:15.757541 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #331 | Epoch Duration: 42.07386636734009
2018-05-16 08:43:15.757716 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #331 | Started Training: True
2018-05-16 08:43:57.217572 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #332 | Epoch Duration: 41.459739685058594
2018-05-16 08:43:57.217778 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #332 | Started Training: True
2018-05-16 08:44:40.450505 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #333 | Epoch Duration: 43.23257303237915
2018-05-16 08:44:40.450723 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #333 | Started Training: True
2018-05-16 08:47:05.285246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #334 | Epoch Duration: 144.83437418937683
2018-05-16 08:47:05.285408 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #334 | Started Training: True
2018-05-16 08:49:40.706136 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #335 | Epoch Duration: 155.42062044143677
2018-05-16 08:49:40.706369 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #335 | Started Training: True
2018-05-16 08:52:16.278014 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #336 | Epoch Duration: 155.57145953178406
2018-05-16 08:52:16.278246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #336 | Started Training: True
2018-05-16 08:54:47.142631 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #337 | Epoch Duration: 150.86422109603882
2018-05-16 08:54:47.142832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #337 | Started Training: True
2018-05-16 08:57:22.255653 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #338 | Epoch Duration: 155.1126847267151
2018-05-16 08:57:22.255842 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #338 | Started Training: True
2018-05-16 08:59:58.522301 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #339 | Epoch Duration: 156.26629972457886
2018-05-16 08:59:58.522543 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #339 | Started Training: True
2018-05-16 09:02:43.224767 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #340 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.35759e-05
Policy Loss                      -0.539638
Raw Policy Loss                  -0.539638
Preactivation Policy Loss         0
Q Predictions Mean                0.536427
Q Predictions Std                 0.208676
Q Predictions Max                 0.757738
Q Predictions Min                -0.303913
Q Targets Mean                    0.536486
Q Targets Std                     0.208231
Q Targets Max                     0.759365
Q Targets Min                    -0.295007
Bellman Errors Mean               3.3576e-05
Bellman Errors Std                6.72279e-05
Bellman Errors Max                0.000407559
Bellman Errors Min                2.44746e-11
Policy Action Mean                0.285312
Policy Action Std                 0.945138
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00815513
Test Rewards Std                  0.00427037
Test Rewards Max                  0.0162728
Test Rewards Min                  7.25917e-05
Test Returns Mean                 1.07342
Test Returns Std                  0.0123209
Test Returns Max                  1.09046
Test Returns Min                  1.05828
Test Actions Mean                 0.282558
Test Actions Std                  0.934788
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00795821
Exploration Rewards Std           0.00428236
Exploration Rewards Max           0.0168488
Exploration Rewards Min          -0.00181851
Exploration Returns Mean          0.967917
Exploration Returns Std           0.0622823
Exploration Returns Max           1.04387
Exploration Returns Min           0.876239
Exploration Actions Mean          0.176291
Exploration Actions Std           0.82146
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.07342
Number of train steps total  340863
Number of env steps total    341000
Number of rollouts total       2465
Train Time (s)                   78.8858
(Previous) Eval Time (s)          2.40401e-06
Sample Time (s)                  76.0399
Epoch Time (s)                  154.926
Total Train Time (s)          39449.5
Epoch                           340
---------------------------  ----------------
2018-05-16 09:06:45.818267 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #340 | Epoch Duration: 407.29557943344116
2018-05-16 09:06:45.818392 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #340 | Started Training: True
2018-05-16 09:07:29.160904 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #341 | Epoch Duration: 43.34240198135376
2018-05-16 09:07:29.161062 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #341 | Started Training: True
2018-05-16 09:08:13.465956 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #342 | Epoch Duration: 44.304784059524536
2018-05-16 09:08:13.466111 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #342 | Started Training: True
2018-05-16 09:08:56.276483 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #343 | Epoch Duration: 42.81026816368103
2018-05-16 09:08:56.276683 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #343 | Started Training: True
2018-05-16 09:09:39.347667 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #344 | Epoch Duration: 43.07085919380188
2018-05-16 09:09:39.347837 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #344 | Started Training: True
2018-05-16 09:10:22.684508 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #345 | Epoch Duration: 43.336554288864136
2018-05-16 09:10:22.684721 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #345 | Started Training: True
2018-05-16 09:11:05.072392 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #346 | Epoch Duration: 42.387518644332886
2018-05-16 09:11:05.072610 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #346 | Started Training: True
2018-05-16 09:11:48.165456 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #347 | Epoch Duration: 43.0926878452301
2018-05-16 09:11:48.165626 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #347 | Started Training: True
2018-05-16 09:12:31.236704 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #348 | Epoch Duration: 43.0709707736969
2018-05-16 09:12:31.236875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #348 | Started Training: True
2018-05-16 09:13:14.780812 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #349 | Epoch Duration: 43.54382276535034
2018-05-16 09:13:14.781037 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #349 | Started Training: True
2018-05-16 09:14:06.779097 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #350 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.3665e-05
Policy Loss                      -0.52596
Raw Policy Loss                  -0.52596
Preactivation Policy Loss         0
Q Predictions Mean                0.522955
Q Predictions Std                 0.210475
Q Predictions Max                 0.703258
Q Predictions Min                -0.341739
Q Targets Mean                    0.520816
Q Targets Std                     0.212743
Q Targets Max                     0.705849
Q Targets Min                    -0.357962
Bellman Errors Mean               6.3665e-05
Bellman Errors Std                0.000246989
Bellman Errors Max                0.00227724
Bellman Errors Min                1.10619e-09
Policy Action Mean                0.228158
Policy Action Std                 0.955355
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00875545
Test Rewards Std                  0.00401257
Test Rewards Max                  0.0137872
Test Rewards Min                 -0.000709232
Test Returns Mean                 1.06719
Test Returns Std                  0.0107999
Test Returns Max                  1.08759
Test Returns Min                  1.05333
Test Actions Mean                 0.355443
Test Actions Std                  0.925314
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.0079349
Exploration Rewards Std           0.004113
Exploration Rewards Max           0.014868
Exploration Rewards Min          -0.00309423
Exploration Returns Mean          1.0365
Exploration Returns Std           0.0984642
Exploration Returns Max           1.19669
Exploration Returns Min           0.906622
Exploration Actions Mean          0.209093
Exploration Actions Std           0.81069
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.06719
Number of train steps total  350863
Number of env steps total    351000
Number of rollouts total       2538
Train Time (s)                   24.3899
(Previous) Eval Time (s)          2.702e-06
Sample Time (s)                  18.0366
Epoch Time (s)                   42.4264
Total Train Time (s)          40118.7
Epoch                           350
---------------------------  ----------------
2018-05-16 09:17:55.174970 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #350 | Epoch Duration: 280.3937749862671
2018-05-16 09:17:55.175106 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #350 | Started Training: True
2018-05-16 09:20:26.769139 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #351 | Epoch Duration: 151.59391808509827
2018-05-16 09:20:26.769353 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #351 | Started Training: True
2018-05-16 09:22:59.300035 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #352 | Epoch Duration: 152.53052306175232
2018-05-16 09:22:59.300315 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #352 | Started Training: True
2018-05-16 09:25:33.016038 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #353 | Epoch Duration: 153.71551752090454
2018-05-16 09:25:33.016235 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #353 | Started Training: True
2018-05-16 09:28:04.017058 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #354 | Epoch Duration: 151.0007016658783
2018-05-16 09:28:04.017771 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #354 | Started Training: True
2018-05-16 09:30:53.953389 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #355 | Epoch Duration: 169.9354739189148
2018-05-16 09:30:53.953617 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #355 | Started Training: True
2018-05-16 09:33:42.898514 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #356 | Epoch Duration: 168.9447467327118
2018-05-16 09:33:42.898670 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #356 | Started Training: True
2018-05-16 09:36:15.120920 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #357 | Epoch Duration: 152.22214770317078
2018-05-16 09:36:15.121091 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #357 | Started Training: True
2018-05-16 09:38:39.465177 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #358 | Epoch Duration: 144.3439712524414
2018-05-16 09:38:39.465409 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #358 | Started Training: True
2018-05-16 09:39:22.869925 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #359 | Epoch Duration: 43.404359102249146
2018-05-16 09:39:22.870089 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #359 | Started Training: True
2018-05-16 09:40:17.137781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #360 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.16203e-05
Policy Loss                      -0.536741
Raw Policy Loss                  -0.536741
Preactivation Policy Loss         0
Q Predictions Mean                0.533617
Q Predictions Std                 0.189016
Q Predictions Max                 0.741305
Q Predictions Min                -0.00751871
Q Targets Mean                    0.536542
Q Targets Std                     0.186813
Q Targets Max                     0.737519
Q Targets Min                     0.00879146
Bellman Errors Mean               9.16203e-05
Bellman Errors Std                0.000208175
Bellman Errors Max                0.00186342
Bellman Errors Min                3.16595e-09
Policy Action Mean                0.278417
Policy Action Std                 0.948949
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00772975
Test Rewards Std                  0.00477745
Test Rewards Max                  0.0145276
Test Rewards Min                 -0.000303586
Test Returns Mean                 0.740651
Test Returns Std                  0.0205824
Test Returns Max                  0.794711
Test Returns Min                  0.69926
Test Actions Mean                 0.26198
Test Actions Std                  0.954327
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.0083119
Exploration Rewards Std           0.00418652
Exploration Rewards Max           0.0160989
Exploration Rewards Min          -0.00151226
Exploration Returns Mean          1.06208
Exploration Returns Std           0.0901809
Exploration Returns Max           1.2461
Exploration Returns Min           0.93168
Exploration Actions Mean          0.168241
Exploration Actions Std           0.822725
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.740651
Number of train steps total  360863
Number of env steps total    361000
Number of rollouts total       2613
Train Time (s)                   24.6798
(Previous) Eval Time (s)          1.95e-06
Sample Time (s)                 133.915
Epoch Time (s)                  158.594
Total Train Time (s)          41750.3
Epoch                           360
---------------------------  ----------------
2018-05-16 09:45:07.026328 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #360 | Epoch Duration: 344.15613079071045
2018-05-16 09:45:07.026478 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #360 | Started Training: True
2018-05-16 09:47:40.167562 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #361 | Epoch Duration: 153.14096570014954
2018-05-16 09:47:40.167786 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #361 | Started Training: True
2018-05-16 09:50:17.353753 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #362 | Epoch Duration: 157.18581414222717
2018-05-16 09:50:17.353985 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #362 | Started Training: True
2018-05-16 09:52:56.905052 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #363 | Epoch Duration: 159.5509135723114
2018-05-16 09:52:56.905246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #363 | Started Training: True
2018-05-16 09:55:31.733256 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #364 | Epoch Duration: 154.82783603668213
2018-05-16 09:55:31.733447 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #364 | Started Training: True
2018-05-16 09:58:06.080062 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #365 | Epoch Duration: 154.3464870452881
2018-05-16 09:58:06.080260 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #365 | Started Training: True
2018-05-16 10:00:41.397374 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #366 | Epoch Duration: 155.31698751449585
2018-05-16 10:00:41.397551 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #366 | Started Training: True
2018-05-16 10:03:15.455505 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #367 | Epoch Duration: 154.05783700942993
2018-05-16 10:03:15.455731 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #367 | Started Training: True
2018-05-16 10:05:31.577351 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #368 | Epoch Duration: 136.1214621067047
2018-05-16 10:05:31.577565 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #368 | Started Training: True
2018-05-16 10:06:14.920211 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #369 | Epoch Duration: 43.3424973487854
2018-05-16 10:06:14.920387 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #369 | Started Training: True
2018-05-16 10:07:08.844961 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #370 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.86576e-05
Policy Loss                      -0.554147
Raw Policy Loss                  -0.554147
Preactivation Policy Loss         0
Q Predictions Mean                0.551186
Q Predictions Std                 0.229574
Q Predictions Max                 0.747781
Q Predictions Min                -0.411673
Q Targets Mean                    0.55252
Q Targets Std                     0.22715
Q Targets Max                     0.748001
Q Targets Min                    -0.367075
Bellman Errors Mean               6.86576e-05
Bellman Errors Std                0.000228471
Bellman Errors Max                0.001989
Bellman Errors Min                5.40368e-10
Policy Action Mean                0.230011
Policy Action Std                 0.962021
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00683671
Test Rewards Std                  0.00346102
Test Rewards Max                  0.014193
Test Rewards Min                  0.00106379
Test Returns Mean                 1.04211
Test Returns Std                  0.0335282
Test Returns Max                  1.1134
Test Returns Min                  0.993601
Test Actions Mean                 0.265102
Test Actions Std                  0.942823
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00761822
Exploration Rewards Std           0.00390945
Exploration Rewards Max           0.0156884
Exploration Rewards Min          -0.00110693
Exploration Returns Mean          1.06111
Exploration Returns Std           0.148744
Exploration Returns Max           1.26926
Exploration Returns Min           0.878453
Exploration Actions Mean          0.215075
Exploration Actions Std           0.804601
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.04211
Number of train steps total  370863
Number of env steps total    371000
Number of rollouts total       2677
Train Time (s)                   24.9446
(Previous) Eval Time (s)          1.88001e-06
Sample Time (s)                  19.1233
Epoch Time (s)                   44.0679
Total Train Time (s)          43305.5
Epoch                           370
---------------------------  ----------------
2018-05-16 10:11:02.515012 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #370 | Epoch Duration: 287.59451055526733
2018-05-16 10:11:02.515126 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #370 | Started Training: True
2018-05-16 10:13:36.459648 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #371 | Epoch Duration: 153.9444124698639
2018-05-16 10:13:36.459811 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #371 | Started Training: True
2018-05-16 10:16:14.003849 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #372 | Epoch Duration: 157.5439281463623
2018-05-16 10:16:14.004072 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #372 | Started Training: True
2018-05-16 10:18:46.819759 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #373 | Epoch Duration: 152.8155357837677
2018-05-16 10:18:46.819982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #373 | Started Training: True
2018-05-16 10:21:21.426398 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #374 | Epoch Duration: 154.6062514781952
2018-05-16 10:21:21.426604 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #374 | Started Training: True
2018-05-16 10:23:55.019830 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #375 | Epoch Duration: 153.59309697151184
2018-05-16 10:23:55.020053 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #375 | Started Training: True
2018-05-16 10:26:28.050353 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #376 | Epoch Duration: 153.03012704849243
2018-05-16 10:26:28.050559 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #376 | Started Training: True
2018-05-16 10:28:59.309100 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #377 | Epoch Duration: 151.25841641426086
2018-05-16 10:28:59.309298 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #377 | Started Training: True
2018-05-16 10:31:33.947828 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #378 | Epoch Duration: 154.6384027004242
2018-05-16 10:31:33.948014 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #378 | Started Training: True
2018-05-16 10:34:07.112219 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #379 | Epoch Duration: 153.16407442092896
2018-05-16 10:34:07.112381 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #379 | Started Training: True
2018-05-16 10:36:15.771933 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #380 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.66264e-05
Policy Loss                      -0.554556
Raw Policy Loss                  -0.554556
Preactivation Policy Loss         0
Q Predictions Mean                0.550984
Q Predictions Std                 0.207058
Q Predictions Max                 0.756417
Q Predictions Min                -0.124443
Q Targets Mean                    0.551355
Q Targets Std                     0.207022
Q Targets Max                     0.76015
Q Targets Min                    -0.136716
Bellman Errors Mean               4.66264e-05
Bellman Errors Std                8.80107e-05
Bellman Errors Max                0.000556776
Bellman Errors Min                9.81346e-09
Policy Action Mean                0.262216
Policy Action Std                 0.95026
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00827658
Test Rewards Std                  0.00402915
Test Rewards Max                  0.0147689
Test Rewards Min                 -5.65712e-05
Test Returns Mean                 1.27105
Test Returns Std                  0.0101239
Test Returns Max                  1.28625
Test Returns Min                  1.25646
Test Actions Mean                 0.142743
Test Actions Std                  0.978046
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00786988
Exploration Rewards Std           0.00399383
Exploration Rewards Max           0.0155569
Exploration Rewards Min          -9.62469e-05
Exploration Returns Mean          1.234
Exploration Returns Std           0.09074
Exploration Returns Max           1.31262
Exploration Returns Min           1.06773
Exploration Actions Mean          0.0956737
Exploration Actions Std           0.825167
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.27105
Number of train steps total  380863
Number of env steps total    381000
Number of rollouts total       2738
Train Time (s)                   60.053
(Previous) Eval Time (s)          2.049e-06
Sample Time (s)                  58.5292
Epoch Time (s)                  118.582
Total Train Time (s)          45148.7
Epoch                           380
---------------------------  ----------------
2018-05-16 10:41:45.939740 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #380 | Epoch Duration: 458.8272511959076
2018-05-16 10:41:45.939858 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #380 | Started Training: True
2018-05-16 10:44:20.374159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #381 | Epoch Duration: 154.43419814109802
2018-05-16 10:44:20.374341 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #381 | Started Training: True
2018-05-16 10:46:54.533188 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #382 | Epoch Duration: 154.1586856842041
2018-05-16 10:46:54.533382 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #382 | Started Training: True
2018-05-16 10:49:24.002783 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #383 | Epoch Duration: 149.46927309036255
2018-05-16 10:49:24.002966 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #383 | Started Training: True
2018-05-16 10:51:58.127781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #384 | Epoch Duration: 154.12468433380127
2018-05-16 10:51:58.127977 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #384 | Started Training: True
2018-05-16 10:54:35.307200 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #385 | Epoch Duration: 157.179039478302
2018-05-16 10:54:35.307421 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #385 | Started Training: True
2018-05-16 10:57:08.416265 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #386 | Epoch Duration: 153.10869073867798
2018-05-16 10:57:08.416459 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #386 | Started Training: True
2018-05-16 10:59:43.272417 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #387 | Epoch Duration: 154.85583066940308
2018-05-16 10:59:43.272618 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #387 | Started Training: True
2018-05-16 11:00:36.502773 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #388 | Epoch Duration: 53.230021953582764
2018-05-16 11:00:36.502978 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #388 | Started Training: True
2018-05-16 11:01:20.671827 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #389 | Epoch Duration: 44.16869902610779
2018-05-16 11:01:20.672041 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #389 | Started Training: True
2018-05-16 11:02:13.596186 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #390 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000394137
Policy Loss                      -0.613065
Raw Policy Loss                  -0.613065
Preactivation Policy Loss         0
Q Predictions Mean                0.609877
Q Predictions Std                 0.190222
Q Predictions Max                 0.853212
Q Predictions Min                -0.140846
Q Targets Mean                    0.609285
Q Targets Std                     0.195624
Q Targets Max                     0.858284
Q Targets Min                    -0.169319
Bellman Errors Mean               0.000394137
Bellman Errors Std                0.00299197
Bellman Errors Max                0.0336288
Bellman Errors Min                1.43105e-08
Policy Action Mean                0.114747
Policy Action Std                 0.980773
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00651463
Test Rewards Std                  0.00434954
Test Rewards Max                  0.0148255
Test Rewards Min                 -0.00529611
Test Returns Mean                 1.00325
Test Returns Std                  0.0634077
Test Returns Max                  1.15463
Test Returns Min                  0.955432
Test Actions Mean                 0.268676
Test Actions Std                  0.948787
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00785449
Exploration Rewards Std           0.00470182
Exploration Rewards Max           0.0159737
Exploration Rewards Min          -0.000897364
Exploration Returns Mean          0.988095
Exploration Returns Std           0.068126
Exploration Returns Max           1.07846
Exploration Returns Min           0.889667
Exploration Actions Mean          0.202628
Exploration Actions Std           0.81895
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.00325
Number of train steps total  390863
Number of env steps total    391000
Number of rollouts total       2814
Train Time (s)                   24.2101
(Previous) Eval Time (s)          2.167e-06
Sample Time (s)                  18.4522
Epoch Time (s)                   42.6623
Total Train Time (s)          46609.1
Epoch                           390
---------------------------  ----------------
2018-05-16 11:06:06.644873 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #390 | Epoch Duration: 285.9726905822754
2018-05-16 11:06:06.644989 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #390 | Started Training: True
2018-05-16 11:08:42.829906 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #391 | Epoch Duration: 156.18481588363647
2018-05-16 11:08:42.830132 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #391 | Started Training: True
2018-05-16 11:11:19.287026 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #392 | Epoch Duration: 156.45673966407776
2018-05-16 11:11:19.287206 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #392 | Started Training: True
2018-05-16 11:13:51.205383 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #393 | Epoch Duration: 151.918048620224
2018-05-16 11:13:51.205641 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #393 | Started Training: True
2018-05-16 11:16:26.418708 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #394 | Epoch Duration: 155.21286797523499
2018-05-16 11:16:26.418907 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #394 | Started Training: True
2018-05-16 11:18:59.129098 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #395 | Epoch Duration: 152.710063457489
2018-05-16 11:18:59.129291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #395 | Started Training: True
2018-05-16 11:21:34.391878 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #396 | Epoch Duration: 155.262455701828
2018-05-16 11:21:34.392106 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #396 | Started Training: True
2018-05-16 11:24:08.133783 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #397 | Epoch Duration: 153.74152421951294
2018-05-16 11:24:08.133974 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #397 | Started Training: True
2018-05-16 11:26:41.702441 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #398 | Epoch Duration: 153.56833171844482
2018-05-16 11:26:41.702598 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #398 | Started Training: True
2018-05-16 11:29:16.374216 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #399 | Epoch Duration: 154.67151260375977
2018-05-16 11:29:16.374415 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #399 | Started Training: True
2018-05-16 11:32:03.421997 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #400 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.98687e-05
Policy Loss                      -0.628629
Raw Policy Loss                  -0.628629
Preactivation Policy Loss         0
Q Predictions Mean                0.624737
Q Predictions Std                 0.193404
Q Predictions Max                 0.847646
Q Predictions Min                 0.106337
Q Targets Mean                    0.624716
Q Targets Std                     0.193301
Q Targets Max                     0.852181
Q Targets Min                     0.104538
Bellman Errors Mean               7.98686e-05
Bellman Errors Std                0.000234672
Bellman Errors Max                0.00207821
Bellman Errors Min                1.12059e-08
Policy Action Mean                0.244219
Policy Action Std                 0.96133
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00769295
Test Rewards Std                  0.0043879
Test Rewards Max                  0.0151375
Test Rewards Min                 -0.00115311
Test Returns Mean                 1.11328
Test Returns Std                  0.0174747
Test Returns Max                  1.14079
Test Returns Min                  1.09189
Test Actions Mean                 0.18074
Test Actions Std                  0.971537
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00778138
Exploration Rewards Std           0.00549872
Exploration Rewards Max           0.0186828
Exploration Rewards Min          -0.0021344
Exploration Returns Mean          1.0881
Exploration Returns Std           0.0685313
Exploration Returns Max           1.16356
Exploration Returns Min           0.991761
Exploration Actions Mean          0.17592
Exploration Actions Std           0.822505
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.11328
Number of train steps total  400863
Number of env steps total    401000
Number of rollouts total       2861
Train Time (s)                   80.6779
(Previous) Eval Time (s)          2.537e-06
Sample Time (s)                  75.644
Epoch Time (s)                  156.322
Total Train Time (s)          48383.8
Epoch                           400
---------------------------  ----------------
2018-05-16 11:35:41.566305 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #400 | Epoch Duration: 385.191769361496
2018-05-16 11:35:41.566438 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #400 | Started Training: True
2018-05-16 11:36:43.039437 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #401 | Epoch Duration: 61.472885608673096
2018-05-16 11:36:43.039656 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #401 | Started Training: True
2018-05-16 11:39:16.491769 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #402 | Epoch Duration: 153.45196723937988
2018-05-16 11:39:16.491945 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #402 | Started Training: True
2018-05-16 11:41:49.945332 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #403 | Epoch Duration: 153.4532721042633
2018-05-16 11:41:49.945530 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #403 | Started Training: True
2018-05-16 11:44:24.478474 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #404 | Epoch Duration: 154.5328152179718
2018-05-16 11:44:24.478666 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #404 | Started Training: True
2018-05-16 11:46:52.221634 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #405 | Epoch Duration: 147.74284744262695
2018-05-16 11:46:52.221820 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #405 | Started Training: True
2018-05-16 11:49:21.787752 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #406 | Epoch Duration: 149.5657980442047
2018-05-16 11:49:21.787988 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #406 | Started Training: True
2018-05-16 11:51:51.309626 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #407 | Epoch Duration: 149.52148485183716
2018-05-16 11:51:51.309857 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #407 | Started Training: True
2018-05-16 11:54:23.603320 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #408 | Epoch Duration: 152.2933120727539
2018-05-16 11:54:23.603526 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #408 | Started Training: True
2018-05-16 11:56:57.927748 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #409 | Epoch Duration: 154.32408785820007
2018-05-16 11:56:57.927911 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #409 | Started Training: True
2018-05-16 11:59:44.705437 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #410 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.75773e-05
Policy Loss                      -0.560749
Raw Policy Loss                  -0.560749
Preactivation Policy Loss         0
Q Predictions Mean                0.557416
Q Predictions Std                 0.217064
Q Predictions Max                 0.791669
Q Predictions Min                -0.0391997
Q Targets Mean                    0.55497
Q Targets Std                     0.216106
Q Targets Max                     0.781877
Q Targets Min                    -0.0433202
Bellman Errors Mean               7.75773e-05
Bellman Errors Std                0.000268692
Bellman Errors Max                0.00282976
Bellman Errors Min                1.55696e-09
Policy Action Mean                0.171463
Policy Action Std                 0.973412
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00710287
Test Rewards Std                  0.00371066
Test Rewards Max                  0.0145438
Test Rewards Min                 -0.000681601
Test Returns Mean                 1.00594
Test Returns Std                  0.169563
Test Returns Max                  1.26768
Test Returns Min                  0.830923
Test Actions Mean                 0.325149
Test Actions Std                  0.930592
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00399259
Exploration Rewards Std           0.00561986
Exploration Rewards Max           0.0162532
Exploration Rewards Min          -0.0162629
Exploration Returns Mean          0.900994
Exploration Returns Std           0.777729
Exploration Returns Max           1.42194
Exploration Returns Min          -0.786992
Exploration Actions Mean          0.18426
Exploration Actions Std           0.819442
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.00594
Number of train steps total  410863
Number of env steps total    411000
Number of rollouts total       2911
Train Time (s)                   78.852
(Previous) Eval Time (s)          2.11901e-06
Sample Time (s)                  77.0059
Epoch Time (s)                  155.858
Total Train Time (s)          50066.1
Epoch                           410
---------------------------  ----------------
2018-05-16 12:03:44.162488 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #410 | Epoch Duration: 406.234468460083
2018-05-16 12:03:44.162600 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #410 | Started Training: True
2018-05-16 12:04:26.404298 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #411 | Epoch Duration: 42.24159502983093
2018-05-16 12:04:26.405172 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #411 | Started Training: True
2018-05-16 12:05:07.846807 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #412 | Epoch Duration: 41.44143271446228
2018-05-16 12:05:07.846956 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #412 | Started Training: True
2018-05-16 12:05:50.793744 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #413 | Epoch Duration: 42.9466814994812
2018-05-16 12:05:50.793920 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #413 | Started Training: True
2018-05-16 12:06:33.355458 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #414 | Epoch Duration: 42.56142210960388
2018-05-16 12:06:33.355612 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #414 | Started Training: True
2018-05-16 12:07:15.284246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #415 | Epoch Duration: 41.928523540496826
2018-05-16 12:07:15.284397 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #415 | Started Training: True
2018-05-16 12:08:23.247924 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #416 | Epoch Duration: 67.96341562271118
2018-05-16 12:08:23.248088 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #416 | Started Training: True
2018-05-16 12:10:59.830251 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #417 | Epoch Duration: 156.58205604553223
2018-05-16 12:10:59.830468 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #417 | Started Training: True
2018-05-16 12:13:44.028841 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #418 | Epoch Duration: 164.19823598861694
2018-05-16 12:13:44.029070 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #418 | Started Training: True
2018-05-16 12:16:15.944554 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #419 | Epoch Duration: 151.91532683372498
2018-05-16 12:16:15.944782 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #419 | Started Training: True
2018-05-16 12:19:03.174719 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #420 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000119575
Policy Loss                      -0.538112
Raw Policy Loss                  -0.538112
Preactivation Policy Loss         0
Q Predictions Mean                0.534564
Q Predictions Std                 0.230261
Q Predictions Max                 0.73654
Q Predictions Min                -0.405476
Q Targets Mean                    0.534183
Q Targets Std                     0.231265
Q Targets Max                     0.736418
Q Targets Min                    -0.403405
Bellman Errors Mean               0.000119575
Bellman Errors Std                0.000417058
Bellman Errors Max                0.0028109
Bellman Errors Min                6.36022e-09
Policy Action Mean                0.305379
Policy Action Std                 0.939928
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00746548
Test Rewards Std                  0.00337906
Test Rewards Max                  0.014999
Test Rewards Min                  0.000723931
Test Returns Mean                 1.38236
Test Returns Std                  0.0162219
Test Returns Max                  1.39703
Test Returns Min                  1.3583
Test Actions Mean                 0.1344
Test Actions Std                  0.980402
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00479534
Exploration Rewards Std           0.00581756
Exploration Rewards Max           0.0163296
Exploration Rewards Min          -0.0191554
Exploration Returns Mean          0.827538
Exploration Returns Std           0.73878
Exploration Returns Max           1.42816
Exploration Returns Min          -0.781154
Exploration Actions Mean          0.106509
Exploration Actions Std           0.831175
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.38236
Number of train steps total  420863
Number of env steps total    421000
Number of rollouts total       2968
Train Time (s)                   78.8903
(Previous) Eval Time (s)          2.464e-06
Sample Time (s)                  76.961
Epoch Time (s)                  155.851
Total Train Time (s)          51228.6
Epoch                           420
---------------------------  ----------------
2018-05-16 12:23:06.862662 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #420 | Epoch Duration: 410.9177348613739
2018-05-16 12:23:06.862781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #420 | Started Training: True
2018-05-16 12:23:48.848831 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #421 | Epoch Duration: 41.985942363739014
2018-05-16 12:23:48.849058 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #421 | Started Training: True
2018-05-16 12:24:33.244818 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #422 | Epoch Duration: 44.39559817314148
2018-05-16 12:24:33.244994 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #422 | Started Training: True
2018-05-16 12:25:13.810442 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #423 | Epoch Duration: 40.56532406806946
2018-05-16 12:25:13.810660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #423 | Started Training: True
2018-05-16 12:25:53.779499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #424 | Epoch Duration: 39.96868681907654
2018-05-16 12:25:53.779686 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #424 | Started Training: True
2018-05-16 12:26:35.275341 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #425 | Epoch Duration: 41.495529890060425
2018-05-16 12:26:35.275556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #425 | Started Training: True
2018-05-16 12:27:16.181721 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #426 | Epoch Duration: 40.90601563453674
2018-05-16 12:27:16.181895 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #426 | Started Training: True
2018-05-16 12:27:57.374833 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #427 | Epoch Duration: 41.1928129196167
2018-05-16 12:27:57.375049 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #427 | Started Training: True
2018-05-16 12:28:38.700832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #428 | Epoch Duration: 41.32562184333801
2018-05-16 12:28:38.700987 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #428 | Started Training: True
2018-05-16 12:29:19.589568 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #429 | Epoch Duration: 40.88847470283508
2018-05-16 12:29:19.589774 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #429 | Started Training: True
2018-05-16 12:30:55.331419 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #430 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000106548
Policy Loss                      -0.507335
Raw Policy Loss                  -0.507335
Preactivation Policy Loss         0
Q Predictions Mean                0.504876
Q Predictions Std                 0.188986
Q Predictions Max                 0.704984
Q Predictions Min                -0.356281
Q Targets Mean                    0.509759
Q Targets Std                     0.191109
Q Targets Max                     0.712123
Q Targets Min                    -0.382591
Bellman Errors Mean               0.000106548
Bellman Errors Std                0.000370003
Bellman Errors Max                0.00404424
Bellman Errors Min                7.89797e-09
Policy Action Mean                0.148257
Policy Action Std                 0.984858
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00706045
Test Rewards Std                  0.00356892
Test Rewards Max                  0.0132641
Test Rewards Min                  9.5861e-05
Test Returns Mean                 1.18616
Test Returns Std                  0.0151601
Test Returns Max                  1.21807
Test Returns Min                  1.1722
Test Actions Mean                 0.234403
Test Actions Std                  0.960955
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00673283
Exploration Rewards Std           0.00345944
Exploration Rewards Max           0.0144822
Exploration Rewards Min          -0.00128399
Exploration Returns Mean          1.23323
Exploration Returns Std           0.103258
Exploration Returns Max           1.35094
Exploration Returns Min           1.04849
Exploration Actions Mean          0.191557
Exploration Actions Std           0.817347
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.18616
Number of train steps total  430863
Number of env steps total    431000
Number of rollouts total       3027
Train Time (s)                   43.4476
(Previous) Eval Time (s)          2.733e-06
Sample Time (s)                  40.7422
Epoch Time (s)                   84.1898
Total Train Time (s)          51948.9
Epoch                           430
---------------------------  ----------------
2018-05-16 12:35:07.351218 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #430 | Epoch Duration: 347.7612991333008
2018-05-16 12:35:07.351334 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #430 | Started Training: True
2018-05-16 12:37:41.521014 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #431 | Epoch Duration: 154.1695852279663
2018-05-16 12:37:41.521176 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #431 | Started Training: True
2018-05-16 12:40:13.919720 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #432 | Epoch Duration: 152.39843940734863
2018-05-16 12:40:13.919888 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #432 | Started Training: True
2018-05-16 12:42:40.462057 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #433 | Epoch Duration: 146.54205965995789
2018-05-16 12:42:40.462239 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #433 | Started Training: True
2018-05-16 12:43:23.280481 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #434 | Epoch Duration: 42.81812071800232
2018-05-16 12:43:23.280666 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #434 | Started Training: True
2018-05-16 12:44:05.176155 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #435 | Epoch Duration: 41.89536714553833
2018-05-16 12:44:05.176322 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #435 | Started Training: True
2018-05-16 12:44:48.604773 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #436 | Epoch Duration: 43.42833685874939
2018-05-16 12:44:48.604931 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #436 | Started Training: True
2018-05-16 12:45:31.677591 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #437 | Epoch Duration: 43.072542667388916
2018-05-16 12:45:31.677817 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #437 | Started Training: True
2018-05-16 12:46:13.948810 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #438 | Epoch Duration: 42.27082133293152
2018-05-16 12:46:13.948986 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #438 | Started Training: True
2018-05-16 12:46:55.348532 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #439 | Epoch Duration: 41.39942646026611
2018-05-16 12:46:55.348728 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #439 | Started Training: True
2018-05-16 12:47:50.115169 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #440 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.94386e-05
Policy Loss                      -0.516905
Raw Policy Loss                  -0.516905
Preactivation Policy Loss         0
Q Predictions Mean                0.514578
Q Predictions Std                 0.202366
Q Predictions Max                 0.743858
Q Predictions Min                -0.337707
Q Targets Mean                    0.514753
Q Targets Std                     0.203111
Q Targets Max                     0.743344
Q Targets Min                    -0.350884
Bellman Errors Mean               4.94386e-05
Bellman Errors Std                0.000171857
Bellman Errors Max                0.00155535
Bellman Errors Min                1.35092e-10
Policy Action Mean                0.253382
Policy Action Std                 0.956521
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00899913
Test Rewards Std                  0.00421502
Test Rewards Max                  0.0155251
Test Rewards Min                 -0.000485722
Test Returns Mean                 1.26888
Test Returns Std                  0.00412019
Test Returns Max                  1.27563
Test Returns Min                  1.26156
Test Actions Mean                 0.203947
Test Actions Std                  0.973574
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                        10
Exploration Rewards Mean          0.00764289
Exploration Rewards Std           0.00395291
Exploration Rewards Max           0.0157108
Exploration Rewards Min          -0.00108385
Exploration Returns Mean          0.992047
Exploration Returns Std           0.185581
Exploration Returns Max           1.29085
Exploration Returns Min           0.796559
Exploration Actions Mean          0.214063
Exploration Actions Std           0.802654
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.26888
Number of train steps total  440863
Number of env steps total    441000
Number of rollouts total       3092
Train Time (s)                   24.124
(Previous) Eval Time (s)          2.131e-06
Sample Time (s)                  19.3344
Epoch Time (s)                   43.4584
Total Train Time (s)          52947.3
Epoch                           440
---------------------------  ----------------
2018-05-16 12:51:45.942909 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #440 | Epoch Duration: 290.59406089782715
2018-05-16 12:51:45.943032 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #440 | Started Training: True
2018-05-16 12:54:23.689675 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #441 | Epoch Duration: 157.74653220176697
2018-05-16 12:54:23.689907 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #441 | Started Training: True
2018-05-16 12:56:58.546444 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #442 | Epoch Duration: 154.85638236999512
2018-05-16 12:56:58.546636 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #442 | Started Training: True
2018-05-16 12:59:30.833101 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #443 | Epoch Duration: 152.28633093833923
2018-05-16 12:59:30.833342 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #443 | Started Training: True
2018-05-16 13:02:07.119438 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #444 | Epoch Duration: 156.28591084480286
2018-05-16 13:02:07.119641 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #444 | Started Training: True
2018-05-16 13:04:44.673033 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #445 | Epoch Duration: 157.55325651168823
2018-05-16 13:04:44.673238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #445 | Started Training: True
2018-05-16 13:07:21.418497 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #446 | Epoch Duration: 156.7450976371765
2018-05-16 13:07:21.419298 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #446 | Started Training: True
2018-05-16 13:09:54.934906 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #447 | Epoch Duration: 153.51544070243835
2018-05-16 13:09:54.935150 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #447 | Started Training: True
2018-05-16 13:10:55.637386 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #448 | Epoch Duration: 60.70206928253174
2018-05-16 13:10:55.637600 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #448 | Started Training: True
2018-05-16 13:11:37.720993 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #449 | Epoch Duration: 42.08323836326599
2018-05-16 13:11:37.721220 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #449 | Started Training: True
2018-05-16 13:12:32.030499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #450 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000222033
Policy Loss                      -0.530708
Raw Policy Loss                  -0.530708
Preactivation Policy Loss         0
Q Predictions Mean                0.527302
Q Predictions Std                 0.168856
Q Predictions Max                 0.730066
Q Predictions Min                -0.184693
Q Targets Mean                    0.529758
Q Targets Std                     0.163148
Q Targets Max                     0.730381
Q Targets Min                    -0.183173
Bellman Errors Mean               0.000222033
Bellman Errors Std                0.00163516
Bellman Errors Max                0.0181209
Bellman Errors Min                2.71073e-09
Policy Action Mean                0.223681
Policy Action Std                 0.966286
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00755818
Test Rewards Std                  0.0039702
Test Rewards Max                  0.0145976
Test Rewards Min                 -0.000106314
Test Returns Mean                 1.29623
Test Returns Std                  0.020987
Test Returns Max                  1.31502
Test Returns Min                  1.25078
Test Actions Mean                 0.174219
Test Actions Std                  0.977137
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00715944
Exploration Rewards Std           0.00396668
Exploration Rewards Max           0.0154189
Exploration Rewards Min          -0.000241307
Exploration Returns Mean          1.29228
Exploration Returns Std           0.0945566
Exploration Returns Max           1.46827
Exploration Returns Min           1.15739
Exploration Actions Mean          0.15343
Exploration Actions Std           0.823999
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.29623
Number of train steps total  450863
Number of env steps total    451000
Number of rollouts total       3161
Train Time (s)                   23.0863
(Previous) Eval Time (s)          3.21701e-06
Sample Time (s)                  36.2977
Epoch Time (s)                   59.384
Total Train Time (s)          54431.5
Epoch                           450
---------------------------  ----------------
2018-05-16 13:16:30.434235 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #450 | Epoch Duration: 292.7128527164459
2018-05-16 13:16:30.434351 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #450 | Started Training: True
2018-05-16 13:19:01.469092 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #451 | Epoch Duration: 151.03462648391724
2018-05-16 13:19:01.469261 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #451 | Started Training: True
2018-05-16 13:21:35.290312 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #452 | Epoch Duration: 153.82094430923462
2018-05-16 13:21:35.290536 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #452 | Started Training: True
2018-05-16 13:24:09.400388 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #453 | Epoch Duration: 154.10971879959106
2018-05-16 13:24:09.400606 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #453 | Started Training: True
2018-05-16 13:26:43.158333 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #454 | Epoch Duration: 153.75757813453674
2018-05-16 13:26:43.158573 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #454 | Started Training: True
2018-05-16 13:29:16.291882 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #455 | Epoch Duration: 153.13315296173096
2018-05-16 13:29:16.292068 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #455 | Started Training: True
2018-05-16 13:31:47.619041 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #456 | Epoch Duration: 151.32683300971985
2018-05-16 13:31:47.619238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #456 | Started Training: True
2018-05-16 13:32:34.100121 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #457 | Epoch Duration: 46.480753898620605
2018-05-16 13:32:34.100304 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #457 | Started Training: True
2018-05-16 13:33:20.633119 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #458 | Epoch Duration: 46.53268766403198
2018-05-16 13:33:20.633315 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #458 | Started Training: True
2018-05-16 13:34:05.333447 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #459 | Epoch Duration: 44.70000219345093
2018-05-16 13:34:05.333659 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #459 | Started Training: True
2018-05-16 13:35:00.128276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #460 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000234277
Policy Loss                      -0.506155
Raw Policy Loss                  -0.506155
Preactivation Policy Loss         0
Q Predictions Mean                0.50209
Q Predictions Std                 0.178671
Q Predictions Max                 0.686145
Q Predictions Min                -0.184159
Q Targets Mean                    0.501924
Q Targets Std                     0.181322
Q Targets Max                     0.686164
Q Targets Min                    -0.172059
Bellman Errors Mean               0.000234277
Bellman Errors Std                0.0013871
Bellman Errors Max                0.012467
Bellman Errors Min                3.99183e-11
Policy Action Mean                0.222162
Policy Action Std                 0.964849
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00881101
Test Rewards Std                  0.00464125
Test Rewards Max                  0.0202634
Test Rewards Min                  0.000544483
Test Returns Mean                 1.03676
Test Returns Std                  0.0436444
Test Returns Max                  1.07548
Test Returns Min                  0.921019
Test Actions Mean                 0.186386
Test Actions Std                  0.968079
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00708702
Exploration Rewards Std           0.0042321
Exploration Rewards Max           0.0185141
Exploration Rewards Min          -0.00140536
Exploration Returns Mean          0.881448
Exploration Returns Std           0.130758
Exploration Returns Max           1.1013
Exploration Returns Min           0.727844
Exploration Actions Mean          0.220007
Exploration Actions Std           0.802794
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.03676
Number of train steps total  460863
Number of env steps total    461000
Number of rollouts total       3232
Train Time (s)                   23.4589
(Previous) Eval Time (s)          2.94e-06
Sample Time (s)                  19.5955
Epoch Time (s)                   43.0545
Total Train Time (s)          56228.5
Epoch                           460
---------------------------  ----------------
2018-05-16 13:46:27.631556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #460 | Epoch Duration: 742.2977447509766
2018-05-16 13:46:27.631673 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #460 | Started Training: True
2018-05-16 13:49:04.501276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #461 | Epoch Duration: 156.8695023059845
2018-05-16 13:49:04.501463 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #461 | Started Training: True
2018-05-16 13:51:38.275670 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #462 | Epoch Duration: 153.77408504486084
2018-05-16 13:51:38.275918 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #462 | Started Training: True
2018-05-16 13:54:19.278006 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #463 | Epoch Duration: 161.00192618370056
2018-05-16 13:54:19.278210 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #463 | Started Training: True
2018-05-16 13:56:57.577159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #464 | Epoch Duration: 158.29881191253662
2018-05-16 13:56:57.577316 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #464 | Started Training: True
2018-05-16 14:00:03.228928 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #465 | Epoch Duration: 185.65148091316223
2018-05-16 14:00:03.229152 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #465 | Started Training: True
2018-05-16 14:02:51.866909 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #466 | Epoch Duration: 168.6376028060913
2018-05-16 14:02:51.867139 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #466 | Started Training: True
2018-05-16 14:05:23.216880 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #467 | Epoch Duration: 151.3495888710022
2018-05-16 14:05:23.217071 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #467 | Started Training: True
2018-05-16 14:06:07.488493 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #468 | Epoch Duration: 44.271291732788086
2018-05-16 14:06:07.488706 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #468 | Started Training: True
2018-05-16 14:07:01.069890 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #469 | Epoch Duration: 53.58104848861694
2018-05-16 14:07:01.070117 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #469 | Started Training: True
2018-05-16 14:08:09.077749 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #470 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000390383
Policy Loss                      -0.526327
Raw Policy Loss                  -0.526327
Preactivation Policy Loss         0
Q Predictions Mean                0.524487
Q Predictions Std                 0.163451
Q Predictions Max                 0.736535
Q Predictions Min                -0.234765
Q Targets Mean                    0.523994
Q Targets Std                     0.156281
Q Targets Max                     0.739946
Q Targets Min                    -0.0184515
Bellman Errors Mean               0.000390383
Bellman Errors Std                0.00411769
Bellman Errors Max                0.0467914
Bellman Errors Min                1.15108e-10
Policy Action Mean                0.146907
Policy Action Std                 0.9786
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00729304
Test Rewards Std                  0.00387017
Test Rewards Max                  0.0167438
Test Rewards Min                  0.000704122
Test Returns Mean                 0.797129
Test Returns Std                  0.0181031
Test Returns Max                  0.823995
Test Returns Min                  0.763215
Test Actions Mean                 0.223388
Test Actions Std                  0.960374
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00804431
Exploration Rewards Std           0.00477578
Exploration Rewards Max           0.0184373
Exploration Rewards Min          -0.0016719
Exploration Returns Mean          0.94476
Exploration Returns Std           0.0638604
Exploration Returns Max           1.09899
Exploration Returns Min           0.894189
Exploration Actions Mean          0.159251
Exploration Actions Std           0.816692
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.797129
Number of train steps total  470863
Number of env steps total    471000
Number of rollouts total       3315
Train Time (s)                   28.4898
(Previous) Eval Time (s)          3.25e-06
Sample Time (s)                  41.9832
Epoch Time (s)                   70.4731
Total Train Time (s)          57771.4
Epoch                           470
---------------------------  ----------------
2018-05-16 14:12:10.801706 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #470 | Epoch Duration: 309.73143005371094
2018-05-16 14:12:10.801822 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #470 | Started Training: True
2018-05-16 14:15:00.886248 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #471 | Epoch Duration: 170.08432579040527
2018-05-16 14:15:00.886455 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #471 | Started Training: True
2018-05-16 14:17:35.195623 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #472 | Epoch Duration: 154.30902791023254
2018-05-16 14:17:35.195806 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #472 | Started Training: True
2018-05-16 14:20:29.858484 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #473 | Epoch Duration: 174.66255497932434
2018-05-16 14:20:29.858678 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #473 | Started Training: True
2018-05-16 14:23:20.604410 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #474 | Epoch Duration: 170.74560976028442
2018-05-16 14:23:20.604626 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #474 | Started Training: True
2018-05-16 14:26:05.811758 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #475 | Epoch Duration: 165.20697283744812
2018-05-16 14:26:05.811984 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #475 | Started Training: True
2018-05-16 14:28:48.635406 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #476 | Epoch Duration: 162.8232707977295
2018-05-16 14:28:48.635586 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #476 | Started Training: True
2018-05-16 14:30:34.183772 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #477 | Epoch Duration: 105.54807019233704
2018-05-16 14:30:34.183941 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #477 | Started Training: True
2018-05-16 14:31:21.437427 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #478 | Epoch Duration: 47.25337886810303
2018-05-16 14:31:21.437587 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #478 | Started Training: True
2018-05-16 14:32:04.766338 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #479 | Epoch Duration: 43.32864284515381
2018-05-16 14:32:04.766617 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #479 | Started Training: True
2018-05-16 14:33:00.629036 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #480 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.77878e-05
Policy Loss                      -0.496837
Raw Policy Loss                  -0.496837
Preactivation Policy Loss         0
Q Predictions Mean                0.495238
Q Predictions Std                 0.171128
Q Predictions Max                 0.682339
Q Predictions Min                -0.161892
Q Targets Mean                    0.49522
Q Targets Std                     0.172271
Q Targets Max                     0.690924
Q Targets Min                    -0.160288
Bellman Errors Mean               5.77878e-05
Bellman Errors Std                0.000397029
Bellman Errors Max                0.00449823
Bellman Errors Min                6.53845e-10
Policy Action Mean                0.174464
Policy Action Std                 0.974662
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00874326
Test Rewards Std                  0.00470032
Test Rewards Max                  0.0154256
Test Rewards Min                 -7.46172e-05
Test Returns Mean                 1.10821
Test Returns Std                  0.0448113
Test Returns Max                  1.21441
Test Returns Min                  1.05705
Test Actions Mean                 0.182286
Test Actions Std                  0.971536
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00837985
Exploration Rewards Std           0.00475594
Exploration Rewards Max           0.0180911
Exploration Rewards Min          -0.000869371
Exploration Returns Mean          1.09124
Exploration Returns Std           0.103577
Exploration Returns Max           1.21872
Exploration Returns Min           0.955348
Exploration Actions Mean          0.15019
Exploration Actions Std           0.826793
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.10821
Number of train steps total  480863
Number of env steps total    481000
Number of rollouts total       3394
Train Time (s)                   24.3221
(Previous) Eval Time (s)          2.778e-06
Sample Time (s)                 166.116
Epoch Time (s)                  190.438
Total Train Time (s)          60419.2
Epoch                           480
---------------------------  ----------------
2018-05-16 14:56:18.896556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #480 | Epoch Duration: 1454.1297507286072
2018-05-16 14:56:18.896680 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #480 | Started Training: True
2018-05-16 14:59:06.632907 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #481 | Epoch Duration: 167.73612022399902
2018-05-16 14:59:06.633096 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #481 | Started Training: True
2018-05-16 15:02:01.117002 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #482 | Epoch Duration: 174.48378109931946
2018-05-16 15:02:01.117230 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #482 | Started Training: True
2018-05-16 15:03:41.811870 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #483 | Epoch Duration: 100.69449019432068
2018-05-16 15:03:41.812047 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #483 | Started Training: True
2018-05-16 15:04:39.588311 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #484 | Epoch Duration: 57.77614212036133
2018-05-16 15:04:39.588528 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #484 | Started Training: True
2018-05-16 15:05:35.976498 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #485 | Epoch Duration: 56.38781714439392
2018-05-16 15:05:35.976723 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #485 | Started Training: True
2018-05-16 15:06:41.361336 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #486 | Epoch Duration: 65.38445997238159
2018-05-16 15:06:41.361535 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #486 | Started Training: True
2018-05-16 15:09:19.415072 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #487 | Epoch Duration: 158.05340814590454
2018-05-16 15:09:19.415276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #487 | Started Training: True
2018-05-16 15:11:55.559761 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #488 | Epoch Duration: 156.1443555355072
2018-05-16 15:11:55.559951 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #488 | Started Training: True
2018-05-16 15:14:33.146449 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #489 | Epoch Duration: 157.5863757133484
2018-05-16 15:14:33.146658 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #489 | Started Training: True
2018-05-16 15:17:21.161095 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #490 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.66181e-05
Policy Loss                      -0.499454
Raw Policy Loss                  -0.499454
Preactivation Policy Loss         0
Q Predictions Mean                0.497193
Q Predictions Std                 0.143163
Q Predictions Max                 0.670417
Q Predictions Min                -0.255649
Q Targets Mean                    0.499168
Q Targets Std                     0.143635
Q Targets Max                     0.677084
Q Targets Min                    -0.263291
Bellman Errors Mean               3.66181e-05
Bellman Errors Std                8.41881e-05
Bellman Errors Max                0.000709007
Bellman Errors Min                1.26102e-08
Policy Action Mean                0.15208
Policy Action Std                 0.976057
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00816178
Test Rewards Std                  0.00397423
Test Rewards Max                  0.0144244
Test Rewards Min                 -0.00062885
Test Returns Mean                 1.12326
Test Returns Std                  0.0122385
Test Returns Max                  1.14355
Test Returns Min                  1.10651
Test Actions Mean                 0.336494
Test Actions Std                  0.937043
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00663823
Exploration Rewards Std           0.00372199
Exploration Rewards Max           0.014905
Exploration Rewards Min          -0.00185815
Exploration Returns Mean          1.05216
Exploration Returns Std           0.147722
Exploration Returns Max           1.34097
Exploration Returns Min           0.851722
Exploration Actions Mean          0.198052
Exploration Actions Std           0.805762
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.12326
Number of train steps total  490863
Number of env steps total    491000
Number of rollouts total       3446
Train Time (s)                   80.1281
(Previous) Eval Time (s)          2.361e-06
Sample Time (s)                  74.9557
Epoch Time (s)                  155.084
Total Train Time (s)          61931
Epoch                           490
---------------------------  ----------------
2018-05-16 15:21:30.914242 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #490 | Epoch Duration: 417.76744413375854
2018-05-16 15:21:30.914374 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #490 | Started Training: True
2018-05-16 15:22:13.018014 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #491 | Epoch Duration: 42.10352420806885
2018-05-16 15:22:13.018193 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #491 | Started Training: True
2018-05-16 15:22:56.125940 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #492 | Epoch Duration: 43.107622385025024
2018-05-16 15:22:56.126200 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #492 | Started Training: True
2018-05-16 15:23:38.500252 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #493 | Epoch Duration: 42.37386465072632
2018-05-16 15:23:38.500420 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #493 | Started Training: True
2018-05-16 15:24:21.652637 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #494 | Epoch Duration: 43.15211248397827
2018-05-16 15:24:21.652814 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #494 | Started Training: True
2018-05-16 15:26:39.909231 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #495 | Epoch Duration: 138.2562952041626
2018-05-16 15:26:39.909435 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #495 | Started Training: True
2018-05-16 15:29:14.083975 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #496 | Epoch Duration: 154.17440819740295
2018-05-16 15:29:14.084186 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #496 | Started Training: True
2018-05-16 15:31:53.063626 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #497 | Epoch Duration: 158.97931003570557
2018-05-16 15:31:53.063851 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #497 | Started Training: True
2018-05-16 15:34:23.741096 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #498 | Epoch Duration: 150.67709183692932
2018-05-16 15:34:23.741278 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #498 | Started Training: True
2018-05-16 15:37:00.101159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #499 | Epoch Duration: 156.35976481437683
2018-05-16 15:37:00.101313 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #499 | Started Training: True
2018-05-16 15:39:46.925892 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #500 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.28473e-05
Policy Loss                      -0.495024
Raw Policy Loss                  -0.495024
Preactivation Policy Loss         0
Q Predictions Mean                0.492188
Q Predictions Std                 0.189916
Q Predictions Max                 0.694801
Q Predictions Min                -0.0129243
Q Targets Mean                    0.494019
Q Targets Std                     0.191132
Q Targets Max                     0.696531
Q Targets Min                    -0.0232347
Bellman Errors Mean               7.28473e-05
Bellman Errors Std                0.00043535
Bellman Errors Max                0.00483975
Bellman Errors Min                1.45519e-11
Policy Action Mean                0.359387
Policy Action Std                 0.924662
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                -1.12044e-05
Test Rewards Std                  0.00106091
Test Rewards Max                  0.0132282
Test Rewards Min                 -0.00202657
Test Returns Mean                -0.0112044
Test Returns Std                  0.00057975
Test Returns Max                 -0.0106246
Test Returns Min                 -0.0117841
Test Actions Mean                 0.340156
Test Actions Std                  0.904496
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00595327
Exploration Rewards Std           0.00364189
Exploration Rewards Max           0.0148424
Exploration Rewards Min          -0.00150758
Exploration Returns Mean          1.1597
Exploration Returns Std           0.04871
Exploration Returns Max           1.23185
Exploration Returns Min           1.09613
Exploration Actions Mean          0.158979
Exploration Actions Std           0.820373
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                    -0.0112044
Number of train steps total  500863
Number of env steps total    501000
Number of rollouts total       3514
Train Time (s)                   78.1989
(Previous) Eval Time (s)          2.361e-06
Sample Time (s)                  75.573
Epoch Time (s)                  153.772
Total Train Time (s)          63338.3
Epoch                           500
---------------------------  ----------------
2018-05-16 15:44:58.467781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #500 | Epoch Duration: 478.36637115478516
2018-05-16 15:44:58.467891 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #500 | Started Training: True
2018-05-16 15:45:38.316275 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #501 | Epoch Duration: 39.84828567504883
2018-05-16 15:45:38.316452 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #501 | Started Training: True
2018-05-16 15:47:55.414191 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #502 | Epoch Duration: 137.09761452674866
2018-05-16 15:47:55.414459 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #502 | Started Training: True
2018-05-16 15:50:27.410637 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #503 | Epoch Duration: 151.99601078033447
2018-05-16 15:50:27.410837 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #503 | Started Training: True
2018-05-16 15:52:54.297275 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #504 | Epoch Duration: 146.88630652427673
2018-05-16 15:52:54.297493 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #504 | Started Training: True
2018-05-16 15:55:28.025122 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #505 | Epoch Duration: 153.7274751663208
2018-05-16 15:55:28.025342 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #505 | Started Training: True
2018-05-16 15:58:03.078209 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #506 | Epoch Duration: 155.05271792411804
2018-05-16 15:58:03.078369 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #506 | Started Training: True
2018-05-16 16:00:31.578594 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #507 | Epoch Duration: 148.50009107589722
2018-05-16 16:00:31.578829 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #507 | Started Training: True
2018-05-16 16:03:03.803652 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #508 | Epoch Duration: 152.22465348243713
2018-05-16 16:03:03.803860 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #508 | Started Training: True
2018-05-16 16:05:36.786913 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #509 | Epoch Duration: 152.98292088508606
2018-05-16 16:05:36.787133 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #509 | Started Training: True
2018-05-16 16:08:22.738401 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #510 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000106366
Policy Loss                      -0.540269
Raw Policy Loss                  -0.540269
Preactivation Policy Loss         0
Q Predictions Mean                0.537266
Q Predictions Std                 0.172884
Q Predictions Max                 0.758546
Q Predictions Min                -0.0366789
Q Targets Mean                    0.537756
Q Targets Std                     0.174116
Q Targets Max                     0.749021
Q Targets Min                    -0.0422281
Bellman Errors Mean               0.000106366
Bellman Errors Std                0.000479229
Bellman Errors Max                0.00520396
Bellman Errors Min                1.02068e-09
Policy Action Mean                0.263274
Policy Action Std                 0.956012
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00840738
Test Rewards Std                  0.00437221
Test Rewards Max                  0.0168188
Test Rewards Min                 -0.000665497
Test Returns Mean                 1.37881
Test Returns Std                  0.0203629
Test Returns Max                  1.40504
Test Returns Min                  1.33708
Test Actions Mean                 0.431426
Test Actions Std                  0.889287
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00768496
Exploration Rewards Std           0.00493945
Exploration Rewards Max           0.0200308
Exploration Rewards Min          -0.00237619
Exploration Returns Mean          1.11347
Exploration Returns Std           0.217921
Exploration Returns Max           1.36492
Exploration Returns Min           0.805632
Exploration Actions Mean          0.23422
Exploration Actions Std           0.80578
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.37881
Number of train steps total  510863
Number of env steps total    511000
Number of rollouts total       3556
Train Time (s)                   78.3794
(Previous) Eval Time (s)          2.449e-06
Sample Time (s)                  74.2086
Epoch Time (s)                  152.588
Total Train Time (s)          64965.9
Epoch                           510
---------------------------  ----------------
2018-05-16 16:12:06.344130 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #510 | Epoch Duration: 389.5568549633026
2018-05-16 16:12:06.344240 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #510 | Started Training: True
2018-05-16 16:13:12.163530 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #511 | Epoch Duration: 65.81918430328369
2018-05-16 16:13:12.163695 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #511 | Started Training: True
2018-05-16 16:15:44.614962 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #512 | Epoch Duration: 152.45110511779785
2018-05-16 16:15:44.615144 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #512 | Started Training: True
2018-05-16 16:18:21.315791 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #513 | Epoch Duration: 156.70051264762878
2018-05-16 16:18:21.316081 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #513 | Started Training: True
2018-05-16 16:20:53.428318 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #514 | Epoch Duration: 152.11207818984985
2018-05-16 16:20:53.428552 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #514 | Started Training: True
2018-05-16 16:23:26.265416 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #515 | Epoch Duration: 152.83671474456787
2018-05-16 16:23:26.265642 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #515 | Started Training: True
2018-05-16 16:25:56.225293 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #516 | Epoch Duration: 149.9595010280609
2018-05-16 16:25:56.225500 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #516 | Started Training: True
2018-05-16 16:28:31.747697 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #517 | Epoch Duration: 155.52206158638
2018-05-16 16:28:31.747860 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #517 | Started Training: True
2018-05-16 16:31:03.898827 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #518 | Epoch Duration: 152.150860786438
2018-05-16 16:31:03.899018 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #518 | Started Training: True
2018-05-16 16:33:42.428739 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #519 | Epoch Duration: 158.52959275245667
2018-05-16 16:33:42.428948 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #519 | Started Training: True
2018-05-16 16:36:27.646934 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #520 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.26804e-05
Policy Loss                      -0.489954
Raw Policy Loss                  -0.489954
Preactivation Policy Loss         0
Q Predictions Mean                0.487806
Q Predictions Std                 0.177069
Q Predictions Max                 0.682894
Q Predictions Min                -0.229692
Q Targets Mean                    0.484044
Q Targets Std                     0.179834
Q Targets Max                     0.674145
Q Targets Min                    -0.246271
Bellman Errors Mean               7.26805e-05
Bellman Errors Std                0.000208204
Bellman Errors Max                0.00206987
Bellman Errors Min                6.53845e-10
Policy Action Mean                0.343709
Policy Action Std                 0.931598
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00956897
Test Rewards Std                  0.00465787
Test Rewards Max                  0.0159897
Test Rewards Min                  0.000429056
Test Returns Mean                 1.19293
Test Returns Std                  0.0383234
Test Returns Max                  1.27585
Test Returns Min                  1.15906
Test Actions Mean                 0.297038
Test Actions Std                  0.950118
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00886724
Exploration Rewards Std           0.00445833
Exploration Rewards Max           0.0171851
Exploration Rewards Min          -0.000956673
Exploration Returns Mean          1.16937
Exploration Returns Std           0.107107
Exploration Returns Max           1.30026
Exploration Returns Min           1.02419
Exploration Actions Mean          0.206672
Exploration Actions Std           0.809952
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.19293
Number of train steps total  520863
Number of env steps total    521000
Number of rollouts total       3621
Train Time (s)                   77.7527
(Previous) Eval Time (s)          2.862e-06
Sample Time (s)                  74.0646
Epoch Time (s)                  151.817
Total Train Time (s)          66652.6
Epoch                           520
---------------------------  ----------------
2018-05-16 16:40:13.290069 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #520 | Epoch Duration: 390.8609998226166
2018-05-16 16:40:13.290178 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #520 | Started Training: True
2018-05-16 16:42:11.501291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #521 | Epoch Duration: 118.21100997924805
2018-05-16 16:42:11.501521 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #521 | Started Training: True
2018-05-16 16:44:43.939732 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #522 | Epoch Duration: 152.43805742263794
2018-05-16 16:44:43.939963 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #522 | Started Training: True
2018-05-16 16:47:16.871428 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #523 | Epoch Duration: 152.93131065368652
2018-05-16 16:47:16.871649 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #523 | Started Training: True
2018-05-16 16:49:49.765851 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #524 | Epoch Duration: 152.8940465450287
2018-05-16 16:49:49.766071 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #524 | Started Training: True
2018-05-16 16:52:21.049030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #525 | Epoch Duration: 151.28279948234558
2018-05-16 16:52:21.049224 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #525 | Started Training: True
2018-05-16 16:54:59.024698 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #526 | Epoch Duration: 157.97534561157227
2018-05-16 16:54:59.024927 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #526 | Started Training: True
2018-05-16 16:57:51.738522 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #527 | Epoch Duration: 172.7134370803833
2018-05-16 16:57:51.738717 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #527 | Started Training: True
2018-05-16 17:00:39.107482 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #528 | Epoch Duration: 167.36864066123962
2018-05-16 17:00:39.107651 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #528 | Started Training: True
2018-05-16 17:03:29.622431 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #529 | Epoch Duration: 170.5146722793579
2018-05-16 17:03:29.622607 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #529 | Started Training: True
2018-05-16 17:06:36.997802 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #530 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.00040754
Policy Loss                      -0.503249
Raw Policy Loss                  -0.503249
Preactivation Policy Loss         0
Q Predictions Mean                0.501076
Q Predictions Std                 0.213573
Q Predictions Max                 0.703893
Q Predictions Min                -0.384225
Q Targets Mean                    0.501085
Q Targets Std                     0.209698
Q Targets Max                     0.699838
Q Targets Min                    -0.381338
Bellman Errors Mean               0.00040754
Bellman Errors Std                0.00427162
Bellman Errors Max                0.0485419
Bellman Errors Min                2.68905e-11
Policy Action Mean                0.241717
Policy Action Std                 0.962343
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00992926
Test Rewards Std                  0.00454734
Test Rewards Max                  0.0155881
Test Rewards Min                 -0.000565773
Test Returns Mean                 1.27963
Test Returns Std                  0.026164
Test Returns Max                  1.3219
Test Returns Min                  1.23427
Test Actions Mean                 0.222694
Test Actions Std                  0.969556
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00897413
Exploration Rewards Std           0.00515676
Exploration Rewards Max           0.0195179
Exploration Rewards Min          -0.000529525
Exploration Returns Mean          1.07191
Exploration Returns Std           0.0820123
Exploration Returns Max           1.19215
Exploration Returns Min           0.945861
Exploration Actions Mean          0.160529
Exploration Actions Std           0.825205
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.27963
Number of train steps total  530863
Number of env steps total    531000
Number of rollouts total       3698
Train Time (s)                   92.5386
(Previous) Eval Time (s)          2.26199e-06
Sample Time (s)                 157.357
Epoch Time (s)                  249.896
Total Train Time (s)          68448.1
Epoch                           530
---------------------------  ----------------
2018-05-16 17:10:09.009650 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #530 | Epoch Duration: 399.3869400024414
2018-05-16 17:10:09.009757 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #530 | Started Training: True
2018-05-16 17:11:38.331552 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #531 | Epoch Duration: 89.32169818878174
2018-05-16 17:11:38.331715 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #531 | Started Training: True
2018-05-16 17:14:30.087586 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #532 | Epoch Duration: 171.7557566165924
2018-05-16 17:14:30.087793 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #532 | Started Training: True
2018-05-16 17:17:24.881852 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #533 | Epoch Duration: 174.7939248085022
2018-05-16 17:17:24.882075 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #533 | Started Training: True
2018-05-16 17:20:18.529014 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #534 | Epoch Duration: 173.6467854976654
2018-05-16 17:20:18.529229 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #534 | Started Training: True
2018-05-16 17:23:31.191710 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #535 | Epoch Duration: 192.6623477935791
2018-05-16 17:23:31.191933 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #535 | Started Training: True
2018-05-16 17:26:41.544083 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #536 | Epoch Duration: 190.35199284553528
2018-05-16 17:26:41.544389 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #536 | Started Training: True
2018-05-16 17:29:18.180608 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #537 | Epoch Duration: 156.6360216140747
2018-05-16 17:29:18.180807 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #537 | Started Training: True
2018-05-16 17:31:48.295695 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #538 | Epoch Duration: 150.11474752426147
2018-05-16 17:31:48.295854 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #538 | Started Training: True
2018-05-16 17:34:19.678125 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #539 | Epoch Duration: 151.38216733932495
2018-05-16 17:34:19.678299 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #539 | Started Training: True
2018-05-16 17:37:08.771385 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #540 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.70227e-05
Policy Loss                      -0.522696
Raw Policy Loss                  -0.522696
Preactivation Policy Loss         0
Q Predictions Mean                0.519365
Q Predictions Std                 0.216734
Q Predictions Max                 0.780429
Q Predictions Min                -0.579821
Q Targets Mean                    0.521092
Q Targets Std                     0.220404
Q Targets Max                     0.764768
Q Targets Min                    -0.58353
Bellman Errors Mean               9.70227e-05
Bellman Errors Std                0.000447136
Bellman Errors Max                0.0048759
Bellman Errors Min                2.16147e-11
Policy Action Mean                0.200091
Policy Action Std                 0.971847
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00982749
Test Rewards Std                  0.00497861
Test Rewards Max                  0.0171426
Test Rewards Min                  0.000412194
Test Returns Mean                 1.22734
Test Returns Std                  0.0148292
Test Returns Max                  1.24823
Test Returns Min                  1.19992
Test Actions Mean                 0.301394
Test Actions Std                  0.946285
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00867663
Exploration Rewards Std           0.00472616
Exploration Rewards Max           0.0166192
Exploration Rewards Min          -0.00169534
Exploration Returns Mean          1.11681
Exploration Returns Std           0.122001
Exploration Returns Max           1.33712
Exploration Returns Min           0.965417
Exploration Actions Mean          0.243892
Exploration Actions Std           0.802562
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.22734
Number of train steps total  540863
Number of env steps total    541000
Number of rollouts total       3781
Train Time (s)                   80.2456
(Previous) Eval Time (s)          1.718e-06
Sample Time (s)                  74.806
Epoch Time (s)                  155.052
Total Train Time (s)          71523.5
Epoch                           540
---------------------------  ----------------
2018-05-16 18:01:24.652806 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #540 | Epoch Duration: 1624.9743931293488
2018-05-16 18:01:24.652982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #540 | Started Training: True
2018-05-16 18:03:57.259823 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #541 | Epoch Duration: 152.60669326782227
2018-05-16 18:03:57.260030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #541 | Started Training: True
2018-05-16 18:06:30.441924 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #542 | Epoch Duration: 153.18175387382507
2018-05-16 18:06:30.442124 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #542 | Started Training: True
2018-05-16 18:09:01.595490 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #543 | Epoch Duration: 151.15324306488037
2018-05-16 18:09:01.602475 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #543 | Started Training: True
2018-05-16 18:11:31.664489 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #544 | Epoch Duration: 150.0617916584015
2018-05-16 18:11:31.664711 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #544 | Started Training: True
2018-05-16 18:13:58.825198 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #545 | Epoch Duration: 147.1603331565857
2018-05-16 18:13:58.825399 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #545 | Started Training: True
2018-05-16 18:16:29.295851 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #546 | Epoch Duration: 150.4703266620636
2018-05-16 18:16:29.296049 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #546 | Started Training: True
2018-05-16 18:18:59.515695 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #547 | Epoch Duration: 150.2195167541504
2018-05-16 18:18:59.515881 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #547 | Started Training: True
2018-05-16 18:21:42.660456 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #548 | Epoch Duration: 163.14445614814758
2018-05-16 18:21:42.660621 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #548 | Started Training: True
2018-05-16 18:22:30.170107 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #549 | Epoch Duration: 47.5093777179718
2018-05-16 18:22:30.170331 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #549 | Started Training: True
2018-05-16 18:23:35.184243 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #550 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.42509e-05
Policy Loss                      -0.579183
Raw Policy Loss                  -0.579183
Preactivation Policy Loss         0
Q Predictions Mean                0.575867
Q Predictions Std                 0.186099
Q Predictions Max                 0.794368
Q Predictions Min                -0.291525
Q Targets Mean                    0.574648
Q Targets Std                     0.186268
Q Targets Max                     0.792482
Q Targets Min                    -0.308332
Bellman Errors Mean               3.42509e-05
Bellman Errors Std                6.87564e-05
Bellman Errors Max                0.000486933
Bellman Errors Min                1.11811e-09
Policy Action Mean                0.257173
Policy Action Std                 0.955377
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00822637
Test Rewards Std                  0.0035913
Test Rewards Max                  0.013995
Test Rewards Min                 -0.000474389
Test Returns Mean                 1.27626
Test Returns Std                  0.0175247
Test Returns Max                  1.3137
Test Returns Min                  1.26253
Test Actions Mean                 0.212373
Test Actions Std                  0.964165
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00894545
Exploration Rewards Std           0.00452756
Exploration Rewards Max           0.017451
Exploration Rewards Min          -4.17996e-05
Exploration Returns Mean          1.24541
Exploration Returns Std           0.0612681
Exploration Returns Max           1.3271
Exploration Returns Min           1.12304
Exploration Actions Mean          0.128002
Exploration Actions Std           0.834675
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.27626
Number of train steps total  550863
Number of env steps total    551000
Number of rollouts total       3859
Train Time (s)                   26.406
(Previous) Eval Time (s)          2.202e-06
Sample Time (s)                  63.6665
Epoch Time (s)                   90.0725
Total Train Time (s)          73087.8
Epoch                           550
---------------------------  ----------------
2018-05-16 18:27:29.203566 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #550 | Epoch Duration: 299.0330684185028
2018-05-16 18:27:29.203710 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #550 | Started Training: True
2018-05-16 18:30:07.758828 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #551 | Epoch Duration: 158.55500316619873
2018-05-16 18:30:07.759073 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #551 | Started Training: True
2018-05-16 18:32:41.824107 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #552 | Epoch Duration: 154.0648548603058
2018-05-16 18:32:41.824294 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #552 | Started Training: True
2018-05-16 18:35:13.273832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #553 | Epoch Duration: 151.44941639900208
2018-05-16 18:35:13.274021 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #553 | Started Training: True
2018-05-16 18:37:46.067897 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #554 | Epoch Duration: 152.79374742507935
2018-05-16 18:37:46.068135 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #554 | Started Training: True
2018-05-16 18:40:17.556079 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #555 | Epoch Duration: 151.4877893924713
2018-05-16 18:40:17.556346 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #555 | Started Training: True
2018-05-16 18:42:51.262265 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #556 | Epoch Duration: 153.70572543144226
2018-05-16 18:42:51.262509 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #556 | Started Training: True
2018-05-16 18:43:58.617253 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #557 | Epoch Duration: 67.35458707809448
2018-05-16 18:43:58.617461 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #557 | Started Training: True
2018-05-16 18:44:40.384002 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #558 | Epoch Duration: 41.76640176773071
2018-05-16 18:44:40.384157 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #558 | Started Training: True
2018-05-16 18:45:24.366662 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #559 | Epoch Duration: 43.982394218444824
2018-05-16 18:45:24.366910 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #559 | Started Training: True
2018-05-16 18:46:21.071150 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #560 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           8.70393e-05
Policy Loss                      -0.548989
Raw Policy Loss                  -0.548989
Preactivation Policy Loss         0
Q Predictions Mean                0.545875
Q Predictions Std                 0.233189
Q Predictions Max                 0.817277
Q Predictions Min                -0.577701
Q Targets Mean                    0.548924
Q Targets Std                     0.231998
Q Targets Max                     0.820582
Q Targets Min                    -0.588678
Bellman Errors Mean               8.70393e-05
Bellman Errors Std                0.000305268
Bellman Errors Max                0.00313468
Bellman Errors Min                1.21592e-12
Policy Action Mean                0.172829
Policy Action Std                 0.974784
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00774323
Test Rewards Std                  0.00339559
Test Rewards Max                  0.0138404
Test Rewards Min                 -0.00067355
Test Returns Mean                 0.729272
Test Returns Std                  0.209976
Test Returns Max                  1.18141
Test Returns Min                  0.58939
Test Actions Mean                 0.0935842
Test Actions Std                  0.978426
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00830405
Exploration Rewards Std           0.00437595
Exploration Rewards Max           0.01842
Exploration Rewards Min          -0.000427721
Exploration Returns Mean          0.859008
Exploration Returns Std           0.156546
Exploration Returns Max           1.15099
Exploration Returns Min           0.656005
Exploration Actions Mean          0.115127
Exploration Actions Std           0.830907
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.729272
Number of train steps total  560863
Number of env steps total    561000
Number of rollouts total       3942
Train Time (s)                   24.3819
(Previous) Eval Time (s)          2.705e-06
Sample Time (s)                  18.3654
Epoch Time (s)                   42.7473
Total Train Time (s)          74463.3
Epoch                           560
---------------------------  ----------------
2018-05-16 18:50:25.000414 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #560 | Epoch Duration: 300.63331627845764
2018-05-16 18:50:25.000538 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #560 | Started Training: True
2018-05-16 18:52:56.200114 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #561 | Epoch Duration: 151.19946908950806
2018-05-16 18:52:56.200310 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #561 | Started Training: True
2018-05-16 18:55:31.085099 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #562 | Epoch Duration: 154.88462257385254
2018-05-16 18:55:31.085309 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #562 | Started Training: True
2018-05-16 18:58:01.325281 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #563 | Epoch Duration: 150.23983526229858
2018-05-16 18:58:01.325497 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #563 | Started Training: True
2018-05-16 19:00:30.724623 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #564 | Epoch Duration: 149.398987531662
2018-05-16 19:00:30.724806 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #564 | Started Training: True
2018-05-16 19:03:05.463423 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #565 | Epoch Duration: 154.73848843574524
2018-05-16 19:03:05.463582 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #565 | Started Training: True
2018-05-16 19:05:38.145111 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #566 | Epoch Duration: 152.68142199516296
2018-05-16 19:05:38.145306 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #566 | Started Training: True
2018-05-16 19:08:13.659749 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #567 | Epoch Duration: 155.51431369781494
2018-05-16 19:08:13.659969 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #567 | Started Training: True
2018-05-16 19:09:05.223963 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #568 | Epoch Duration: 51.56384086608887
2018-05-16 19:09:05.224154 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #568 | Started Training: True
2018-05-16 19:09:47.694401 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #569 | Epoch Duration: 42.47012376785278
2018-05-16 19:09:47.694588 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #569 | Started Training: True
2018-05-16 19:10:43.616629 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #570 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           8.30839e-05
Policy Loss                      -0.528106
Raw Policy Loss                  -0.528106
Preactivation Policy Loss         0
Q Predictions Mean                0.526439
Q Predictions Std                 0.176245
Q Predictions Max                 0.69619
Q Predictions Min                -0.614025
Q Targets Mean                    0.526021
Q Targets Std                     0.176342
Q Targets Max                     0.697133
Q Targets Min                    -0.627691
Bellman Errors Mean               8.30839e-05
Bellman Errors Std                0.000551641
Bellman Errors Max                0.00623688
Bellman Errors Min                1.83151e-09
Policy Action Mean                0.111553
Policy Action Std                 0.987867
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00781612
Test Rewards Std                  0.00395137
Test Rewards Max                  0.0165612
Test Rewards Min                 -0.000247743
Test Returns Mean                 0.798807
Test Returns Std                  0.053681
Test Returns Max                  0.941149
Test Returns Min                  0.74307
Test Actions Mean                 0.164179
Test Actions Std                  0.971381
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         4
Exploration Rewards Mean          0.00720075
Exploration Rewards Std           0.00366081
Exploration Rewards Max           0.0143504
Exploration Rewards Min           0.000249272
Exploration Returns Mean          1.20252
Exploration Returns Std           0.129069
Exploration Returns Max           1.3361
Exploration Returns Min           1.03811
Exploration Actions Mean          0.172
Exploration Actions Std           0.826365
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.798807
Number of train steps total  570863
Number of env steps total    571000
Number of rollouts total       3999
Train Time (s)                   23.3083
(Previous) Eval Time (s)          1.825e-06
Sample Time (s)                  18.3475
Epoch Time (s)                   41.6558
Total Train Time (s)          75892
Epoch                           570
---------------------------  ----------------
2018-05-16 19:14:13.903154 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #570 | Epoch Duration: 266.2084593772888
2018-05-16 19:14:13.903263 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #570 | Started Training: True
2018-05-16 19:14:55.145069 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #571 | Epoch Duration: 41.24171018600464
2018-05-16 19:14:55.145246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #571 | Started Training: True
2018-05-16 19:15:39.181015 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #572 | Epoch Duration: 44.03565192222595
2018-05-16 19:15:39.181175 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #572 | Started Training: True
2018-05-16 19:16:37.156393 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #573 | Epoch Duration: 57.97510385513306
2018-05-16 19:16:37.157280 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #573 | Started Training: True
2018-05-16 19:19:13.531385 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #574 | Epoch Duration: 156.37394762039185
2018-05-16 19:19:13.531607 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #574 | Started Training: True
2018-05-16 19:21:47.881172 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #575 | Epoch Duration: 154.34941124916077
2018-05-16 19:21:47.881412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #575 | Started Training: True
2018-05-16 19:24:23.271267 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #576 | Epoch Duration: 155.3896999359131
2018-05-16 19:24:23.271511 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #576 | Started Training: True
2018-05-16 19:26:58.094458 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #577 | Epoch Duration: 154.82279181480408
2018-05-16 19:26:58.094642 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #577 | Started Training: True
2018-05-16 19:29:43.136969 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #578 | Epoch Duration: 165.04219698905945
2018-05-16 19:29:43.137195 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #578 | Started Training: True
2018-05-16 19:32:29.357495 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #579 | Epoch Duration: 166.22015285491943
2018-05-16 19:32:29.357740 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #579 | Started Training: True
2018-05-16 19:35:19.550841 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #580 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           9.00563e-05
Policy Loss                      -0.489614
Raw Policy Loss                  -0.489614
Preactivation Policy Loss         0
Q Predictions Mean                0.48774
Q Predictions Std                 0.147803
Q Predictions Max                 0.597238
Q Predictions Min                -0.0804282
Q Targets Mean                    0.488545
Q Targets Std                     0.144071
Q Targets Max                     0.59532
Q Targets Min                    -0.014415
Bellman Errors Mean               9.00563e-05
Bellman Errors Std                0.000748123
Bellman Errors Max                0.00850841
Bellman Errors Min                9.79217e-12
Policy Action Mean                0.12485
Policy Action Std                 0.980819
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00904218
Test Rewards Std                  0.00397797
Test Rewards Max                  0.0151268
Test Rewards Min                 -0.000197327
Test Returns Mean                 1.13705
Test Returns Std                  0.0113259
Test Returns Max                  1.15355
Test Returns Min                  1.1145
Test Actions Mean                 0.143483
Test Actions Std                  0.978811
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00804274
Exploration Rewards Std           0.00445337
Exploration Rewards Max           0.0171663
Exploration Rewards Min          -0.000796115
Exploration Returns Mean          1.01964
Exploration Returns Std           0.198603
Exploration Returns Max           1.49152
Exploration Returns Min           0.780786
Exploration Actions Mean          0.0739047
Exploration Actions Std           0.841526
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.13705
Number of train steps total  580863
Number of env steps total    581000
Number of rollouts total       4074
Train Time (s)                   80.9762
(Previous) Eval Time (s)          3.166e-06
Sample Time (s)                  74.5053
Epoch Time (s)                  155.482
Total Train Time (s)          77580.3
Epoch                           580
---------------------------  ----------------
2018-05-16 19:42:22.532790 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #580 | Epoch Duration: 593.1748695373535
2018-05-16 19:42:22.532924 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #580 | Started Training: True
2018-05-16 19:44:59.985887 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #581 | Epoch Duration: 157.4528477191925
2018-05-16 19:44:59.986084 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #581 | Started Training: True
2018-05-16 19:47:37.709209 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #582 | Epoch Duration: 157.7229609489441
2018-05-16 19:47:37.709439 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #582 | Started Training: True
2018-05-16 19:50:09.916949 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #583 | Epoch Duration: 152.20735883712769
2018-05-16 19:50:09.917156 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #583 | Started Training: True
2018-05-16 19:52:40.199096 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #584 | Epoch Duration: 150.2818033695221
2018-05-16 19:52:40.199327 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #584 | Started Training: True
2018-05-16 19:55:11.276347 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #585 | Epoch Duration: 151.07686400413513
2018-05-16 19:55:11.276528 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #585 | Started Training: True
2018-05-16 19:57:45.291732 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #586 | Epoch Duration: 154.01508331298828
2018-05-16 19:57:45.291963 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #586 | Started Training: True
2018-05-16 20:00:21.132744 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #587 | Epoch Duration: 155.8406322002411
2018-05-16 20:00:21.132913 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #587 | Started Training: True
2018-05-16 20:02:57.840451 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #588 | Epoch Duration: 156.7074236869812
2018-05-16 20:02:57.840651 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #588 | Started Training: True
2018-05-16 20:05:41.999449 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #589 | Epoch Duration: 164.15866708755493
2018-05-16 20:05:41.999648 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #589 | Started Training: True
2018-05-16 20:08:31.619522 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #590 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.08147e-05
Policy Loss                      -0.478128
Raw Policy Loss                  -0.478128
Preactivation Policy Loss         0
Q Predictions Mean                0.476882
Q Predictions Std                 0.161014
Q Predictions Max                 0.58231
Q Predictions Min                -0.388523
Q Targets Mean                    0.476209
Q Targets Std                     0.160085
Q Targets Max                     0.583447
Q Targets Min                    -0.392182
Bellman Errors Mean               3.08147e-05
Bellman Errors Std                7.97102e-05
Bellman Errors Max                0.000608358
Bellman Errors Min                2.51576e-09
Policy Action Mean                0.143735
Policy Action Std                 0.975233
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00716438
Test Rewards Std                  0.00310201
Test Rewards Max                  0.0147064
Test Rewards Min                 -3.65975e-05
Test Returns Mean                 1.21197
Test Returns Std                  0.152235
Test Returns Max                  1.31286
Test Returns Min                  0.885785
Test Actions Mean                 0.093145
Test Actions Std                  0.981091
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00692676
Exploration Rewards Std           0.00393267
Exploration Rewards Max           0.0158994
Exploration Rewards Min          -0.000369276
Exploration Returns Mean          0.999432
Exploration Returns Std           0.139507
Exploration Returns Max           1.27584
Exploration Returns Min           0.827684
Exploration Actions Mean          0.1334
Exploration Actions Std           0.830569
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.21197
Number of train steps total  590863
Number of env steps total    591000
Number of rollouts total       4136
Train Time (s)                   79.7601
(Previous) Eval Time (s)          2.324e-06
Sample Time (s)                  74.8778
Epoch Time (s)                  154.638
Total Train Time (s)          79343.9
Epoch                           590
---------------------------  ----------------
2018-05-16 20:11:46.363842 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #590 | Epoch Duration: 364.364075422287
2018-05-16 20:11:46.363953 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #590 | Started Training: True
2018-05-16 20:12:39.172067 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #591 | Epoch Duration: 52.80801177024841
2018-05-16 20:12:39.172280 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #591 | Started Training: True
2018-05-16 20:15:11.766412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #592 | Epoch Duration: 152.59396862983704
2018-05-16 20:15:11.766610 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #592 | Started Training: True
2018-05-16 20:17:46.048645 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #593 | Epoch Duration: 154.28190517425537
2018-05-16 20:17:46.048834 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #593 | Started Training: True
2018-05-16 20:20:20.794139 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #594 | Epoch Duration: 154.74517154693604
2018-05-16 20:20:20.806540 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #594 | Started Training: True
2018-05-16 20:22:59.662800 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #595 | Epoch Duration: 158.85609555244446
2018-05-16 20:22:59.662993 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #595 | Started Training: True
2018-05-16 20:25:37.268420 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #596 | Epoch Duration: 157.60529732704163
2018-05-16 20:25:37.268647 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #596 | Started Training: True
2018-05-16 20:28:11.897089 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #597 | Epoch Duration: 154.62828040122986
2018-05-16 20:28:11.897260 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #597 | Started Training: True
2018-05-16 20:30:45.008594 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #598 | Epoch Duration: 153.11120986938477
2018-05-16 20:30:45.008790 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #598 | Started Training: True
2018-05-16 20:33:17.501507 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #599 | Epoch Duration: 152.49257564544678
2018-05-16 20:33:17.501776 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #599 | Started Training: True
2018-05-16 20:36:08.642838 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #600 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.77931e-05
Policy Loss                      -0.474422
Raw Policy Loss                  -0.474422
Preactivation Policy Loss         0
Q Predictions Mean                0.471571
Q Predictions Std                 0.210464
Q Predictions Max                 0.664236
Q Predictions Min                -0.324133
Q Targets Mean                    0.472752
Q Targets Std                     0.209977
Q Targets Max                     0.660826
Q Targets Min                    -0.329272
Bellman Errors Mean               7.77932e-05
Bellman Errors Std                0.000382318
Bellman Errors Max                0.00424835
Bellman Errors Min                2.27232e-09
Policy Action Mean                0.0837829
Policy Action Std                 0.985542
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00779993
Test Rewards Std                  0.00319562
Test Rewards Max                  0.0144956
Test Rewards Min                 -0.00014569
Test Returns Mean                 1.40919
Test Returns Std                  0.00808073
Test Returns Max                  1.42145
Test Returns Min                  1.39631
Test Actions Mean                 0.0851484
Test Actions Std                  0.984393
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00810826
Exploration Rewards Std           0.00424293
Exploration Rewards Max           0.0169148
Exploration Rewards Min          -0.000622781
Exploration Returns Mean          1.25354
Exploration Returns Std           0.102302
Exploration Returns Max           1.37015
Exploration Returns Min           1.06632
Exploration Actions Mean          0.118239
Exploration Actions Std           0.828104
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.40919
Number of train steps total  600863
Number of env steps total    601000
Number of rollouts total       4193
Train Time (s)                   81.2181
(Previous) Eval Time (s)          3.53399e-06
Sample Time (s)                  74.6089
Epoch Time (s)                  155.827
Total Train Time (s)          81024.1
Epoch                           600
---------------------------  ----------------
2018-05-16 20:39:46.824727 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #600 | Epoch Duration: 389.3227469921112
2018-05-16 20:39:46.824839 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #600 | Started Training: True
2018-05-16 20:40:29.973518 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #601 | Epoch Duration: 43.14857292175293
2018-05-16 20:40:29.973678 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #601 | Started Training: True
2018-05-16 20:41:12.401051 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #602 | Epoch Duration: 42.42725491523743
2018-05-16 20:41:12.401307 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #602 | Started Training: True
2018-05-16 20:42:42.486297 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #603 | Epoch Duration: 90.0847749710083
2018-05-16 20:42:42.486514 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #603 | Started Training: True
2018-05-16 20:45:14.749192 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #604 | Epoch Duration: 152.26254439353943
2018-05-16 20:45:14.749371 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #604 | Started Training: True
2018-05-16 20:47:47.145125 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #605 | Epoch Duration: 152.39563083648682
2018-05-16 20:47:47.145360 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #605 | Started Training: True
2018-05-16 20:50:20.900739 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #606 | Epoch Duration: 153.75522470474243
2018-05-16 20:50:20.900939 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #606 | Started Training: True
2018-05-16 20:52:55.054414 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #607 | Epoch Duration: 154.15334391593933
2018-05-16 20:52:55.054635 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #607 | Started Training: True
2018-05-16 20:55:29.304406 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #608 | Epoch Duration: 154.24961638450623
2018-05-16 20:55:29.304590 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #608 | Started Training: True
2018-05-16 20:58:08.867748 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #609 | Epoch Duration: 159.56303191184998
2018-05-16 20:58:08.867911 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #609 | Started Training: True
2018-05-16 21:00:58.038412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #610 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.60749e-05
Policy Loss                      -0.521601
Raw Policy Loss                  -0.521601
Preactivation Policy Loss         0
Q Predictions Mean                0.517581
Q Predictions Std                 0.211579
Q Predictions Max                 0.702587
Q Predictions Min                -0.22836
Q Targets Mean                    0.517533
Q Targets Std                     0.213302
Q Targets Max                     0.710389
Q Targets Min                    -0.238795
Bellman Errors Mean               6.6075e-05
Bellman Errors Std                0.000206605
Bellman Errors Max                0.00188691
Bellman Errors Min                3.75445e-09
Policy Action Mean                0.0651276
Policy Action Std                 0.991559
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00784204
Test Rewards Std                  0.00452802
Test Rewards Max                  0.0155512
Test Rewards Min                  0.000334672
Test Returns Mean                 0.810082
Test Returns Std                  0.0145477
Test Returns Max                  0.828318
Test Returns Min                  0.788394
Test Actions Mean                 0.133981
Test Actions Std                  0.978325
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00775831
Exploration Rewards Std           0.00449727
Exploration Rewards Max           0.0157109
Exploration Rewards Min          -0.000454087
Exploration Returns Mean          0.844686
Exploration Returns Std           0.0453419
Exploration Returns Max           0.937847
Exploration Returns Min           0.793278
Exploration Actions Mean          0.115322
Exploration Actions Std           0.833156
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.810082
Number of train steps total  610863
Number of env steps total    611000
Number of rollouts total       4268
Train Time (s)                   81.2511
(Previous) Eval Time (s)          2.09e-06
Sample Time (s)                  72.2811
Epoch Time (s)                  153.532
Total Train Time (s)          82568
Epoch                           610
---------------------------  ----------------
2018-05-16 21:05:30.959238 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #610 | Epoch Duration: 442.09122228622437
2018-05-16 21:05:30.959352 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #610 | Started Training: True
2018-05-16 21:06:12.467698 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #611 | Epoch Duration: 41.508251667022705
2018-05-16 21:06:12.467868 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #611 | Started Training: True
2018-05-16 21:06:53.624198 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #612 | Epoch Duration: 41.156216859817505
2018-05-16 21:06:53.624416 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #612 | Started Training: True
2018-05-16 21:08:47.340043 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #613 | Epoch Duration: 113.71547436714172
2018-05-16 21:08:47.340236 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #613 | Started Training: True
2018-05-16 21:11:20.679946 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #614 | Epoch Duration: 153.33958101272583
2018-05-16 21:11:20.680142 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #614 | Started Training: True
2018-05-16 21:13:54.042453 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #615 | Epoch Duration: 153.36218118667603
2018-05-16 21:13:54.042689 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #615 | Started Training: True
2018-05-16 21:16:28.122153 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #616 | Epoch Duration: 154.0793056488037
2018-05-16 21:16:28.122417 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #616 | Started Training: True
2018-05-16 21:18:58.878635 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #617 | Epoch Duration: 150.75606203079224
2018-05-16 21:18:58.878872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #617 | Started Training: True
2018-05-16 21:21:31.109575 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #618 | Epoch Duration: 152.23053669929504
2018-05-16 21:21:31.109816 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #618 | Started Training: True
2018-05-16 21:24:05.019226 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #619 | Epoch Duration: 153.9092402458191
2018-05-16 21:24:05.019867 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #619 | Started Training: True
2018-05-16 21:26:52.299956 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #620 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.99695e-05
Policy Loss                      -0.539554
Raw Policy Loss                  -0.539554
Preactivation Policy Loss         0
Q Predictions Mean                0.537384
Q Predictions Std                 0.180389
Q Predictions Max                 0.703738
Q Predictions Min                -0.411981
Q Targets Mean                    0.537537
Q Targets Std                     0.181436
Q Targets Max                     0.706449
Q Targets Min                    -0.396058
Bellman Errors Mean               7.99695e-05
Bellman Errors Std                0.000269007
Bellman Errors Max                0.00263879
Bellman Errors Min                3.52435e-09
Policy Action Mean                0.127855
Policy Action Std                 0.982318
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00378445
Test Rewards Std                  0.00402758
Test Rewards Max                  0.0138988
Test Rewards Min                 -0.00123524
Test Returns Mean                 0.847718
Test Returns Std                  0.00456982
Test Returns Max                  0.852313
Test Returns Min                  0.840752
Test Actions Mean                 0.356732
Test Actions Std                  0.922132
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00525951
Exploration Rewards Std           0.00390055
Exploration Rewards Max           0.0131399
Exploration Rewards Min          -0.00271657
Exploration Returns Mean          0.771395
Exploration Returns Std           0.0270567
Exploration Returns Max           0.818769
Exploration Returns Min           0.743554
Exploration Actions Mean          0.263779
Exploration Actions Std           0.789627
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.847718
Number of train steps total  620863
Number of env steps total    621000
Number of rollouts total       4342
Train Time (s)                   76.3511
(Previous) Eval Time (s)          2.513e-06
Sample Time (s)                  75.048
Epoch Time (s)                  151.399
Total Train Time (s)          84083
Epoch                           620
---------------------------  ----------------
2018-05-16 21:30:46.233604 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #620 | Epoch Duration: 401.21358823776245
2018-05-16 21:30:46.233719 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #620 | Started Training: True
2018-05-16 21:31:27.481116 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #621 | Epoch Duration: 41.247294425964355
2018-05-16 21:31:27.481320 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #621 | Started Training: True
2018-05-16 21:32:09.621144 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #622 | Epoch Duration: 42.13967227935791
2018-05-16 21:32:09.621423 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #622 | Started Training: True
2018-05-16 21:33:57.780540 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #623 | Epoch Duration: 108.15891671180725
2018-05-16 21:33:57.781555 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #623 | Started Training: True
2018-05-16 21:36:32.768943 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #624 | Epoch Duration: 154.98724031448364
2018-05-16 21:36:32.769167 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #624 | Started Training: True
2018-05-16 21:39:07.481040 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #625 | Epoch Duration: 154.7117063999176
2018-05-16 21:39:07.487993 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #625 | Started Training: True
2018-05-16 21:41:42.290814 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #626 | Epoch Duration: 154.8026580810547
2018-05-16 21:41:42.291039 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #626 | Started Training: True
2018-05-16 21:44:20.262004 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #627 | Epoch Duration: 157.9708113670349
2018-05-16 21:44:20.262196 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #627 | Started Training: True
2018-05-16 21:46:54.653110 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #628 | Epoch Duration: 154.3907823562622
2018-05-16 21:46:54.653340 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #628 | Started Training: True
2018-05-16 21:49:28.398551 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #629 | Epoch Duration: 153.7450556755066
2018-05-16 21:49:28.398748 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #629 | Started Training: True
2018-05-16 21:52:18.235017 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #630 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           8.71007e-05
Policy Loss                      -0.454844
Raw Policy Loss                  -0.454844
Preactivation Policy Loss         0
Q Predictions Mean                0.45423
Q Predictions Std                 0.176804
Q Predictions Max                 0.601432
Q Predictions Min                -0.0508131
Q Targets Mean                    0.453382
Q Targets Std                     0.174723
Q Targets Max                     0.607487
Q Targets Min                    -0.0218274
Bellman Errors Mean               8.71007e-05
Bellman Errors Std                0.000401301
Bellman Errors Max                0.003567
Bellman Errors Min                2.27374e-11
Policy Action Mean                0.364981
Policy Action Std                 0.927027
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00867387
Test Rewards Std                  0.00389492
Test Rewards Max                  0.0158939
Test Rewards Min                  0.000337108
Test Returns Mean                 1.14712
Test Returns Std                  0.0084559
Test Returns Max                  1.15666
Test Returns Min                  1.133
Test Actions Mean                 0.187446
Test Actions Std                  0.97027
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.0075771
Exploration Rewards Std           0.00358971
Exploration Rewards Max           0.0157789
Exploration Rewards Min          -0.00132049
Exploration Returns Mean          1.2035
Exploration Returns Std           0.188032
Exploration Returns Max           1.39522
Exploration Returns Min           0.873513
Exploration Actions Mean          0.141037
Exploration Actions Std           0.825537
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.14712
Number of train steps total  630863
Number of env steps total    631000
Number of rollouts total       4409
Train Time (s)                   77.6754
(Previous) Eval Time (s)          2.628e-06
Sample Time (s)                  76.2371
Epoch Time (s)                  153.913
Total Train Time (s)          85615.3
Epoch                           630
---------------------------  ----------------
2018-05-16 21:56:18.722246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #630 | Epoch Duration: 410.32337832450867
2018-05-16 21:56:18.722375 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #630 | Started Training: True
2018-05-16 21:57:01.768598 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #631 | Epoch Duration: 43.04610753059387
2018-05-16 21:57:01.768819 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #631 | Started Training: True
2018-05-16 21:57:42.735204 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #632 | Epoch Duration: 40.966219425201416
2018-05-16 21:57:42.735458 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #632 | Started Training: True
2018-05-16 21:58:24.488836 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #633 | Epoch Duration: 41.75318169593811
2018-05-16 21:58:24.489018 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #633 | Started Training: True
2018-05-16 21:59:04.988217 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #634 | Epoch Duration: 40.49907302856445
2018-05-16 21:59:04.988376 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #634 | Started Training: True
2018-05-16 21:59:45.245782 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #635 | Epoch Duration: 40.257291316986084
2018-05-16 21:59:45.246008 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #635 | Started Training: True
2018-05-16 22:00:53.025952 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #636 | Epoch Duration: 67.77978014945984
2018-05-16 22:00:53.026149 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #636 | Started Training: True
2018-05-16 22:03:31.264176 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #637 | Epoch Duration: 158.23790073394775
2018-05-16 22:03:31.264398 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #637 | Started Training: True
2018-05-16 22:06:01.141886 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #638 | Epoch Duration: 149.87734127044678
2018-05-16 22:06:01.142044 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #638 | Started Training: True
2018-05-16 22:08:35.829170 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #639 | Epoch Duration: 154.68702292442322
2018-05-16 22:08:35.829363 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #639 | Started Training: True
2018-05-16 22:11:25.087623 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #640 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           2.36024e-05
Policy Loss                      -0.501952
Raw Policy Loss                  -0.501952
Preactivation Policy Loss         0
Q Predictions Mean                0.50063
Q Predictions Std                 0.172757
Q Predictions Max                 0.612316
Q Predictions Min                -0.392563
Q Targets Mean                    0.499559
Q Targets Std                     0.174366
Q Targets Max                     0.61628
Q Targets Min                    -0.398612
Bellman Errors Mean               2.36024e-05
Bellman Errors Std                9.53704e-05
Bellman Errors Max                0.00105954
Bellman Errors Min                4.75907e-10
Policy Action Mean                0.167522
Policy Action Std                 0.971704
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00654989
Test Rewards Std                  0.0050852
Test Rewards Max                  0.0175655
Test Rewards Min                 -0.00106405
Test Returns Mean                 1.11785
Test Returns Std                  0.0185033
Test Returns Max                  1.14323
Test Returns Min                  1.09447
Test Actions Mean                 0.269905
Test Actions Std                  0.951217
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         3
Exploration Rewards Mean          0.00534525
Exploration Rewards Std           0.00373482
Exploration Rewards Max           0.0150232
Exploration Rewards Min          -0.00193999
Exploration Returns Mean          1.38264
Exploration Returns Std           0.0194625
Exploration Returns Max           1.40899
Exploration Returns Min           1.36259
Exploration Actions Mean          0.191765
Exploration Actions Std           0.813722
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.11785
Number of train steps total  640863
Number of env steps total    641000
Number of rollouts total       4455
Train Time (s)                   78.1455
(Previous) Eval Time (s)          2.54101e-06
Sample Time (s)                  74.9468
Epoch Time (s)                  153.092
Total Train Time (s)          86800.9
Epoch                           640
---------------------------  ----------------
2018-05-16 22:16:04.613459 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #640 | Epoch Duration: 448.78397607803345
2018-05-16 22:16:04.613570 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #640 | Started Training: True
2018-05-16 22:16:45.115741 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #641 | Epoch Duration: 40.502076387405396
2018-05-16 22:16:45.115900 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #641 | Started Training: True
2018-05-16 22:17:27.057683 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #642 | Epoch Duration: 41.94166874885559
2018-05-16 22:17:27.057898 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #642 | Started Training: True
2018-05-16 22:18:08.542073 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #643 | Epoch Duration: 41.4840190410614
2018-05-16 22:18:08.542287 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #643 | Started Training: True
2018-05-16 22:18:48.242407 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #644 | Epoch Duration: 39.699923276901245
2018-05-16 22:18:48.242637 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #644 | Started Training: True
2018-05-16 22:19:28.366660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #645 | Epoch Duration: 40.12385940551758
2018-05-16 22:19:28.366881 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #645 | Started Training: True
2018-05-16 22:20:08.869750 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #646 | Epoch Duration: 40.50270938873291
2018-05-16 22:20:08.869964 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #646 | Started Training: True
2018-05-16 22:21:13.731856 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #647 | Epoch Duration: 64.86174154281616
2018-05-16 22:21:13.732080 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #647 | Started Training: True
2018-05-16 22:23:46.257621 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #648 | Epoch Duration: 152.5253837108612
2018-05-16 22:23:46.257781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #648 | Started Training: True
2018-05-16 22:26:17.503873 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #649 | Epoch Duration: 151.24597668647766
2018-05-16 22:26:17.504063 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #649 | Started Training: True
2018-05-16 22:29:03.298248 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #650 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.38451e-05
Policy Loss                      -0.488568
Raw Policy Loss                  -0.488568
Preactivation Policy Loss         0
Q Predictions Mean                0.486923
Q Predictions Std                 0.172562
Q Predictions Max                 0.596447
Q Predictions Min                -0.270875
Q Targets Mean                    0.487848
Q Targets Std                     0.170939
Q Targets Max                     0.601415
Q Targets Min                    -0.279266
Bellman Errors Mean               4.3845e-05
Bellman Errors Std                0.000141847
Bellman Errors Max                0.00109782
Bellman Errors Min                9.38613e-10
Policy Action Mean                0.263858
Policy Action Std                 0.950476
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00609649
Test Rewards Std                  0.0043214
Test Rewards Max                  0.0134555
Test Rewards Min                 -0.00250522
Test Returns Mean                 0.779589
Test Returns Std                  0.00255588
Test Returns Max                  0.783372
Test Returns Min                  0.776274
Test Actions Mean                 0.0759737
Test Actions Std                  0.981039
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00661676
Exploration Rewards Std           0.0044412
Exploration Rewards Max           0.015395
Exploration Rewards Min          -0.00244049
Exploration Returns Mean          0.861006
Exploration Returns Std           0.0272519
Exploration Returns Max           0.91006
Exploration Returns Min           0.826878
Exploration Actions Mean          0.0185762
Exploration Actions Std           0.83648
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.779589
Number of train steps total  650863
Number of env steps total    651000
Number of rollouts total       4516
Train Time (s)                   74.5375
(Previous) Eval Time (s)          2.42601e-06
Sample Time (s)                  74.7485
Epoch Time (s)                  149.286
Total Train Time (s)          87794
Epoch                           650
---------------------------  ----------------
2018-05-16 22:32:37.891964 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #650 | Epoch Duration: 380.3877794742584
2018-05-16 22:32:37.892111 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #650 | Started Training: True
2018-05-16 22:35:08.500412 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #651 | Epoch Duration: 150.60818529129028
2018-05-16 22:35:08.500645 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #651 | Started Training: True
2018-05-16 22:35:50.041110 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #652 | Epoch Duration: 41.54031729698181
2018-05-16 22:35:50.041284 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #652 | Started Training: True
2018-05-16 22:36:30.736726 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #653 | Epoch Duration: 40.695332050323486
2018-05-16 22:36:30.736902 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #653 | Started Training: True
2018-05-16 22:37:11.936090 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #654 | Epoch Duration: 41.199074268341064
2018-05-16 22:37:11.936287 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #654 | Started Training: True
2018-05-16 22:37:51.618196 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #655 | Epoch Duration: 39.68178701400757
2018-05-16 22:37:51.618359 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #655 | Started Training: True
2018-05-16 22:38:34.112994 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #656 | Epoch Duration: 42.494497299194336
2018-05-16 22:38:34.113219 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #656 | Started Training: True
2018-05-16 22:39:15.136398 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #657 | Epoch Duration: 41.023027420043945
2018-05-16 22:39:15.136571 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #657 | Started Training: True
2018-05-16 22:39:56.490733 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #658 | Epoch Duration: 41.354048013687134
2018-05-16 22:39:56.490929 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #658 | Started Training: True
2018-05-16 22:40:39.311671 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #659 | Epoch Duration: 42.82061195373535
2018-05-16 22:40:39.311875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #659 | Started Training: True
2018-05-16 22:41:37.486714 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #660 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.00021411
Policy Loss                      -0.492112
Raw Policy Loss                  -0.492112
Preactivation Policy Loss         0
Q Predictions Mean                0.489837
Q Predictions Std                 0.176291
Q Predictions Max                 0.6032
Q Predictions Min                -0.447351
Q Targets Mean                    0.488942
Q Targets Std                     0.179088
Q Targets Max                     0.607068
Q Targets Min                    -0.453158
Bellman Errors Mean               0.00021411
Bellman Errors Std                0.00216552
Bellman Errors Max                0.0246055
Bellman Errors Min                1.11413e-09
Policy Action Mean                0.0630339
Policy Action Std                 0.981899
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.0085868
Test Rewards Std                  0.00459916
Test Rewards Max                  0.0176034
Test Rewards Min                  0.000525915
Test Returns Mean                 1.19679
Test Returns Std                  0.0442263
Test Returns Max                  1.262
Test Returns Min                  1.15074
Test Actions Mean                 0.332381
Test Actions Std                  0.933438
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00791209
Exploration Rewards Std           0.00487801
Exploration Rewards Max           0.0187041
Exploration Rewards Min          -0.00107802
Exploration Returns Mean          1.05231
Exploration Returns Std           0.136911
Exploration Returns Max           1.26907
Exploration Returns Min           0.909748
Exploration Actions Mean          0.215345
Exploration Actions Std           0.814344
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.19679
Number of train steps total  660863
Number of env steps total    661000
Number of rollouts total       4586
Train Time (s)                   23.5236
(Previous) Eval Time (s)          2.733e-06
Sample Time (s)                  18.6975
Epoch Time (s)                   42.2211
Total Train Time (s)          88562.7
Epoch                           660
---------------------------  ----------------
2018-05-16 22:45:26.794868 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #660 | Epoch Duration: 287.48285126686096
2018-05-16 22:45:26.794982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #660 | Started Training: True
2018-05-16 22:46:09.233740 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #661 | Epoch Duration: 42.4386522769928
2018-05-16 22:46:09.233963 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #661 | Started Training: True
2018-05-16 22:46:50.816577 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #662 | Epoch Duration: 41.5824453830719
2018-05-16 22:46:50.816814 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #662 | Started Training: True
2018-05-16 22:49:04.919864 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #663 | Epoch Duration: 134.1028983592987
2018-05-16 22:49:04.920061 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #663 | Started Training: True
2018-05-16 22:51:37.859948 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #664 | Epoch Duration: 152.93975710868835
2018-05-16 22:51:37.860147 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #664 | Started Training: True
2018-05-16 22:54:09.603937 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #665 | Epoch Duration: 151.7436559200287
2018-05-16 22:54:09.604096 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #665 | Started Training: True
2018-05-16 22:56:40.892647 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #666 | Epoch Duration: 151.28844571113586
2018-05-16 22:56:40.892862 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #666 | Started Training: True
2018-05-16 22:59:12.125002 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #667 | Epoch Duration: 151.23200130462646
2018-05-16 22:59:12.125191 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #667 | Started Training: True
2018-05-16 23:01:41.130361 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #668 | Epoch Duration: 149.00504231452942
2018-05-16 23:01:41.130565 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #668 | Started Training: True
2018-05-16 23:04:14.855816 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #669 | Epoch Duration: 153.7251181602478
2018-05-16 23:04:14.856025 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #669 | Started Training: True
2018-05-16 23:07:01.230718 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #670 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.28011e-05
Policy Loss                      -0.478656
Raw Policy Loss                  -0.478656
Preactivation Policy Loss         0
Q Predictions Mean                0.476497
Q Predictions Std                 0.200735
Q Predictions Max                 0.635659
Q Predictions Min                -0.377442
Q Targets Mean                    0.476554
Q Targets Std                     0.200672
Q Targets Max                     0.642139
Q Targets Min                    -0.387753
Bellman Errors Mean               3.28011e-05
Bellman Errors Std                0.000117033
Bellman Errors Max                0.00124781
Bellman Errors Min                1.18977e-10
Policy Action Mean                0.312355
Policy Action Std                 0.937632
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.004608
Test Rewards Std                  0.00369192
Test Rewards Max                  0.0142213
Test Rewards Min                 -0.000493701
Test Returns Mean                 1.30061
Test Returns Std                  0.00474888
Test Returns Max                  1.30767
Test Returns Min                  1.29544
Test Actions Mean                -0.0125628
Test Actions Std                  0.979125
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean         -0.00642251
Exploration Rewards Std           0.00781907
Exploration Rewards Max           0.0113661
Exploration Rewards Min          -0.0213738
Exploration Returns Mean         -0.822883
Exploration Returns Std           0.0250663
Exploration Returns Max          -0.763984
Exploration Returns Min          -0.850458
Exploration Actions Mean          0.0174456
Exploration Actions Std           0.837241
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.30061
Number of train steps total  670863
Number of env steps total    671000
Number of rollouts total       4653
Train Time (s)                   77.7877
(Previous) Eval Time (s)          2.574e-06
Sample Time (s)                  71.7245
Epoch Time (s)                  149.512
Total Train Time (s)          90093.8
Epoch                           670
---------------------------  ----------------
2018-05-16 23:10:58.158037 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #670 | Epoch Duration: 403.3018832206726
2018-05-16 23:10:58.158159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #670 | Started Training: True
2018-05-16 23:11:37.825291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #671 | Epoch Duration: 39.6670184135437
2018-05-16 23:11:37.825501 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #671 | Started Training: True
2018-05-16 23:12:21.137847 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #672 | Epoch Duration: 43.31218910217285
2018-05-16 23:12:21.138004 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #672 | Started Training: True
2018-05-16 23:13:03.824871 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #673 | Epoch Duration: 42.6867573261261
2018-05-16 23:13:03.825038 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #673 | Started Training: True
2018-05-16 23:13:45.777188 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #674 | Epoch Duration: 41.952032804489136
2018-05-16 23:13:45.777445 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #674 | Started Training: True
2018-05-16 23:14:29.709171 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #675 | Epoch Duration: 43.93152642250061
2018-05-16 23:14:29.709384 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #675 | Started Training: True
2018-05-16 23:15:12.645325 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #676 | Epoch Duration: 42.935786962509155
2018-05-16 23:15:12.645530 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #676 | Started Training: True
2018-05-16 23:15:55.504304 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #677 | Epoch Duration: 42.85862708091736
2018-05-16 23:15:55.504470 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #677 | Started Training: True
2018-05-16 23:16:37.941282 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #678 | Epoch Duration: 42.43669557571411
2018-05-16 23:16:37.941498 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #678 | Started Training: True
2018-05-16 23:17:21.942265 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #679 | Epoch Duration: 44.00059413909912
2018-05-16 23:17:21.942556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #679 | Started Training: True
2018-05-16 23:18:21.284229 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #680 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000471748
Policy Loss                      -0.498447
Raw Policy Loss                  -0.498447
Preactivation Policy Loss         0
Q Predictions Mean                0.494143
Q Predictions Std                 0.220655
Q Predictions Max                 0.702285
Q Predictions Min                -0.500097
Q Targets Mean                    0.495556
Q Targets Std                     0.218007
Q Targets Max                     0.705969
Q Targets Min                    -0.458805
Bellman Errors Mean               0.000471748
Bellman Errors Std                0.00347567
Bellman Errors Max                0.0392209
Bellman Errors Min                6.87805e-12
Policy Action Mean                0.0622653
Policy Action Std                 0.989009
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00722763
Test Rewards Std                  0.00417965
Test Rewards Max                  0.016529
Test Rewards Min                 -0.000314914
Test Returns Mean                 1.16158
Test Returns Std                  0.0694745
Test Returns Max                  1.21233
Test Returns Min                  1.04822
Test Actions Mean                 0.0850295
Test Actions Std                  0.987751
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00755989
Exploration Rewards Std           0.00420816
Exploration Rewards Max           0.0162374
Exploration Rewards Min          -0.00253778
Exploration Returns Mean          0.953806
Exploration Returns Std           0.0989284
Exploration Returns Max           1.09822
Exploration Returns Min           0.77661
Exploration Actions Mean          0.10441
Exploration Actions Std           0.831745
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.16158
Number of train steps total  680863
Number of env steps total    681000
Number of rollouts total       4726
Train Time (s)                   24.2991
(Previous) Eval Time (s)          3.276e-06
Sample Time (s)                  18.6576
Epoch Time (s)                   42.9567
Total Train Time (s)          90779.1
Epoch                           680
---------------------------  ----------------
2018-05-16 23:22:23.702688 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #680 | Epoch Duration: 301.75993275642395
2018-05-16 23:22:23.702830 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #680 | Started Training: True
2018-05-16 23:24:57.843646 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #681 | Epoch Duration: 154.14069962501526
2018-05-16 23:24:57.843851 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #681 | Started Training: True
2018-05-16 23:27:31.071806 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #682 | Epoch Duration: 153.2278232574463
2018-05-16 23:27:31.072025 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #682 | Started Training: True
2018-05-16 23:30:03.929000 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #683 | Epoch Duration: 152.85682201385498
2018-05-16 23:30:03.929198 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #683 | Started Training: True
2018-05-16 23:32:35.007740 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #684 | Epoch Duration: 151.07837915420532
2018-05-16 23:32:35.007935 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #684 | Started Training: True
2018-05-16 23:35:08.569336 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #685 | Epoch Duration: 153.56126880645752
2018-05-16 23:35:08.569576 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #685 | Started Training: True
2018-05-16 23:37:43.517273 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #686 | Epoch Duration: 154.9475438594818
2018-05-16 23:37:43.517506 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #686 | Started Training: True
2018-05-16 23:39:49.633938 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #687 | Epoch Duration: 126.11627507209778
2018-05-16 23:39:49.634167 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #687 | Started Training: True
2018-05-16 23:40:33.269875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #688 | Epoch Duration: 43.63554263114929
2018-05-16 23:40:33.270099 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #688 | Started Training: True
2018-05-16 23:41:17.222876 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #689 | Epoch Duration: 43.95262122154236
2018-05-16 23:41:17.223107 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #689 | Started Training: True
2018-05-16 23:42:17.527602 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #690 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.86704e-05
Policy Loss                      -0.495254
Raw Policy Loss                  -0.495254
Preactivation Policy Loss         0
Q Predictions Mean                0.493034
Q Predictions Std                 0.188311
Q Predictions Max                 0.761286
Q Predictions Min                -0.590704
Q Targets Mean                    0.491538
Q Targets Std                     0.188862
Q Targets Max                     0.756118
Q Targets Min                    -0.576303
Bellman Errors Mean               5.86704e-05
Bellman Errors Std                0.00014837
Bellman Errors Max                0.00155218
Bellman Errors Min                1.33297e-08
Policy Action Mean                0.0848391
Policy Action Std                 0.991488
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00781177
Test Rewards Std                  0.00383154
Test Rewards Max                  0.0147646
Test Rewards Min                 -0.000149697
Test Returns Mean                 0.963452
Test Returns Std                  0.00319781
Test Returns Max                  0.96977
Test Returns Min                  0.958672
Test Actions Mean                 0.320883
Test Actions Std                  0.937599
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00726209
Exploration Rewards Std           0.0040452
Exploration Rewards Max           0.0173997
Exploration Rewards Min          -0.000804194
Exploration Returns Mean          0.952242
Exploration Returns Std           0.0327791
Exploration Returns Max           1.01191
Exploration Returns Min           0.902906
Exploration Actions Mean          0.153833
Exploration Actions Std           0.82174
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.963452
Number of train steps total  690863
Number of env steps total    691000
Number of rollouts total       4793
Train Time (s)                   24.5184
(Previous) Eval Time (s)          2.94601e-06
Sample Time (s)                  19.2659
Epoch Time (s)                   43.7844
Total Train Time (s)          92227.4
Epoch                           690
---------------------------  ----------------
2018-05-16 23:46:32.275460 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #690 | Epoch Duration: 315.0521981716156
2018-05-16 23:46:32.275575 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #690 | Started Training: True
2018-05-16 23:49:05.881454 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #691 | Epoch Duration: 153.60578393936157
2018-05-16 23:49:05.881613 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #691 | Started Training: True
2018-05-16 23:51:39.014319 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #692 | Epoch Duration: 153.13259434700012
2018-05-16 23:51:39.014558 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #692 | Started Training: True
2018-05-16 23:54:12.891665 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #693 | Epoch Duration: 153.87694120407104
2018-05-16 23:54:12.891822 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #693 | Started Training: True
2018-05-16 23:56:48.479925 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #694 | Epoch Duration: 155.5879945755005
2018-05-16 23:56:48.480154 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #694 | Started Training: True
2018-05-16 23:58:37.104898 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #695 | Epoch Duration: 108.62459659576416
2018-05-16 23:58:37.105063 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #695 | Started Training: True
2018-05-16 23:59:20.832795 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #696 | Epoch Duration: 43.727625131607056
2018-05-16 23:59:20.832978 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #696 | Started Training: True
2018-05-17 00:00:02.040352 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #697 | Epoch Duration: 41.20725440979004
2018-05-17 00:00:02.040510 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #697 | Started Training: True
2018-05-17 00:00:42.669756 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #698 | Epoch Duration: 40.62913155555725
2018-05-17 00:00:42.669964 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #698 | Started Training: True
2018-05-17 00:01:23.741664 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #699 | Epoch Duration: 41.07155108451843
2018-05-17 00:01:23.741831 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #699 | Started Training: True
2018-05-17 00:04:09.997409 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #700 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.17305e-05
Policy Loss                      -0.537217
Raw Policy Loss                  -0.537217
Preactivation Policy Loss         0
Q Predictions Mean                0.533603
Q Predictions Std                 0.203098
Q Predictions Max                 0.778538
Q Predictions Min                -0.518903
Q Targets Mean                    0.531525
Q Targets Std                     0.205377
Q Targets Max                     0.779893
Q Targets Min                    -0.577069
Bellman Errors Mean               7.17304e-05
Bellman Errors Std                0.000311296
Bellman Errors Max                0.00338328
Bellman Errors Min                1.75578e-09
Policy Action Mean                0.300091
Policy Action Std                 0.943259
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.0061214
Test Rewards Std                  0.00464602
Test Rewards Max                  0.0154755
Test Rewards Min                 -0.00318177
Test Returns Mean                 1.01178
Test Returns Std                  0.123445
Test Returns Max                  1.13464
Test Returns Min                  0.848721
Test Actions Mean                 0.142821
Test Actions Std                  0.97732
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                        10
Exploration Rewards Mean          0.00699186
Exploration Rewards Std           0.00408154
Exploration Rewards Max           0.0163052
Exploration Rewards Min          -0.00280551
Exploration Returns Mean          0.808259
Exploration Returns Std           0.0373311
Exploration Returns Max           0.866355
Exploration Returns Min           0.743567
Exploration Actions Mean          0.15957
Exploration Actions Std           0.826352
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.01178
Number of train steps total  700863
Number of env steps total    701000
Number of rollouts total       4880
Train Time (s)                   77.2485
(Previous) Eval Time (s)          1.87999e-06
Sample Time (s)                  71.764
Epoch Time (s)                  149.013
Total Train Time (s)          93567
Epoch                           700
---------------------------  ----------------
2018-05-17 00:08:52.059911 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #700 | Epoch Duration: 448.3179762363434
2018-05-17 00:08:52.060024 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #700 | Started Training: True
2018-05-17 00:11:26.705980 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #701 | Epoch Duration: 154.64585733413696
2018-05-17 00:11:26.706168 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #701 | Started Training: True
2018-05-17 00:12:33.509686 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #702 | Epoch Duration: 66.80338597297668
2018-05-17 00:12:33.509927 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #702 | Started Training: True
2018-05-17 00:13:15.773176 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #703 | Epoch Duration: 42.26309394836426
2018-05-17 00:13:15.773340 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #703 | Started Training: True
2018-05-17 00:13:56.362057 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #704 | Epoch Duration: 40.58859920501709
2018-05-17 00:13:56.362279 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #704 | Started Training: True
2018-05-17 00:14:38.654516 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #705 | Epoch Duration: 42.29206728935242
2018-05-17 00:14:38.654680 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #705 | Started Training: True
2018-05-17 00:15:21.344894 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #706 | Epoch Duration: 42.690112590789795
2018-05-17 00:15:21.345067 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #706 | Started Training: True
2018-05-17 00:16:05.633613 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #707 | Epoch Duration: 44.288437366485596
2018-05-17 00:16:05.633829 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #707 | Started Training: True
2018-05-17 00:18:02.324079 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #708 | Epoch Duration: 116.69009518623352
2018-05-17 00:18:02.324315 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #708 | Started Training: True
2018-05-17 00:20:39.128982 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #709 | Epoch Duration: 156.80451273918152
2018-05-17 00:20:39.129182 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #709 | Started Training: True
2018-05-17 00:23:33.873924 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #710 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.00031385
Policy Loss                      -0.527694
Raw Policy Loss                  -0.527694
Preactivation Policy Loss         0
Q Predictions Mean                0.524169
Q Predictions Std                 0.222022
Q Predictions Max                 0.73998
Q Predictions Min                -0.630088
Q Targets Mean                    0.523113
Q Targets Std                     0.226778
Q Targets Max                     0.742452
Q Targets Min                    -0.626551
Bellman Errors Mean               0.00031385
Bellman Errors Std                0.00250437
Bellman Errors Max                0.0278386
Bellman Errors Min                7.98209e-10
Policy Action Mean                0.11296
Policy Action Std                 0.984087
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00199011
Test Rewards Std                  0.00365047
Test Rewards Max                  0.015066
Test Rewards Min                 -0.00116749
Test Returns Mean                 1.00036
Test Returns Std                  0.0042545
Test Returns Max                  1.00537
Test Returns Min                  0.99497
Test Actions Mean                 0.294246
Test Actions Std                  0.937989
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00588837
Exploration Rewards Std           0.00419283
Exploration Rewards Max           0.0167911
Exploration Rewards Min          -0.00231622
Exploration Returns Mean          0.956859
Exploration Returns Std           0.157805
Exploration Returns Max           1.21549
Exploration Returns Min           0.74475
Exploration Actions Mean          0.24936
Exploration Actions Std           0.803734
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.00036
Number of train steps total  710863
Number of env steps total    711000
Number of rollouts total       4952
Train Time (s)                   79.6076
(Previous) Eval Time (s)          2.50699e-06
Sample Time (s)                  77.53
Epoch Time (s)                  157.138
Total Train Time (s)          94770.3
Epoch                           710
---------------------------  ----------------
2018-05-17 00:28:55.583088 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #710 | Epoch Duration: 496.45377826690674
2018-05-17 00:28:55.583202 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #710 | Started Training: True
2018-05-17 00:29:38.952956 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #711 | Epoch Duration: 43.36966133117676
2018-05-17 00:29:38.953127 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #711 | Started Training: True
2018-05-17 00:30:21.980845 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #712 | Epoch Duration: 43.027607679367065
2018-05-17 00:30:21.981019 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #712 | Started Training: True
2018-05-17 00:31:06.064923 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #713 | Epoch Duration: 44.08378291130066
2018-05-17 00:31:06.065085 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #713 | Started Training: True
2018-05-17 00:31:48.834193 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #714 | Epoch Duration: 42.76899743080139
2018-05-17 00:31:48.834362 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #714 | Started Training: True
2018-05-17 00:33:53.357506 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #715 | Epoch Duration: 124.52300977706909
2018-05-17 00:33:53.357736 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #715 | Started Training: True
2018-05-17 00:36:27.075798 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #716 | Epoch Duration: 153.71791124343872
2018-05-17 00:36:27.076032 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #716 | Started Training: True
2018-05-17 00:39:01.788030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #717 | Epoch Duration: 154.71184468269348
2018-05-17 00:39:01.788255 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #717 | Started Training: True
2018-05-17 00:41:33.649297 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #718 | Epoch Duration: 151.8608901500702
2018-05-17 00:41:33.649493 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #718 | Started Training: True
2018-05-17 00:44:07.905664 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #719 | Epoch Duration: 154.25604009628296
2018-05-17 00:44:07.905902 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #719 | Started Training: True
2018-05-17 00:47:00.685970 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #720 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.61728e-05
Policy Loss                      -0.507499
Raw Policy Loss                  -0.507499
Preactivation Policy Loss         0
Q Predictions Mean                0.505776
Q Predictions Std                 0.174651
Q Predictions Max                 0.688486
Q Predictions Min                -0.338284
Q Targets Mean                    0.50339
Q Targets Std                     0.173825
Q Targets Max                     0.687868
Q Targets Min                    -0.332169
Bellman Errors Mean               4.61728e-05
Bellman Errors Std                7.39269e-05
Bellman Errors Max                0.000438764
Bellman Errors Min                5.19293e-09
Policy Action Mean                0.405919
Policy Action Std                 0.907637
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00653724
Test Rewards Std                  0.00398165
Test Rewards Max                  0.0141808
Test Rewards Min                 -0.000131803
Test Returns Mean                 0.671374
Test Returns Std                  0.00474682
Test Returns Max                  0.675521
Test Returns Min                  0.663534
Test Actions Mean                 0.33808
Test Actions Std                  0.936634
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.0071365
Exploration Rewards Std           0.00392182
Exploration Rewards Max           0.0181154
Exploration Rewards Min          -0.00082952
Exploration Returns Mean          0.852301
Exploration Returns Std           0.154717
Exploration Returns Max           1.22099
Exploration Returns Min           0.755158
Exploration Actions Mean          0.249525
Exploration Actions Std           0.80371
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.671374
Number of train steps total  720863
Number of env steps total    721000
Number of rollouts total       5020
Train Time (s)                   81.1154
(Previous) Eval Time (s)          3.241e-06
Sample Time (s)                  73.923
Epoch Time (s)                  155.038
Total Train Time (s)          96121.2
Epoch                           720
---------------------------  ----------------
2018-05-17 00:51:26.725255 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #720 | Epoch Duration: 438.81918692588806
2018-05-17 00:51:26.725369 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #720 | Started Training: True
2018-05-17 00:52:10.182003 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #721 | Epoch Duration: 43.456536054611206
2018-05-17 00:52:10.182199 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #721 | Started Training: True
2018-05-17 00:52:53.078503 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #722 | Epoch Duration: 42.89616847038269
2018-05-17 00:52:53.078719 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #722 | Started Training: True
2018-05-17 00:53:35.060377 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #723 | Epoch Duration: 41.981497049331665
2018-05-17 00:53:35.060538 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #723 | Started Training: True
2018-05-17 00:54:15.715264 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #724 | Epoch Duration: 40.65460181236267
2018-05-17 00:54:15.715479 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #724 | Started Training: True
2018-05-17 00:56:38.698454 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #725 | Epoch Duration: 142.9828019142151
2018-05-17 00:56:38.698652 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #725 | Started Training: True
2018-05-17 00:59:12.614473 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #726 | Epoch Duration: 153.91568422317505
2018-05-17 00:59:12.614636 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #726 | Started Training: True
2018-05-17 01:01:43.311745 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #727 | Epoch Duration: 150.6969985961914
2018-05-17 01:01:43.311923 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #727 | Started Training: True
2018-05-17 01:04:18.076098 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #728 | Epoch Duration: 154.764066696167
2018-05-17 01:04:18.076309 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #728 | Started Training: True
2018-05-17 01:06:51.343824 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #729 | Epoch Duration: 153.2673852443695
2018-05-17 01:06:51.344017 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #729 | Started Training: True
2018-05-17 01:09:43.308270 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #730 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000153889
Policy Loss                      -0.515186
Raw Policy Loss                  -0.515186
Preactivation Policy Loss         0
Q Predictions Mean                0.512485
Q Predictions Std                 0.208138
Q Predictions Max                 0.739591
Q Predictions Min                -0.460155
Q Targets Mean                    0.516406
Q Targets Std                     0.211203
Q Targets Max                     0.736538
Q Targets Min                    -0.499127
Bellman Errors Mean               0.000153889
Bellman Errors Std                0.000304168
Bellman Errors Max                0.00163181
Bellman Errors Min                4.45212e-10
Policy Action Mean                0.260091
Policy Action Std                 0.960425
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00786787
Test Rewards Std                  0.00486903
Test Rewards Max                  0.0161326
Test Rewards Min                  0.0011761
Test Returns Mean                 0.762468
Test Returns Std                  0.00709883
Test Returns Max                  0.774917
Test Returns Min                  0.749846
Test Actions Mean                 0.235409
Test Actions Std                  0.967033
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00789745
Exploration Rewards Std           0.00484312
Exploration Rewards Max           0.0185832
Exploration Rewards Min          -0.00192675
Exploration Returns Mean          1.06791
Exploration Returns Std           0.185506
Exploration Returns Max           1.39069
Exploration Returns Min           0.75269
Exploration Actions Mean          0.143694
Exploration Actions Std           0.823777
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.762468
Number of train steps total  730863
Number of env steps total    731000
Number of rollouts total       5093
Train Time (s)                   80.3141
(Previous) Eval Time (s)          2.289e-06
Sample Time (s)                  73.1329
Epoch Time (s)                  153.447
Total Train Time (s)          97442.9
Epoch                           730
---------------------------  ----------------
2018-05-17 01:13:28.725243 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #730 | Epoch Duration: 397.3747591972351
2018-05-17 01:13:28.725351 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #730 | Started Training: True
2018-05-17 01:14:11.282597 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #731 | Epoch Duration: 42.55714225769043
2018-05-17 01:14:11.282806 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #731 | Started Training: True
2018-05-17 01:14:53.618508 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #732 | Epoch Duration: 42.33554673194885
2018-05-17 01:14:53.618668 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #732 | Started Training: True
2018-05-17 01:15:35.310062 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #733 | Epoch Duration: 41.69128727912903
2018-05-17 01:15:35.310232 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #733 | Started Training: True
2018-05-17 01:16:15.373946 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #734 | Epoch Duration: 40.063602685928345
2018-05-17 01:16:15.374109 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #734 | Started Training: True
2018-05-17 01:18:05.410300 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #735 | Epoch Duration: 110.03607678413391
2018-05-17 01:18:05.410552 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #735 | Started Training: True
2018-05-17 01:20:39.018071 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #736 | Epoch Duration: 153.60735750198364
2018-05-17 01:20:39.018246 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #736 | Started Training: True
2018-05-17 01:23:11.435599 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #737 | Epoch Duration: 152.41724395751953
2018-05-17 01:23:11.435821 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #737 | Started Training: True
2018-05-17 01:25:42.734010 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #738 | Epoch Duration: 151.29802823066711
2018-05-17 01:25:42.734195 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #738 | Started Training: True
2018-05-17 01:28:17.829570 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #739 | Epoch Duration: 155.0952546596527
2018-05-17 01:28:17.829781 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #739 | Started Training: True
2018-05-17 01:31:09.279556 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #740 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000136788
Policy Loss                      -0.47949
Raw Policy Loss                  -0.47949
Preactivation Policy Loss         0
Q Predictions Mean                0.4767
Q Predictions Std                 0.197283
Q Predictions Max                 0.669863
Q Predictions Min                -0.404735
Q Targets Mean                    0.474319
Q Targets Std                     0.196302
Q Targets Max                     0.669739
Q Targets Min                    -0.387922
Bellman Errors Mean               0.000136788
Bellman Errors Std                0.00119005
Bellman Errors Max                0.0135157
Bellman Errors Min                1.23985e-09
Policy Action Mean                0.122019
Policy Action Std                 0.98881
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00718346
Test Rewards Std                  0.00347909
Test Rewards Max                  0.0138912
Test Rewards Min                 -0.000614549
Test Returns Mean                 1.3888
Test Returns Std                  0.0363309
Test Returns Max                  1.44312
Test Returns Min                  1.33691
Test Actions Mean                 0.361797
Test Actions Std                  0.92244
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00759895
Exploration Rewards Std           0.00416253
Exploration Rewards Max           0.0177991
Exploration Rewards Min          -0.000983692
Exploration Returns Mean          0.896676
Exploration Returns Std           0.145713
Exploration Returns Max           1.28642
Exploration Returns Min           0.776281
Exploration Actions Mean          0.265952
Exploration Actions Std           0.791058
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.3888
Number of train steps total  740863
Number of env steps total    741000
Number of rollouts total       5181
Train Time (s)                   76.0602
(Previous) Eval Time (s)          2.516e-06
Sample Time (s)                  76.3402
Epoch Time (s)                  152.4
Total Train Time (s)          98748.9
Epoch                           740
---------------------------  ----------------
2018-05-17 01:35:14.926998 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #740 | Epoch Duration: 417.09709215164185
2018-05-17 01:35:14.927112 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #740 | Started Training: True
2018-05-17 01:35:57.719769 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #741 | Epoch Duration: 42.792553424835205
2018-05-17 01:35:57.719993 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #741 | Started Training: True
2018-05-17 01:36:40.624715 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #742 | Epoch Duration: 42.904560804367065
2018-05-17 01:36:40.624967 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #742 | Started Training: True
2018-05-17 01:37:24.380696 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #743 | Epoch Duration: 43.75552749633789
2018-05-17 01:37:24.380850 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #743 | Started Training: True
2018-05-17 01:38:07.476061 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #744 | Epoch Duration: 43.095102310180664
2018-05-17 01:38:07.476279 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #744 | Started Training: True
2018-05-17 01:38:50.965290 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #745 | Epoch Duration: 43.488853216171265
2018-05-17 01:38:50.965443 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #745 | Started Training: True
2018-05-17 01:39:33.375837 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #746 | Epoch Duration: 42.41029143333435
2018-05-17 01:39:33.376013 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #746 | Started Training: True
2018-05-17 01:40:15.629020 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #747 | Epoch Duration: 42.25289487838745
2018-05-17 01:40:15.629199 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #747 | Started Training: True
2018-05-17 01:40:59.666702 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #748 | Epoch Duration: 44.037378549575806
2018-05-17 01:40:59.666918 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #748 | Started Training: True
2018-05-17 01:41:42.660060 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #749 | Epoch Duration: 42.99298691749573
2018-05-17 01:41:42.660273 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #749 | Started Training: True
2018-05-17 01:42:44.674485 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #750 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.60205e-05
Policy Loss                      -0.485456
Raw Policy Loss                  -0.485456
Preactivation Policy Loss         0
Q Predictions Mean                0.482828
Q Predictions Std                 0.224415
Q Predictions Max                 0.710701
Q Predictions Min                -0.426371
Q Targets Mean                    0.477822
Q Targets Std                     0.225821
Q Targets Max                     0.712125
Q Targets Min                    -0.440555
Bellman Errors Mean               7.60205e-05
Bellman Errors Std                0.000194008
Bellman Errors Max                0.00195457
Bellman Errors Min                2.84533e-10
Policy Action Mean                0.368817
Policy Action Std                 0.925387
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00832664
Test Rewards Std                  0.00405955
Test Rewards Max                  0.0145186
Test Rewards Min                 -0.000743766
Test Returns Mean                 1.00845
Test Returns Std                  0.0216606
Test Returns Max                  1.05071
Test Returns Min                  0.979205
Test Actions Mean                 0.363387
Test Actions Std                  0.9281
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00741
Exploration Rewards Std           0.0039589
Exploration Rewards Max           0.0177751
Exploration Rewards Min          -0.000904043
Exploration Returns Mean          0.882613
Exploration Returns Std           0.121309
Exploration Returns Max           1.20291
Exploration Returns Min           0.800463
Exploration Actions Mean          0.262764
Exploration Actions Std           0.796858
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.00845
Number of train steps total  750863
Number of env steps total    751000
Number of rollouts total       5259
Train Time (s)                   24.4735
(Previous) Eval Time (s)          2.26199e-06
Sample Time (s)                  19.1981
Epoch Time (s)                   43.6717
Total Train Time (s)          99432.6
Epoch                           750
---------------------------  ----------------
2018-05-17 01:46:38.811137 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #750 | Epoch Duration: 296.1507217884064
2018-05-17 01:46:38.811249 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #750 | Started Training: True
2018-05-17 01:48:27.352718 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #751 | Epoch Duration: 108.54136896133423
2018-05-17 01:48:27.352909 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #751 | Started Training: True
2018-05-17 01:51:00.841793 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #752 | Epoch Duration: 153.48875546455383
2018-05-17 01:51:00.841968 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #752 | Started Training: True
2018-05-17 01:53:35.461666 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #753 | Epoch Duration: 154.61959147453308
2018-05-17 01:53:35.461872 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #753 | Started Training: True
2018-05-17 01:56:10.778003 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #754 | Epoch Duration: 155.315993309021
2018-05-17 01:56:10.778183 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #754 | Started Training: True
2018-05-17 01:58:47.087494 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #755 | Epoch Duration: 156.3091869354248
2018-05-17 01:58:47.087684 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #755 | Started Training: True
2018-05-17 02:01:19.430502 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #756 | Epoch Duration: 152.34268879890442
2018-05-17 02:01:19.430728 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #756 | Started Training: True
2018-05-17 02:03:52.459644 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #757 | Epoch Duration: 153.0287618637085
2018-05-17 02:03:52.459844 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #757 | Started Training: True
2018-05-17 02:06:29.823795 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #758 | Epoch Duration: 157.3638138771057
2018-05-17 02:06:29.824003 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #758 | Started Training: True
2018-05-17 02:09:01.342457 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #759 | Epoch Duration: 151.51831102371216
2018-05-17 02:09:01.355356 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #759 | Started Training: True
2018-05-17 02:11:55.561007 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #760 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.1932e-05
Policy Loss                      -0.467967
Raw Policy Loss                  -0.467967
Preactivation Policy Loss         0
Q Predictions Mean                0.465305
Q Predictions Std                 0.193947
Q Predictions Max                 0.699734
Q Predictions Min                -0.170691
Q Targets Mean                    0.470317
Q Targets Std                     0.194939
Q Targets Max                     0.703305
Q Targets Min                    -0.185061
Bellman Errors Mean               7.1932e-05
Bellman Errors Std                0.000116291
Bellman Errors Max                0.000786654
Bellman Errors Min                6.51325e-09
Policy Action Mean                0.360828
Policy Action Std                 0.926568
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00673337
Test Rewards Std                  0.00333499
Test Rewards Max                  0.0138369
Test Rewards Min                 -0.000448891
Test Returns Mean                 1.29393
Test Returns Std                  0.0344781
Test Returns Max                  1.31979
Test Returns Min                  1.21833
Test Actions Mean                 0.370069
Test Actions Std                  0.920914
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00724785
Exploration Rewards Std           0.00377549
Exploration Rewards Max           0.0163824
Exploration Rewards Min          -0.000706406
Exploration Returns Mean          1.05577
Exploration Returns Std           0.106238
Exploration Returns Max           1.20256
Exploration Returns Min           0.879127
Exploration Actions Mean          0.305232
Exploration Actions Std           0.777291
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.29393
Number of train steps total  760863
Number of env steps total    761000
Number of rollouts total       5331
Train Time (s)                   79.8639
(Previous) Eval Time (s)          2.332e-06
Sample Time (s)                  74.4742
Epoch Time (s)                  154.338
Total Train Time (s)         101231
Epoch                           760
---------------------------  ----------------
2018-05-17 02:16:37.748722 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #760 | Epoch Duration: 456.3932373523712
2018-05-17 02:16:37.748838 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #760 | Started Training: True
2018-05-17 02:19:12.909315 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #761 | Epoch Duration: 155.16037821769714
2018-05-17 02:19:12.909547 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #761 | Started Training: True
2018-05-17 02:21:45.136131 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #762 | Epoch Duration: 152.22643113136292
2018-05-17 02:21:45.136368 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #762 | Started Training: True
2018-05-17 02:24:18.502523 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #763 | Epoch Duration: 153.36598134040833
2018-05-17 02:24:18.502717 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #763 | Started Training: True
2018-05-17 02:26:52.395643 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #764 | Epoch Duration: 153.89280223846436
2018-05-17 02:26:52.395832 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #764 | Started Training: True
2018-05-17 02:29:27.168011 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #765 | Epoch Duration: 154.7720546722412
2018-05-17 02:29:27.168172 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #765 | Started Training: True
2018-05-17 02:32:03.929059 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #766 | Epoch Duration: 156.76077938079834
2018-05-17 02:32:03.929284 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #766 | Started Training: True
2018-05-17 02:34:38.959797 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #767 | Epoch Duration: 155.03036379814148
2018-05-17 02:34:38.960030 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #767 | Started Training: True
2018-05-17 02:37:13.706291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #768 | Epoch Duration: 154.74611043930054
2018-05-17 02:37:13.706542 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #768 | Started Training: True
2018-05-17 02:39:47.467981 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #769 | Epoch Duration: 153.76128268241882
2018-05-17 02:39:47.468200 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #769 | Started Training: True
2018-05-17 02:42:40.656857 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #770 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.9858e-05
Policy Loss                      -0.523781
Raw Policy Loss                  -0.523781
Preactivation Policy Loss         0
Q Predictions Mean                0.522127
Q Predictions Std                 0.167682
Q Predictions Max                 0.699891
Q Predictions Min                -0.299803
Q Targets Mean                    0.523081
Q Targets Std                     0.167617
Q Targets Max                     0.710246
Q Targets Min                    -0.296915
Bellman Errors Mean               3.9858e-05
Bellman Errors Std                8.08575e-05
Bellman Errors Max                0.000508997
Bellman Errors Min                1.02673e-10
Policy Action Mean                0.339589
Policy Action Std                 0.933922
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00697538
Test Rewards Std                  0.00332289
Test Rewards Max                  0.0144637
Test Rewards Min                 -0.000652376
Test Returns Mean                 1.33346
Test Returns Std                  0.0131385
Test Returns Max                  1.35564
Test Returns Min                  1.31584
Test Actions Mean                 0.320863
Test Actions Std                  0.937833
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00806286
Exploration Rewards Std           0.00424511
Exploration Rewards Max           0.0161137
Exploration Rewards Min          -0.000867395
Exploration Returns Mean          0.945659
Exploration Returns Std           0.0851425
Exploration Returns Max           1.12259
Exploration Returns Min           0.84558
Exploration Actions Mean          0.209859
Exploration Actions Std           0.816413
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.33346
Number of train steps total  770863
Number of env steps total    771000
Number of rollouts total       5404
Train Time (s)                   79.9508
(Previous) Eval Time (s)          2.406e-06
Sample Time (s)                  73.3357
Epoch Time (s)                  153.286
Total Train Time (s)         103051
Epoch                           770
---------------------------  ----------------
2018-05-17 02:46:57.667542 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #770 | Epoch Duration: 430.1991991996765
2018-05-17 02:46:57.667663 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #770 | Started Training: True
2018-05-17 02:47:41.013129 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #771 | Epoch Duration: 43.3453483581543
2018-05-17 02:47:41.013362 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #771 | Started Training: True
2018-05-17 02:49:19.472490 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #772 | Epoch Duration: 98.4589684009552
2018-05-17 02:49:19.472660 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #772 | Started Training: True
2018-05-17 02:51:50.955745 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #773 | Epoch Duration: 151.48297810554504
2018-05-17 02:51:50.955974 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #773 | Started Training: True
2018-05-17 02:54:25.181856 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #774 | Epoch Duration: 154.22572898864746
2018-05-17 02:54:25.182100 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #774 | Started Training: True
2018-05-17 02:57:03.624109 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #775 | Epoch Duration: 158.4418535232544
2018-05-17 02:57:03.624305 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #775 | Started Training: True
2018-05-17 02:59:39.394574 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #776 | Epoch Duration: 155.77013969421387
2018-05-17 02:59:39.394813 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #776 | Started Training: True
2018-05-17 03:02:11.872937 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #777 | Epoch Duration: 152.47796058654785
2018-05-17 03:02:11.873134 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #777 | Started Training: True
2018-05-17 03:04:44.638455 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #778 | Epoch Duration: 152.76519107818604
2018-05-17 03:04:44.638656 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #778 | Started Training: True
2018-05-17 03:07:20.283359 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #779 | Epoch Duration: 155.64457535743713
2018-05-17 03:07:20.283582 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #779 | Started Training: True
2018-05-17 03:10:15.397671 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #780 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.35041e-05
Policy Loss                      -0.509212
Raw Policy Loss                  -0.509212
Preactivation Policy Loss         0
Q Predictions Mean                0.50569
Q Predictions Std                 0.222223
Q Predictions Max                 0.72623
Q Predictions Min                -0.355982
Q Targets Mean                    0.501764
Q Targets Std                     0.221266
Q Targets Max                     0.729228
Q Targets Min                    -0.340442
Bellman Errors Mean               5.35041e-05
Bellman Errors Std                8.8312e-05
Bellman Errors Max                0.000633008
Bellman Errors Min                2.07438e-09
Policy Action Mean                0.309899
Policy Action Std                 0.946303
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00862565
Test Rewards Std                  0.00445237
Test Rewards Max                  0.0154213
Test Rewards Min                 -0.000346882
Test Returns Mean                 0.947958
Test Returns Std                  0.00548212
Test Returns Max                  0.952125
Test Returns Min                  0.932455
Test Actions Mean                 0.289047
Test Actions Std                  0.949897
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00804823
Exploration Rewards Std           0.00415809
Exploration Rewards Max           0.0166556
Exploration Rewards Min          -0.000799312
Exploration Returns Mean          0.999769
Exploration Returns Std           0.149048
Exploration Returns Max           1.31595
Exploration Returns Min           0.852029
Exploration Actions Mean          0.185859
Exploration Actions Std           0.814348
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.947958
Number of train steps total  780863
Number of env steps total    781000
Number of rollouts total       5472
Train Time (s)                   79.1013
(Previous) Eval Time (s)          2.402e-06
Sample Time (s)                  75.8598
Epoch Time (s)                  154.961
Total Train Time (s)         104713
Epoch                           780
---------------------------  ----------------
2018-05-17 03:14:40.489852 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #780 | Epoch Duration: 440.2061266899109
2018-05-17 03:14:40.489963 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #780 | Started Training: True
2018-05-17 03:15:23.122744 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #781 | Epoch Duration: 42.63268542289734
2018-05-17 03:15:23.122912 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #781 | Started Training: True
2018-05-17 03:16:05.715508 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #782 | Epoch Duration: 42.592480421066284
2018-05-17 03:16:05.715772 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #782 | Started Training: True
2018-05-17 03:16:48.965189 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #783 | Epoch Duration: 43.24919843673706
2018-05-17 03:16:48.965401 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #783 | Started Training: True
2018-05-17 03:17:31.960762 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #784 | Epoch Duration: 42.995203495025635
2018-05-17 03:17:31.960986 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #784 | Started Training: True
2018-05-17 03:18:14.495895 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #785 | Epoch Duration: 42.53475022315979
2018-05-17 03:18:14.496071 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #785 | Started Training: True
2018-05-17 03:18:57.304352 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #786 | Epoch Duration: 42.80816435813904
2018-05-17 03:18:57.304524 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #786 | Started Training: True
2018-05-17 03:19:38.883889 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #787 | Epoch Duration: 41.57925343513489
2018-05-17 03:19:38.884073 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #787 | Started Training: True
2018-05-17 03:20:20.332882 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #788 | Epoch Duration: 41.4486825466156
2018-05-17 03:20:20.333110 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #788 | Started Training: True
2018-05-17 03:21:02.200426 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #789 | Epoch Duration: 41.86715507507324
2018-05-17 03:21:02.200641 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #789 | Started Training: True
2018-05-17 03:22:04.163592 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #790 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           5.70295e-05
Policy Loss                      -0.547298
Raw Policy Loss                  -0.547298
Preactivation Policy Loss         0
Q Predictions Mean                0.544223
Q Predictions Std                 0.187476
Q Predictions Max                 0.734884
Q Predictions Min                -0.08287
Q Targets Mean                    0.546827
Q Targets Std                     0.188341
Q Targets Max                     0.741385
Q Targets Min                    -0.0824939
Bellman Errors Mean               5.70295e-05
Bellman Errors Std                0.000161109
Bellman Errors Max                0.00140023
Bellman Errors Min                2.46673e-10
Policy Action Mean                0.26251
Policy Action Std                 0.960101
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00794715
Test Rewards Std                  0.00419992
Test Rewards Max                  0.0149003
Test Rewards Min                 -0.00113331
Test Returns Mean                 0.975733
Test Returns Std                  0.0122653
Test Returns Max                  0.998823
Test Returns Min                  0.961954
Test Actions Mean                 0.399462
Test Actions Std                  0.911917
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00807161
Exploration Rewards Std           0.00403054
Exploration Rewards Max           0.0159528
Exploration Rewards Min          -0.000894748
Exploration Returns Mean          1.10379
Exploration Returns Std           0.180439
Exploration Returns Max           1.34503
Exploration Returns Min           0.851347
Exploration Actions Mean          0.260869
Exploration Actions Std           0.798711
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.975733
Number of train steps total  790863
Number of env steps total    791000
Number of rollouts total       5547
Train Time (s)                   23.3186
(Previous) Eval Time (s)          2.242e-06
Sample Time (s)                  18.9019
Epoch Time (s)                   42.2205
Total Train Time (s)         105387
Epoch                           790
---------------------------  ----------------
2018-05-17 03:25:54.084301 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #790 | Epoch Duration: 291.8835163116455
2018-05-17 03:25:54.084419 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #790 | Started Training: True
2018-05-17 03:28:28.203863 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #791 | Epoch Duration: 154.11934065818787
2018-05-17 03:28:28.204051 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #791 | Started Training: True
2018-05-17 03:30:57.545850 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #792 | Epoch Duration: 149.34167408943176
2018-05-17 03:30:57.546072 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #792 | Started Training: True
2018-05-17 03:33:26.538958 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #793 | Epoch Duration: 148.99273467063904
2018-05-17 03:33:26.539152 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #793 | Started Training: True
2018-05-17 03:35:57.413007 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #794 | Epoch Duration: 150.87372756004333
2018-05-17 03:35:57.413210 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #794 | Started Training: True
2018-05-17 03:38:29.168428 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #795 | Epoch Duration: 151.75507736206055
2018-05-17 03:38:29.168644 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #795 | Started Training: True
2018-05-17 03:40:57.886203 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #796 | Epoch Duration: 148.7174210548401
2018-05-17 03:40:57.886389 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #796 | Started Training: True
2018-05-17 03:43:29.739744 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #797 | Epoch Duration: 151.85323214530945
2018-05-17 03:43:29.739957 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #797 | Started Training: True
2018-05-17 03:45:57.856490 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #798 | Epoch Duration: 148.11639857292175
2018-05-17 03:45:57.856668 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #798 | Started Training: True
2018-05-17 03:46:39.073567 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #799 | Epoch Duration: 41.21677207946777
2018-05-17 03:46:39.073771 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #799 | Started Training: True
2018-05-17 03:47:38.768320 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #800 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000222779
Policy Loss                      -0.483367
Raw Policy Loss                  -0.483367
Preactivation Policy Loss         0
Q Predictions Mean                0.480158
Q Predictions Std                 0.186404
Q Predictions Max                 0.721342
Q Predictions Min                -0.136574
Q Targets Mean                    0.476652
Q Targets Std                     0.188973
Q Targets Max                     0.720816
Q Targets Min                    -0.159516
Bellman Errors Mean               0.000222779
Bellman Errors Std                0.0017222
Bellman Errors Max                0.0195253
Bellman Errors Min                9.60654e-12
Policy Action Mean                0.384918
Policy Action Std                 0.9195
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00949947
Test Rewards Std                  0.00462215
Test Rewards Max                  0.0163685
Test Rewards Min                 -0.00109609
Test Returns Mean                 1.16316
Test Returns Std                  0.00495269
Test Returns Max                  1.17322
Test Returns Min                  1.15276
Test Actions Mean                 0.261345
Test Actions Std                  0.961834
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         9
Exploration Rewards Mean          0.00798658
Exploration Rewards Std           0.00423799
Exploration Rewards Max           0.0151888
Exploration Rewards Min          -0.00179101
Exploration Returns Mean          0.856339
Exploration Returns Std           0.0229277
Exploration Returns Max           0.889163
Exploration Returns Min           0.820727
Exploration Actions Mean          0.244302
Exploration Actions Std           0.800896
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.16316
Number of train steps total  800863
Number of env steps total    801000
Number of rollouts total       5629
Train Time (s)                   22.6143
(Previous) Eval Time (s)          2.81799e-06
Sample Time (s)                  17.73
Epoch Time (s)                   40.3442
Total Train Time (s)         108124
Epoch                           800
---------------------------  ----------------
2018-05-17 04:11:31.126465 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #800 | Epoch Duration: 1492.0525450706482
2018-05-17 04:11:31.126579 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #800 | Started Training: True
2018-05-17 04:12:14.805007 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #801 | Epoch Duration: 43.678327560424805
2018-05-17 04:12:14.805181 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #801 | Started Training: True
2018-05-17 04:13:05.769669 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #802 | Epoch Duration: 50.964350938797
2018-05-17 04:13:05.769830 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #802 | Started Training: True
2018-05-17 04:13:54.074058 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #803 | Epoch Duration: 48.30411458015442
2018-05-17 04:13:54.074239 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #803 | Started Training: True
2018-05-17 04:14:42.120388 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #804 | Epoch Duration: 48.046032190322876
2018-05-17 04:14:42.120557 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #804 | Started Training: True
2018-05-17 04:15:25.201200 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #805 | Epoch Duration: 43.08052849769592
2018-05-17 04:15:25.201375 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #805 | Started Training: True
2018-05-17 04:16:08.650228 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #806 | Epoch Duration: 43.44872331619263
2018-05-17 04:16:08.650386 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #806 | Started Training: True
2018-05-17 04:17:58.239979 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #807 | Epoch Duration: 109.58946537971497
2018-05-17 04:17:58.240168 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #807 | Started Training: True
2018-05-17 04:20:31.455865 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #808 | Epoch Duration: 153.21556544303894
2018-05-17 04:20:31.456094 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #808 | Started Training: True
2018-05-17 04:23:06.238095 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #809 | Epoch Duration: 154.78184723854065
2018-05-17 04:23:06.238295 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #809 | Started Training: True
2018-05-17 04:26:00.832036 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #810 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.72297e-05
Policy Loss                      -0.524558
Raw Policy Loss                  -0.524558
Preactivation Policy Loss         0
Q Predictions Mean                0.52096
Q Predictions Std                 0.201099
Q Predictions Max                 0.745798
Q Predictions Min                -0.3544
Q Targets Mean                    0.520892
Q Targets Std                     0.20276
Q Targets Max                     0.744541
Q Targets Min                    -0.352381
Bellman Errors Mean               6.72297e-05
Bellman Errors Std                0.000298827
Bellman Errors Max                0.00312694
Bellman Errors Min                1.24931e-09
Policy Action Mean                0.227209
Policy Action Std                 0.969835
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00941961
Test Rewards Std                  0.00432633
Test Rewards Max                  0.0160399
Test Rewards Min                 -0.000667539
Test Returns Mean                 1.2269
Test Returns Std                  0.0121588
Test Returns Max                  1.241
Test Returns Min                  1.20259
Test Actions Mean                 0.256644
Test Actions Std                  0.963731
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00937865
Exploration Rewards Std           0.00492919
Exploration Rewards Max           0.0192936
Exploration Rewards Min          -0.000957516
Exploration Returns Mean          1.19377
Exploration Returns Std           0.0780468
Exploration Returns Max           1.36357
Exploration Returns Min           1.1205
Exploration Actions Mean          0.194384
Exploration Actions Std           0.81721
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.2269
Number of train steps total  810863
Number of env steps total    811000
Number of rollouts total       5703
Train Time (s)                   79.3947
(Previous) Eval Time (s)          2.53001e-06
Sample Time (s)                  74.133
Epoch Time (s)                  153.528
Total Train Time (s)         109226
Epoch                           810
---------------------------  ----------------
2018-05-17 04:29:54.095016 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #810 | Epoch Duration: 407.8565685749054
2018-05-17 04:29:54.095134 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #810 | Started Training: True
2018-05-17 04:30:37.989068 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #811 | Epoch Duration: 43.89383053779602
2018-05-17 04:30:37.989329 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #811 | Started Training: True
2018-05-17 04:31:18.877420 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #812 | Epoch Duration: 40.887876987457275
2018-05-17 04:31:18.877684 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #812 | Started Training: True
2018-05-17 04:32:01.429126 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #813 | Epoch Duration: 42.55124378204346
2018-05-17 04:32:01.429355 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #813 | Started Training: True
2018-05-17 04:32:43.488067 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #814 | Epoch Duration: 42.05855178833008
2018-05-17 04:32:43.488282 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #814 | Started Training: True
2018-05-17 04:33:26.053155 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #815 | Epoch Duration: 42.56472420692444
2018-05-17 04:33:26.053342 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #815 | Started Training: True
2018-05-17 04:34:08.170677 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #816 | Epoch Duration: 42.11720538139343
2018-05-17 04:34:08.170831 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #816 | Started Training: True
2018-05-17 04:34:50.790481 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #817 | Epoch Duration: 42.619529724121094
2018-05-17 04:34:50.790689 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #817 | Started Training: True
2018-05-17 04:35:32.090353 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #818 | Epoch Duration: 41.29951047897339
2018-05-17 04:35:32.090600 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #818 | Started Training: True
2018-05-17 04:36:13.964222 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #819 | Epoch Duration: 41.87345862388611
2018-05-17 04:36:13.964440 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #819 | Started Training: True
2018-05-17 04:37:17.607217 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #820 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.96769e-05
Policy Loss                      -0.540299
Raw Policy Loss                  -0.540299
Preactivation Policy Loss         0
Q Predictions Mean                0.536795
Q Predictions Std                 0.227305
Q Predictions Max                 0.77462
Q Predictions Min                -0.310036
Q Targets Mean                    0.541599
Q Targets Std                     0.228387
Q Targets Max                     0.777368
Q Targets Min                    -0.333592
Bellman Errors Mean               7.96769e-05
Bellman Errors Std                0.00011406
Bellman Errors Max                0.000761199
Bellman Errors Min                8.63484e-10
Policy Action Mean                0.231128
Policy Action Std                 0.969476
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00923981
Test Rewards Std                  0.00458728
Test Rewards Max                  0.0151308
Test Rewards Min                 -0.000853062
Test Returns Mean                 1.09338
Test Returns Std                  0.0258082
Test Returns Max                  1.14135
Test Returns Min                  1.05555
Test Actions Mean                 0.235184
Test Actions Std                  0.967392
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         4
Exploration Rewards Mean          0.00886315
Exploration Rewards Std           0.00433854
Exploration Rewards Max           0.0181835
Exploration Rewards Min          -0.000232484
Exploration Returns Mean          1.20096
Exploration Returns Std           0.059117
Exploration Returns Max           1.27565
Exploration Returns Min           1.12431
Exploration Actions Mean          0.178414
Exploration Actions Std           0.817516
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.09338
Number of train steps total  820863
Number of env steps total    821000
Number of rollouts total       5781
Train Time (s)                   24.2686
(Previous) Eval Time (s)          2.185e-06
Sample Time (s)                  18.8032
Epoch Time (s)                   43.0718
Total Train Time (s)         109892
Epoch                           820
---------------------------  ----------------
2018-05-17 04:40:59.914531 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #820 | Epoch Duration: 285.94994735717773
2018-05-17 04:40:59.914648 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #820 | Started Training: True
2018-05-17 04:41:42.788374 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #821 | Epoch Duration: 42.87363052368164
2018-05-17 04:41:42.788541 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #821 | Started Training: True
2018-05-17 04:42:25.430678 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #822 | Epoch Duration: 42.642024993896484
2018-05-17 04:42:25.430932 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #822 | Started Training: True
2018-05-17 04:43:08.964232 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #823 | Epoch Duration: 43.53308987617493
2018-05-17 04:43:08.964400 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #823 | Started Training: True
2018-05-17 04:43:51.874368 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #824 | Epoch Duration: 42.90985417366028
2018-05-17 04:43:51.874568 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #824 | Started Training: True
2018-05-17 04:44:33.982451 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #825 | Epoch Duration: 42.10775876045227
2018-05-17 04:44:33.982709 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #825 | Started Training: True
2018-05-17 04:46:59.886736 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #826 | Epoch Duration: 145.90383434295654
2018-05-17 04:46:59.886898 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #826 | Started Training: True
2018-05-17 04:49:31.990174 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #827 | Epoch Duration: 152.10316681861877
2018-05-17 04:49:31.990358 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #827 | Started Training: True
2018-05-17 04:52:05.869146 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #828 | Epoch Duration: 153.87863945960999
2018-05-17 04:52:05.869311 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #828 | Started Training: True
2018-05-17 04:54:40.121100 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #829 | Epoch Duration: 154.25168204307556
2018-05-17 04:54:40.121306 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #829 | Started Training: True
2018-05-17 04:57:36.671291 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #830 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           7.72755e-05
Policy Loss                      -0.540678
Raw Policy Loss                  -0.540678
Preactivation Policy Loss         0
Q Predictions Mean                0.537748
Q Predictions Std                 0.213508
Q Predictions Max                 0.77275
Q Predictions Min                -0.337411
Q Targets Mean                    0.53708
Q Targets Std                     0.211193
Q Targets Max                     0.774411
Q Targets Min                    -0.330698
Bellman Errors Mean               7.72754e-05
Bellman Errors Std                0.000286935
Bellman Errors Max                0.00302362
Bellman Errors Min                9.09495e-13
Policy Action Mean                0.231684
Policy Action Std                 0.967019
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00896038
Test Rewards Std                  0.00483923
Test Rewards Max                  0.0161731
Test Rewards Min                 -0.000616392
Test Returns Mean                 1.10312
Test Returns Std                  0.0266635
Test Returns Max                  1.15835
Test Returns Min                  1.06407
Test Actions Mean                 0.299044
Test Actions Std                  0.95005
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00861131
Exploration Rewards Std           0.00421482
Exploration Rewards Max           0.0161458
Exploration Rewards Min          -0.00111316
Exploration Returns Mean          1.15822
Exploration Returns Std           0.0549348
Exploration Returns Max           1.20835
Exploration Returns Min           1.06996
Exploration Actions Mean          0.205156
Exploration Actions Std           0.809259
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.10312
Number of train steps total  830863
Number of env steps total    831000
Number of rollouts total       5859
Train Time (s)                   81.15
(Previous) Eval Time (s)          2.471e-06
Sample Time (s)                  74.0595
Epoch Time (s)                  155.21
Total Train Time (s)         111136
Epoch                           830
---------------------------  ----------------
2018-05-17 05:01:44.394926 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #830 | Epoch Duration: 424.2734913825989
2018-05-17 05:01:44.395039 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #830 | Started Training: True
2018-05-17 05:02:27.747048 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #831 | Epoch Duration: 43.351908683776855
2018-05-17 05:02:27.747262 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #831 | Started Training: True
2018-05-17 05:03:11.853278 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #832 | Epoch Duration: 44.105865716934204
2018-05-17 05:03:11.853434 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #832 | Started Training: True
2018-05-17 05:03:55.128501 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #833 | Epoch Duration: 43.274961948394775
2018-05-17 05:03:55.128699 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #833 | Started Training: True
2018-05-17 05:04:39.206099 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #834 | Epoch Duration: 44.07727789878845
2018-05-17 05:04:39.206264 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #834 | Started Training: True
2018-05-17 05:06:20.094814 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #835 | Epoch Duration: 100.88844013214111
2018-05-17 05:06:20.095050 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #835 | Started Training: True
2018-05-17 05:08:51.473344 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #836 | Epoch Duration: 151.37811040878296
2018-05-17 05:08:51.473566 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #836 | Started Training: True
2018-05-17 05:11:24.849747 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #837 | Epoch Duration: 153.3760323524475
2018-05-17 05:11:24.849933 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #837 | Started Training: True
2018-05-17 05:13:58.361309 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #838 | Epoch Duration: 153.51124453544617
2018-05-17 05:13:58.361525 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #838 | Started Training: True
2018-05-17 05:16:34.999668 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #839 | Epoch Duration: 156.63800621032715
2018-05-17 05:16:34.999843 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #839 | Started Training: True
2018-05-17 05:19:29.883303 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #840 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000940549
Policy Loss                      -0.543489
Raw Policy Loss                  -0.543489
Preactivation Policy Loss         0
Q Predictions Mean                0.540115
Q Predictions Std                 0.225854
Q Predictions Max                 0.761898
Q Predictions Min                -0.295884
Q Targets Mean                    0.535286
Q Targets Std                     0.232156
Q Targets Max                     0.760499
Q Targets Min                    -0.304334
Bellman Errors Mean               0.000940549
Bellman Errors Std                0.0077719
Bellman Errors Max                0.0843615
Bellman Errors Min                6.43651e-09
Policy Action Mean                0.271749
Policy Action Std                 0.955258
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00874258
Test Rewards Std                  0.0044754
Test Rewards Max                  0.0156952
Test Rewards Min                 -0.000579831
Test Returns Mean                 1.12889
Test Returns Std                  0.0101731
Test Returns Max                  1.14994
Test Returns Min                  1.11443
Test Actions Mean                 0.298907
Test Actions Std                  0.94802
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.0081087
Exploration Rewards Std           0.00444771
Exploration Rewards Max           0.0183051
Exploration Rewards Min          -0.00144278
Exploration Returns Mean          1.08657
Exploration Returns Std           0.0775129
Exploration Returns Max           1.23402
Exploration Returns Min           0.979765
Exploration Actions Mean          0.216279
Exploration Actions Std           0.815608
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.12889
Number of train steps total  840863
Number of env steps total    841000
Number of rollouts total       5933
Train Time (s)                   78.4492
(Previous) Eval Time (s)          1.899e-06
Sample Time (s)                  75.0392
Epoch Time (s)                  153.488
Total Train Time (s)         112434
Epoch                           840
---------------------------  ----------------
2018-05-17 05:23:22.833290 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #840 | Epoch Duration: 407.8333353996277
2018-05-17 05:23:22.833413 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #840 | Started Training: True
2018-05-17 05:24:05.421380 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #841 | Epoch Duration: 42.58784198760986
2018-05-17 05:24:05.421595 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #841 | Started Training: True
2018-05-17 05:24:47.036419 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #842 | Epoch Duration: 41.61466908454895
2018-05-17 05:24:47.036601 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #842 | Started Training: True
2018-05-17 05:25:29.500724 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #843 | Epoch Duration: 42.4639995098114
2018-05-17 05:25:29.500938 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #843 | Started Training: True
2018-05-17 05:27:55.303920 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #844 | Epoch Duration: 145.80282855033875
2018-05-17 05:27:55.304110 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #844 | Started Training: True
2018-05-17 05:30:29.806278 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #845 | Epoch Duration: 154.50204396247864
2018-05-17 05:30:29.806527 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #845 | Started Training: True
2018-05-17 05:33:02.795327 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #846 | Epoch Duration: 152.98864579200745
2018-05-17 05:33:02.795499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #846 | Started Training: True
2018-05-17 05:35:37.129933 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #847 | Epoch Duration: 154.33431482315063
2018-05-17 05:35:37.130154 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #847 | Started Training: True
2018-05-17 05:38:38.854435 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #848 | Epoch Duration: 181.72410559654236
2018-05-17 05:38:38.854675 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #848 | Started Training: True
2018-05-17 05:41:25.307026 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #849 | Epoch Duration: 166.45219826698303
2018-05-17 05:41:25.307187 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #849 | Started Training: True
2018-05-17 05:44:31.946436 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #850 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.77379e-05
Policy Loss                      -0.525973
Raw Policy Loss                  -0.525973
Preactivation Policy Loss         0
Q Predictions Mean                0.523206
Q Predictions Std                 0.214639
Q Predictions Max                 0.762488
Q Predictions Min                -0.182783
Q Targets Mean                    0.525105
Q Targets Std                     0.214246
Q Targets Max                     0.764037
Q Targets Min                    -0.198772
Bellman Errors Mean               6.77379e-05
Bellman Errors Std                0.000267655
Bellman Errors Max                0.00275629
Bellman Errors Min                1.26906e-10
Policy Action Mean                0.277568
Policy Action Std                 0.956011
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00805781
Test Rewards Std                  0.00346283
Test Rewards Max                  0.0145155
Test Rewards Min                 -0.000751301
Test Returns Mean                 1.27313
Test Returns Std                  0.0057419
Test Returns Max                  1.28146
Test Returns Min                  1.26527
Test Actions Mean                 0.33975
Test Actions Std                  0.93282
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00851155
Exploration Rewards Std           0.00451707
Exploration Rewards Max           0.0189964
Exploration Rewards Min          -0.00136611
Exploration Returns Mean          1.16122
Exploration Returns Std           0.102421
Exploration Returns Max           1.25627
Exploration Returns Min           0.964001
Exploration Actions Mean          0.247308
Exploration Actions Std           0.801648
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.27313
Number of train steps total  850863
Number of env steps total    851000
Number of rollouts total       6006
Train Time (s)                   84.2374
(Previous) Eval Time (s)          2.232e-06
Sample Time (s)                  86.5385
Epoch Time (s)                  170.776
Total Train Time (s)         113957
Epoch                           850
---------------------------  ----------------
2018-05-17 05:48:45.602048 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #850 | Epoch Duration: 440.2947618961334
2018-05-17 05:48:45.602159 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #850 | Started Training: True
2018-05-17 05:49:28.166729 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #851 | Epoch Duration: 42.56446647644043
2018-05-17 05:49:28.166889 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #851 | Started Training: True
2018-05-17 05:50:12.185625 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #852 | Epoch Duration: 44.01862692832947
2018-05-17 05:50:12.185831 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #852 | Started Training: True
2018-05-17 05:50:54.509745 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #853 | Epoch Duration: 42.32375478744507
2018-05-17 05:50:54.509978 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #853 | Started Training: True
2018-05-17 05:51:39.456848 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #854 | Epoch Duration: 44.94671559333801
2018-05-17 05:51:39.457075 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #854 | Started Training: True
2018-05-17 05:52:26.672439 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #855 | Epoch Duration: 47.215219020843506
2018-05-17 05:52:26.672601 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #855 | Started Training: True
2018-05-17 05:53:10.950006 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #856 | Epoch Duration: 44.277289152145386
2018-05-17 05:53:10.950269 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #856 | Started Training: True
2018-05-17 05:54:03.575874 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #857 | Epoch Duration: 52.62536954879761
2018-05-17 05:54:03.576075 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #857 | Started Training: True
2018-05-17 05:55:24.515618 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #858 | Epoch Duration: 80.9394097328186
2018-05-17 05:55:24.515790 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #858 | Started Training: True
2018-05-17 05:58:55.269019 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #859 | Epoch Duration: 210.75311589241028
2018-05-17 05:58:55.269208 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #859 | Started Training: True
2018-05-17 06:02:35.143755 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #860 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           6.43896e-05
Policy Loss                      -0.514691
Raw Policy Loss                  -0.514691
Preactivation Policy Loss         0
Q Predictions Mean                0.512006
Q Predictions Std                 0.199758
Q Predictions Max                 0.77339
Q Predictions Min                -0.0867614
Q Targets Mean                    0.507936
Q Targets Std                     0.200689
Q Targets Max                     0.775373
Q Targets Min                    -0.103834
Bellman Errors Mean               6.43895e-05
Bellman Errors Std                0.000147407
Bellman Errors Max                0.00135604
Bellman Errors Min                4.01087e-10
Policy Action Mean                0.308615
Policy Action Std                 0.946103
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00739826
Test Rewards Std                  0.00354839
Test Rewards Max                  0.0140359
Test Rewards Min                 -0.00104962
Test Returns Mean                 1.21014
Test Returns Std                  0.00370983
Test Returns Max                  1.21769
Test Returns Min                  1.20563
Test Actions Mean                 0.269841
Test Actions Std                  0.957647
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         7
Exploration Rewards Mean          0.00777732
Exploration Rewards Std           0.00412315
Exploration Rewards Max           0.0167239
Exploration Rewards Min          -0.00178882
Exploration Returns Mean          1.22326
Exploration Returns Std           0.0836347
Exploration Returns Max           1.3341
Exploration Returns Min           1.10762
Exploration Actions Mean          0.253084
Exploration Actions Std           0.805097
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.21014
Number of train steps total  860863
Number of env steps total    861000
Number of rollouts total       6066
Train Time (s)                  101.521
(Previous) Eval Time (s)          2.463e-06
Sample Time (s)                 183.109
Epoch Time (s)                  284.63
Total Train Time (s)         115055
Epoch                           860
---------------------------  ----------------
2018-05-17 06:07:03.869787 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #860 | Epoch Duration: 488.60046458244324
2018-05-17 06:07:03.869907 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #860 | Started Training: True
2018-05-17 06:10:27.434557 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #861 | Epoch Duration: 203.56455039978027
2018-05-17 06:10:27.434760 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #861 | Started Training: True
2018-05-17 06:11:26.570802 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #862 | Epoch Duration: 59.13591265678406
2018-05-17 06:11:26.570986 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #862 | Started Training: True
2018-05-17 06:12:39.717484 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #863 | Epoch Duration: 73.14636945724487
2018-05-17 06:12:39.717644 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #863 | Started Training: True
2018-05-17 06:14:06.789213 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #864 | Epoch Duration: 87.07145929336548
2018-05-17 06:14:06.789434 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #864 | Started Training: True
2018-05-17 06:15:13.706554 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #865 | Epoch Duration: 66.91695594787598
2018-05-17 06:15:13.706778 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #865 | Started Training: True
2018-05-17 06:16:23.133118 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #866 | Epoch Duration: 69.42616701126099
2018-05-17 06:16:23.133392 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #866 | Started Training: True
2018-05-17 06:17:25.208171 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #867 | Epoch Duration: 62.0745804309845
2018-05-17 06:17:25.208390 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #867 | Started Training: True
2018-05-17 06:18:24.091510 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #868 | Epoch Duration: 58.882959842681885
2018-05-17 06:18:24.091703 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #868 | Started Training: True
2018-05-17 06:19:16.620499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #869 | Epoch Duration: 52.52867031097412
2018-05-17 06:19:16.620735 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #869 | Started Training: True
2018-05-17 06:20:29.690294 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #870 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000137602
Policy Loss                      -0.555631
Raw Policy Loss                  -0.555631
Preactivation Policy Loss         0
Q Predictions Mean                0.552283
Q Predictions Std                 0.22327
Q Predictions Max                 0.779856
Q Predictions Min                -0.198067
Q Targets Mean                    0.554568
Q Targets Std                     0.221416
Q Targets Max                     0.781083
Q Targets Min                    -0.198164
Bellman Errors Mean               0.000137602
Bellman Errors Std                0.000902615
Bellman Errors Max                0.0101699
Bellman Errors Min                1.27898e-11
Policy Action Mean                0.219531
Policy Action Std                 0.971529
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00656173
Test Rewards Std                  0.00350094
Test Rewards Max                  0.0145393
Test Rewards Min                 -0.00053282
Test Returns Mean                 1.32284
Test Returns Std                  0.0493029
Test Returns Max                  1.37415
Test Returns Min                  1.24514
Test Actions Mean                 0.213761
Test Actions Std                  0.961404
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00738482
Exploration Rewards Std           0.00392009
Exploration Rewards Max           0.0188295
Exploration Rewards Min          -0.000717512
Exploration Returns Mean          1.2308
Exploration Returns Std           0.0795524
Exploration Returns Max           1.33511
Exploration Returns Min           1.08516
Exploration Actions Mean          0.229909
Exploration Actions Std           0.804597
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.32284
Number of train steps total  870863
Number of env steps total    871000
Number of rollouts total       6130
Train Time (s)                   26.7288
(Previous) Eval Time (s)          2.59301e-06
Sample Time (s)                  76.86
Epoch Time (s)                  103.589
Total Train Time (s)         116092
Epoch                           870
---------------------------  ----------------
2018-05-17 06:24:20.692917 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #870 | Epoch Duration: 304.07203006744385
2018-05-17 06:24:20.693089 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #870 | Started Training: True
2018-05-17 06:27:05.366036 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #871 | Epoch Duration: 164.67280459403992
2018-05-17 06:27:05.366355 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #871 | Started Training: True
2018-05-17 06:29:41.488161 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #872 | Epoch Duration: 156.12161993980408
2018-05-17 06:29:41.488335 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #872 | Started Training: True
2018-05-17 06:32:18.751729 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #873 | Epoch Duration: 157.26327991485596
2018-05-17 06:32:18.751911 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #873 | Started Training: True
2018-05-17 06:34:51.799522 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #874 | Epoch Duration: 153.0474817752838
2018-05-17 06:34:51.799685 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #874 | Started Training: True
2018-05-17 06:37:31.572990 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #875 | Epoch Duration: 159.77318048477173
2018-05-17 06:37:31.573195 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #875 | Started Training: True
2018-05-17 06:38:29.853875 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #876 | Epoch Duration: 58.28054094314575
2018-05-17 06:38:29.854097 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #876 | Started Training: True
2018-05-17 06:39:12.209678 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #877 | Epoch Duration: 42.35543251037598
2018-05-17 06:39:12.209845 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #877 | Started Training: True
2018-05-17 06:39:56.690297 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #878 | Epoch Duration: 44.48033404350281
2018-05-17 06:39:56.690547 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #878 | Started Training: True
2018-05-17 06:40:40.227869 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #879 | Epoch Duration: 43.53715181350708
2018-05-17 06:40:40.228026 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #879 | Started Training: True
2018-05-17 06:41:47.515096 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #880 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           0.000266787
Policy Loss                      -0.536391
Raw Policy Loss                  -0.536391
Preactivation Policy Loss         0
Q Predictions Mean                0.531626
Q Predictions Std                 0.21553
Q Predictions Max                 0.751685
Q Predictions Min                -0.243629
Q Targets Mean                    0.532248
Q Targets Std                     0.216197
Q Targets Max                     0.753808
Q Targets Min                    -0.267661
Bellman Errors Mean               0.000266787
Bellman Errors Std                0.00172999
Bellman Errors Max                0.0182922
Bellman Errors Min                1.57346e-09
Policy Action Mean                0.17465
Policy Action Std                 0.976942
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00699831
Test Rewards Std                  0.00352184
Test Rewards Max                  0.0136775
Test Rewards Min                 -0.000513618
Test Returns Mean                 1.18038
Test Returns Std                  0.0393891
Test Returns Max                  1.26526
Test Returns Min                  1.14428
Test Actions Mean                 0.237082
Test Actions Std                  0.956284
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00738964
Exploration Rewards Std           0.00395948
Exploration Rewards Max           0.0171514
Exploration Rewards Min          -0.00148947
Exploration Returns Mean          1.27656
Exploration Returns Std           0.0316617
Exploration Returns Max           1.33055
Exploration Returns Min           1.2331
Exploration Actions Mean          0.221375
Exploration Actions Std           0.81041
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.18038
Number of train steps total  880863
Number of env steps total    881000
Number of rollouts total       6191
Train Time (s)                   23.6632
(Previous) Eval Time (s)          1.86701e-06
Sample Time (s)                  18.9817
Epoch Time (s)                   42.6448
Total Train Time (s)         117360
Epoch                           880
---------------------------  ----------------
2018-05-17 06:45:29.359894 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #880 | Epoch Duration: 289.13177156448364
2018-05-17 06:45:29.360005 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #880 | Started Training: True
2018-05-17 06:46:15.010484 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #881 | Epoch Duration: 45.6503643989563
2018-05-17 06:46:15.010684 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #881 | Started Training: True
2018-05-17 06:47:01.479809 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #882 | Epoch Duration: 46.46899962425232
2018-05-17 06:47:01.479995 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #882 | Started Training: True
2018-05-17 06:47:49.532118 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #883 | Epoch Duration: 48.05200266838074
2018-05-17 06:47:49.532317 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #883 | Started Training: True
2018-05-17 06:48:44.884305 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #884 | Epoch Duration: 55.35186243057251
2018-05-17 06:48:44.884488 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #884 | Started Training: True
2018-05-17 06:50:58.197393 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #885 | Epoch Duration: 133.31278705596924
2018-05-17 06:50:58.197642 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #885 | Started Training: True
2018-05-17 06:53:42.686741 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #886 | Epoch Duration: 164.48892188072205
2018-05-17 06:53:42.686951 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #886 | Started Training: True
2018-05-17 06:56:21.656463 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #887 | Epoch Duration: 158.96937584877014
2018-05-17 06:56:21.656734 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #887 | Started Training: True
2018-05-17 06:59:13.556039 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #888 | Epoch Duration: 171.8990933895111
2018-05-17 06:59:13.556263 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #888 | Started Training: True
2018-05-17 07:01:59.019499 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #889 | Epoch Duration: 165.46308732032776
2018-05-17 07:01:59.019668 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #889 | Started Training: True
2018-05-17 07:04:59.465176 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #890 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.72439e-05
Policy Loss                      -0.525437
Raw Policy Loss                  -0.525437
Preactivation Policy Loss         0
Q Predictions Mean                0.521826
Q Predictions Std                 0.211004
Q Predictions Max                 0.728643
Q Predictions Min                -0.209373
Q Targets Mean                    0.525035
Q Targets Std                     0.209962
Q Targets Max                     0.73282
Q Targets Min                    -0.205633
Bellman Errors Mean               4.72439e-05
Bellman Errors Std                0.000106977
Bellman Errors Max                0.00111747
Bellman Errors Min                4.45652e-11
Policy Action Mean                0.213589
Policy Action Std                 0.966732
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00596483
Test Rewards Std                  0.00337487
Test Rewards Max                  0.0148472
Test Rewards Min                 -0.000230233
Test Returns Mean                 1.24784
Test Returns Std                  0.00737821
Test Returns Max                  1.25715
Test Returns Min                  1.23672
Test Actions Mean                 0.249647
Test Actions Std                  0.950569
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         5
Exploration Rewards Mean          0.00754357
Exploration Rewards Std           0.00403993
Exploration Rewards Max           0.0171178
Exploration Rewards Min          -0.000404451
Exploration Returns Mean          1.28241
Exploration Returns Std           0.0484114
Exploration Returns Max           1.32945
Exploration Returns Min           1.20323
Exploration Actions Mean          0.125918
Exploration Actions Std           0.828417
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.24784
Number of train steps total  890863
Number of env steps total    891000
Number of rollouts total       6248
Train Time (s)                   77.7662
(Previous) Eval Time (s)          2.35901e-06
Sample Time (s)                  77.5702
Epoch Time (s)                  155.336
Total Train Time (s)         118780
Epoch                           890
---------------------------  ----------------
2018-05-17 07:09:09.836437 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #890 | Epoch Duration: 430.81665992736816
2018-05-17 07:09:09.836549 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #890 | Started Training: True
2018-05-17 07:09:52.566176 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #891 | Epoch Duration: 42.729525566101074
2018-05-17 07:09:52.566415 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #891 | Started Training: True
2018-05-17 07:10:36.455321 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #892 | Epoch Duration: 43.88874006271362
2018-05-17 07:10:36.455534 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #892 | Started Training: True
2018-05-17 07:11:18.772889 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #893 | Epoch Duration: 42.31720781326294
2018-05-17 07:11:18.773054 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #893 | Started Training: True
2018-05-17 07:12:03.953225 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #894 | Epoch Duration: 45.18006348609924
2018-05-17 07:12:03.953456 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #894 | Started Training: True
2018-05-17 07:12:51.329092 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #895 | Epoch Duration: 47.37547469139099
2018-05-17 07:12:51.329312 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #895 | Started Training: True
2018-05-17 07:14:07.342885 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #896 | Epoch Duration: 76.01341319084167
2018-05-17 07:14:07.343069 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #896 | Started Training: True
2018-05-17 07:15:43.927878 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #897 | Epoch Duration: 96.58467507362366
2018-05-17 07:15:43.928076 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #897 | Started Training: True
2018-05-17 07:17:24.475791 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #898 | Epoch Duration: 100.54759001731873
2018-05-17 07:17:24.475987 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #898 | Started Training: True
2018-05-17 07:21:12.980491 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #899 | Epoch Duration: 228.50437378883362
2018-05-17 07:21:12.980727 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #899 | Started Training: True
2018-05-17 07:24:36.632276 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #900 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.19759e-05
Policy Loss                      -0.542183
Raw Policy Loss                  -0.542183
Preactivation Policy Loss         0
Q Predictions Mean                0.539421
Q Predictions Std                 0.194432
Q Predictions Max                 0.717287
Q Predictions Min                 0.0100971
Q Targets Mean                    0.538416
Q Targets Std                     0.195376
Q Targets Max                     0.721498
Q Targets Min                     0.0245709
Bellman Errors Mean               3.19759e-05
Bellman Errors Std                6.89134e-05
Bellman Errors Max                0.000534495
Bellman Errors Min                2.72104e-10
Policy Action Mean                0.147077
Policy Action Std                 0.979838
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00690671
Test Rewards Std                  0.00358074
Test Rewards Max                  0.0147673
Test Rewards Min                 -0.00052649
Test Returns Mean                 1.25127
Test Returns Std                  0.0325636
Test Returns Max                  1.29519
Test Returns Min                  1.20129
Test Actions Mean                 0.128216
Test Actions Std                  0.979006
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         6
Exploration Rewards Mean          0.00792532
Exploration Rewards Std           0.00412436
Exploration Rewards Max           0.0161146
Exploration Rewards Min          -0.00107973
Exploration Returns Mean          1.24163
Exploration Returns Std           0.0713132
Exploration Returns Max           1.32328
Exploration Returns Min           1.11586
Exploration Actions Mean          0.0936281
Exploration Actions Std           0.829967
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.25127
Number of train steps total  900863
Number of env steps total    901000
Number of rollouts total       6310
Train Time (s)                   94.2877
(Previous) Eval Time (s)          3.21199e-06
Sample Time (s)                 144.345
Epoch Time (s)                  238.633
Total Train Time (s)         119981
Epoch                           900
---------------------------  ----------------
2018-05-17 07:29:10.649273 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #900 | Epoch Duration: 477.66837525367737
2018-05-17 07:29:10.649398 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #900 | Started Training: True
2018-05-17 07:30:37.581301 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #901 | Epoch Duration: 86.93179607391357
2018-05-17 07:30:37.581489 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #901 | Started Training: True
2018-05-17 07:31:19.604308 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #902 | Epoch Duration: 42.0226891040802
2018-05-17 07:31:19.604523 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #902 | Started Training: True
2018-05-17 07:32:02.392700 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #903 | Epoch Duration: 42.78801441192627
2018-05-17 07:32:02.392902 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #903 | Started Training: True
2018-05-17 07:32:45.323548 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #904 | Epoch Duration: 42.930514097213745
2018-05-17 07:32:45.323735 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #904 | Started Training: True
2018-05-17 07:33:28.616394 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #905 | Epoch Duration: 43.29253387451172
2018-05-17 07:33:28.616567 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #905 | Started Training: True
2018-05-17 07:34:11.833090 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #906 | Epoch Duration: 43.216408491134644
2018-05-17 07:34:11.833242 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #906 | Started Training: True
2018-05-17 07:34:54.589264 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #907 | Epoch Duration: 42.7559118270874
2018-05-17 07:34:54.589418 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #907 | Started Training: True
2018-05-17 07:35:38.164404 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #908 | Epoch Duration: 43.57488417625427
2018-05-17 07:35:38.164561 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #908 | Started Training: True
2018-05-17 07:36:20.984631 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #909 | Epoch Duration: 42.8199622631073
2018-05-17 07:36:20.984849 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #909 | Started Training: True
2018-05-17 07:37:28.771087 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #910 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           3.64199e-05
Policy Loss                      -0.544272
Raw Policy Loss                  -0.544272
Preactivation Policy Loss         0
Q Predictions Mean                0.541856
Q Predictions Std                 0.184534
Q Predictions Max                 0.753577
Q Predictions Min                 0.0252215
Q Targets Mean                    0.542947
Q Targets Std                     0.184694
Q Targets Max                     0.752567
Q Targets Min                     0.0139355
Bellman Errors Mean               3.64199e-05
Bellman Errors Std                0.000109365
Bellman Errors Max                0.00109289
Bellman Errors Min                8.81087e-10
Policy Action Mean                0.126889
Policy Action Std                 0.982781
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00725501
Test Rewards Std                  0.00346127
Test Rewards Max                  0.0146273
Test Rewards Min                  0.000320931
Test Returns Mean                 1.24182
Test Returns Std                  0.0248787
Test Returns Max                  1.28381
Test Returns Min                  1.20094
Test Actions Mean                 0.319419
Test Actions Std                  0.930692
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00728937
Exploration Rewards Std           0.00370172
Exploration Rewards Max           0.0155386
Exploration Rewards Min          -0.000460855
Exploration Returns Mean          1.1663
Exploration Returns Std           0.0903858
Exploration Returns Max           1.25049
Exploration Returns Min           0.957569
Exploration Actions Mean          0.189383
Exploration Actions Std           0.817798
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     1.24182
Number of train steps total  910863
Number of env steps total    911000
Number of rollouts total       6377
Train Time (s)                   24.2522
(Previous) Eval Time (s)          3.34901e-06
Sample Time (s)                  18.8066
Epoch Time (s)                   43.0588
Total Train Time (s)         120693
Epoch                           910
---------------------------  ----------------
2018-05-17 07:41:03.376924 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #910 | Epoch Duration: 282.39193296432495
2018-05-17 07:41:03.377035 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #910 | Started Training: True
2018-05-17 07:41:47.397138 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #911 | Epoch Duration: 44.02000570297241
2018-05-17 07:41:47.397349 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #911 | Started Training: True
2018-05-17 07:42:30.388147 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #912 | Epoch Duration: 42.99064040184021
2018-05-17 07:42:30.388313 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #912 | Started Training: True
2018-05-17 07:43:13.083374 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #913 | Epoch Duration: 42.694945335388184
2018-05-17 07:43:13.083589 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #913 | Started Training: True
2018-05-17 07:44:43.160709 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #914 | Epoch Duration: 90.07696771621704
2018-05-17 07:44:43.160927 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #914 | Started Training: True
2018-05-17 07:47:17.576437 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #915 | Epoch Duration: 154.4153709411621
2018-05-17 07:47:17.576622 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #915 | Started Training: True
2018-05-17 07:49:53.579799 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #916 | Epoch Duration: 156.00305581092834
2018-05-17 07:49:53.579995 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #916 | Started Training: True
2018-05-17 07:52:30.485244 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #917 | Epoch Duration: 156.9051034450531
2018-05-17 07:52:30.485443 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #917 | Started Training: True
2018-05-17 07:55:07.703578 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #918 | Epoch Duration: 157.21800637245178
2018-05-17 07:55:07.703779 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #918 | Started Training: True
2018-05-17 07:57:43.304863 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #919 | Epoch Duration: 155.60095405578613
2018-05-17 07:57:43.305051 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #919 | Started Training: True
2018-05-17 08:00:46.192031 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #920 | Collecting samples for evaluation
---------------------------  ----------------
QF Loss                           4.7449e-05
Policy Loss                      -0.542102
Raw Policy Loss                  -0.542102
Preactivation Policy Loss         0
Q Predictions Mean                0.538205
Q Predictions Std                 0.181715
Q Predictions Max                 0.778598
Q Predictions Min                 0.0355234
Q Targets Mean                    0.537617
Q Targets Std                     0.183433
Q Targets Max                     0.779442
Q Targets Min                     0.035621
Bellman Errors Mean               4.7449e-05
Bellman Errors Std                0.000168183
Bellman Errors Max                0.0018537
Bellman Errors Min                2.41221e-09
Policy Action Mean                0.338089
Policy Action Std                 0.927267
Policy Action Max                 1
Policy Action Min                -1
Test Rewards Mean                 0.00696714
Test Rewards Std                  0.00337472
Test Rewards Max                  0.0144561
Test Rewards Min                 -0.000965496
Test Returns Mean                 0.978012
Test Returns Std                  0.0461348
Test Returns Max                  1.02729
Test Returns Min                  0.91015
Test Actions Mean                 0.257968
Test Actions Std                  0.957641
Test Actions Max                  1
Test Actions Min                 -1
Num Paths                         8
Exploration Rewards Mean          0.00691551
Exploration Rewards Std           0.0038496
Exploration Rewards Max           0.0158821
Exploration Rewards Min          -0.00126332
Exploration Returns Mean          1.11945
Exploration Returns Std           0.223872
Exploration Returns Max           1.30681
Exploration Returns Min           0.731445
Exploration Actions Mean          0.226321
Exploration Actions Std           0.802314
Exploration Actions Max           1
Exploration Actions Min          -1
AverageReturn                     0.978012
Number of train steps total  920863
Number of env steps total    921000
Number of rollouts total       6440
Train Time (s)                   78.1101
(Previous) Eval Time (s)          2.35101e-06
Sample Time (s)                  79.3467
Epoch Time (s)                  157.457
Total Train Time (s)         122136
Epoch                           920
---------------------------  ----------------
2018-05-17 08:05:05.887041 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #920 | Epoch Duration: 442.581871509552
2018-05-17 08:05:05.887152 UTC | [name-of-experiment_2018_05_15_22_09_07_0000--s-0] Iteration #920 | Started Training: True
